{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"APTrust Preservation Services Welcome to the official documentation for APTrust's preservation services, also called Preserv. This documentation is intended for people who will have to maintain Preserv and dig into its internals. These pages present an overview of the system's components, what they do, and how they fit together. You should find enough info in these pages to troubleshoot ingest and restoration problems on live systems, and to know which parts of the source code to look at if you're going to make changes. This documentation does not aim to get down into the details of the code. The code itself is well commented where necessary. Why did you name it preservation-services? So you know what it does. In the past, APTrust has created projects like Fluctus, Pharos, EarthDiver, and SunDancer. Any idea what those things do? Alrighty, then. I like to be clear, like this guy:","title":"Home"},{"location":"#aptrust-preservation-services","text":"Welcome to the official documentation for APTrust's preservation services, also called Preserv. This documentation is intended for people who will have to maintain Preserv and dig into its internals. These pages present an overview of the system's components, what they do, and how they fit together. You should find enough info in these pages to troubleshoot ingest and restoration problems on live systems, and to know which parts of the source code to look at if you're going to make changes. This documentation does not aim to get down into the details of the code. The code itself is well commented where necessary.","title":"APTrust Preservation Services"},{"location":"#why-did-you-name-it-preservation-services","text":"So you know what it does. In the past, APTrust has created projects like Fluctus, Pharos, EarthDiver, and SunDancer. Any idea what those things do? Alrighty, then. I like to be clear, like this guy:","title":"Why did you name it preservation-services?"},{"location":"auditor/","text":"Auditor The auditor is a command-line tool for doing sanity checks on individual files and batches of files. It checks that the files are in the proper locations in preservation strorage, and that file sizes and S3 metadata are correct. This tool can check dozens of files per minute. For usage info, see the README .","title":"Auditor"},{"location":"auditor/#auditor","text":"The auditor is a command-line tool for doing sanity checks on individual files and batches of files. It checks that the files are in the proper locations in preservation strorage, and that file sizes and S3 metadata are correct. This tool can check dozens of files per minute. For usage info, see the README .","title":"Auditor"},{"location":"docker/","text":"Docker All Preserv components run in Docker containers in Amazon's Fargate service. All except the three cron jobs, ingest_bucket_reader , apt_queue and apt_queue_fixity , scale automatically based on ECS triggers. The cron jobs don't need to scale because their workload is so light. There are 17 containers in all. Each container includes a single executable. The Makefile generates the containers with the following commands: make release make update-template The first command, make release builds the containers and pushes them to Docker hub. The second cmmand, make update-template updates the CloudFormation template to point to the newly built containers. (Each container is tagged with a Git commit ID. Newly built containers are tagged with the most recent commit ID. make update-template tells CloudFormation to pull the containers with the latest commit ID.) CI Testing and Docker Builds Travis CI will run the Preserv unit tests, and if they pass, will run make release , pushing new containers to Docker Hub. Travis does not run make update-template . You'll have to do that on your own and then deploy if you want the new containers to run on AWS. Warning Travis is not currently set up to run the full Preserv test suite, so just because Travis tests pass doesn't mean Preserv is safe to deploy. We intend to fix this deficiency when we move from Travis to GitLab. Until then, read the next section, Testing Before Deployment! Testing Before Deployment Travis runs only the unit tests. You should not deploy Preserv to any environment without running the integration and end-to-end tests first. Currently, you can only run those locally. See Putting it All Together for info on how run the full test suite, and note that the suite takes several minutes to run. Command-line Deployment TO BE FILLED IN LATER Deployment Through the AWS Console After you've built new containers, follow these steps to deploy them manually through the AWS console. Login to the AWS using your administrator account. Proceed to the CloudFormation Console Select the stack preserv-services-<env> link to open stack details and options. (Where <env> is one of prod , demo , or staging .) Select update. Select replace current template Select upload template file. Upload the updated \u2018cfn-preserv-master.yml\u2019 Select next. Select next again. ( You do not need to change any settings.) At the final pane for deployment you will need to select that you know you are creating new resources. Submit. You can then monitor changes under the events tab. Failures will roll back automatically. Give about 10 minutes before worrying it has failed. List of Containers The Preserv suite includes 17 containers. The workers inside these containers are compiled by the Makefile from sources in Preserv's apps directory Name Executable Service Description Bucket Reader ingest_ bucket_ reader Ingest A cron job that scans for new items in receiving buckets. Creates an ingest work item in Registry and pushes the work item ID into NSQ. Metadata Gatherer apt_ pre_fetch Ingest Streams a bag from a receiving bucket through a number of functions to calculate checksums and parse tag files and manifests. Saves tag files and manifests to S3 staging bucket. Saves all other metadata to Redis. Bag Validator ingest_ validator Ingest Analyzes the metdata gathered by apt_pre_fetch to ensure bag is valid. If bag is invalid, processing stops here. Reingest Manager reingest_ manager Ingest Checks to see if the bag has ever been ingested before. If so, checks to see which files are new or updated. Staging Uploader ingest_ staging_ uploader Ingest Unpacks the tarred bag from the receiving bucket and stores its individual files in a temporary staging bucket, where other workers can access them. Format Identifier ingest_ format_ identifier Ingest Streams files from the staging bucket through the Siegfried format identifier, which matches byte streams against a Pronom registry. Preservation Uploader ingest_ preservation_ uploader Ingest Uploads files to long-term preservation buckets in S3, Glacier, and/or Wasabi. Preservation Verifier ingest_ preservation_ verifier Ingest Verifies that the files copied into long-term preservation actually arrived intact and are accessible. Ingest Recorder ingest_ recorder Ingest Records details of an ingest in the Registry. Cleanup ingest_ cleanup Ingest Cleans up temporary resources no longer required after ingest. These include files in the staging bucket, metadata records in Redis, and the tarred bag in the receiving bucket. Queue Fixity apt_ queue_ fixity Fixity Check Cron job that queues Generic Files for scheduled fixity checks. Fixity Checker apt_ fixity Fixity Check Worker that permforms scheduled fixity checks. Glacier Restorer glacier_ restorer Restoration Moves files from Glacier and Glacier Deep Archive into S3 so they can be restored. File Restorer file_restorer Restoration Restores individual files to depositor restoration buckets. Bag Restorer bag_ restorer Restoration Restores entire bags (intellectual objects) to depositor restoration buckets. Deletion Worker apt_delete Deletion Permanently deletes files and objects from preservation storage. APT Queue apt_queue Restoration and Deletion Queues deletion and restoration requests created by Registry users. Those requests should be queued automatically by Registry itself. If they're not, apt_queue will find them. This cron job is a vestige from the old, unreliable Pharos system, which did occasionally fail at queueing requests. It may no longer be needed, but we'll keep it around as a failsafe.","title":"Docker"},{"location":"docker/#docker","text":"All Preserv components run in Docker containers in Amazon's Fargate service. All except the three cron jobs, ingest_bucket_reader , apt_queue and apt_queue_fixity , scale automatically based on ECS triggers. The cron jobs don't need to scale because their workload is so light. There are 17 containers in all. Each container includes a single executable. The Makefile generates the containers with the following commands: make release make update-template The first command, make release builds the containers and pushes them to Docker hub. The second cmmand, make update-template updates the CloudFormation template to point to the newly built containers. (Each container is tagged with a Git commit ID. Newly built containers are tagged with the most recent commit ID. make update-template tells CloudFormation to pull the containers with the latest commit ID.)","title":"Docker"},{"location":"docker/#ci-testing-and-docker-builds","text":"Travis CI will run the Preserv unit tests, and if they pass, will run make release , pushing new containers to Docker Hub. Travis does not run make update-template . You'll have to do that on your own and then deploy if you want the new containers to run on AWS. Warning Travis is not currently set up to run the full Preserv test suite, so just because Travis tests pass doesn't mean Preserv is safe to deploy. We intend to fix this deficiency when we move from Travis to GitLab. Until then, read the next section, Testing Before Deployment!","title":"CI Testing and Docker Builds"},{"location":"docker/#testing-before-deployment","text":"Travis runs only the unit tests. You should not deploy Preserv to any environment without running the integration and end-to-end tests first. Currently, you can only run those locally. See Putting it All Together for info on how run the full test suite, and note that the suite takes several minutes to run.","title":"Testing Before Deployment"},{"location":"docker/#command-line-deployment","text":"TO BE FILLED IN LATER","title":"Command-line Deployment"},{"location":"docker/#deployment-through-the-aws-console","text":"After you've built new containers, follow these steps to deploy them manually through the AWS console. Login to the AWS using your administrator account. Proceed to the CloudFormation Console Select the stack preserv-services-<env> link to open stack details and options. (Where <env> is one of prod , demo , or staging .) Select update. Select replace current template Select upload template file. Upload the updated \u2018cfn-preserv-master.yml\u2019 Select next. Select next again. ( You do not need to change any settings.) At the final pane for deployment you will need to select that you know you are creating new resources. Submit. You can then monitor changes under the events tab. Failures will roll back automatically. Give about 10 minutes before worrying it has failed.","title":"Deployment Through the AWS Console"},{"location":"docker/#list-of-containers","text":"The Preserv suite includes 17 containers. The workers inside these containers are compiled by the Makefile from sources in Preserv's apps directory Name Executable Service Description Bucket Reader ingest_ bucket_ reader Ingest A cron job that scans for new items in receiving buckets. Creates an ingest work item in Registry and pushes the work item ID into NSQ. Metadata Gatherer apt_ pre_fetch Ingest Streams a bag from a receiving bucket through a number of functions to calculate checksums and parse tag files and manifests. Saves tag files and manifests to S3 staging bucket. Saves all other metadata to Redis. Bag Validator ingest_ validator Ingest Analyzes the metdata gathered by apt_pre_fetch to ensure bag is valid. If bag is invalid, processing stops here. Reingest Manager reingest_ manager Ingest Checks to see if the bag has ever been ingested before. If so, checks to see which files are new or updated. Staging Uploader ingest_ staging_ uploader Ingest Unpacks the tarred bag from the receiving bucket and stores its individual files in a temporary staging bucket, where other workers can access them. Format Identifier ingest_ format_ identifier Ingest Streams files from the staging bucket through the Siegfried format identifier, which matches byte streams against a Pronom registry. Preservation Uploader ingest_ preservation_ uploader Ingest Uploads files to long-term preservation buckets in S3, Glacier, and/or Wasabi. Preservation Verifier ingest_ preservation_ verifier Ingest Verifies that the files copied into long-term preservation actually arrived intact and are accessible. Ingest Recorder ingest_ recorder Ingest Records details of an ingest in the Registry. Cleanup ingest_ cleanup Ingest Cleans up temporary resources no longer required after ingest. These include files in the staging bucket, metadata records in Redis, and the tarred bag in the receiving bucket. Queue Fixity apt_ queue_ fixity Fixity Check Cron job that queues Generic Files for scheduled fixity checks. Fixity Checker apt_ fixity Fixity Check Worker that permforms scheduled fixity checks. Glacier Restorer glacier_ restorer Restoration Moves files from Glacier and Glacier Deep Archive into S3 so they can be restored. File Restorer file_restorer Restoration Restores individual files to depositor restoration buckets. Bag Restorer bag_ restorer Restoration Restores entire bags (intellectual objects) to depositor restoration buckets. Deletion Worker apt_delete Deletion Permanently deletes files and objects from preservation storage. APT Queue apt_queue Restoration and Deletion Queues deletion and restoration requests created by Registry users. Those requests should be queued automatically by Registry itself. If they're not, apt_queue will find them. This cron job is a vestige from the old, unreliable Pharos system, which did occasionally fail at queueing requests. It may no longer be needed, but we'll keep it around as a failsafe.","title":"List of Containers"},{"location":"overview/","text":"System Overview Preservation services consists of the following components: A set of Docker containers, each running a microservice to handle various aspects of the ingest, restoration, deletion and fixity processes. NSQ - a queue service for orchestrating work items. Redis/Elasticache - for sharing interim processing data between workers. S3, Glacier and Wasabi for storage The Registry REST API for storing and retrieving persistent metadata about the content of our preservation repository. While other parts of this documentation describe the components in detail, this page provides a graphical overview of the system's components, where they live, and how they communicate. The High Level The components of the system are divided into three privilege zones, with privileges enforced by IAM roles and policies. Depositors have access to the leftmost zone in the diagram below. They can upload files into their own receiving buckets for ingest, and download them from their own restoration buckets for retrieval. Depositors can access only their own buckets, no one else's. The rightmost zone is preservation storage. No one can access this except the IAM account of the Go workers, which has full access, and APTrust admins, who have read-only access. The Go service workers move items from depositor receiving buckets on the left into long-term preservation storage buckets on the right. They also move items from long-term storage back to depositor restoration buckets. The workers keep temporary housekeeping data about items in process in Redis/Elasticache, and store permanent data in the Registry. All of the working components of the system--all services using CPU and memory--live in the middle zone. Depositors cannot access anything in the middle zone. APTrust administrators can, generally through the AWS console and CloudFormation templates. The diagram below shows all of the components in their respective zones. Details follow below. Note These diagrams omit three containers that run cron jobs. The bucket reader, ingest_bucket_reader , scans depositor receiving buckets for new ingests. It creates an ingest WorkItem for each new bag and queues it in the NSQ ingest01_prefetch topic. The apt_queue_fixity worker runs every half hour or so, queueing files for fixity checks. Files stored in S3 and Wasabi are checked every 90 days. The apt_queue cron job queues deletion and restoration requests created by Registry users. Those requests should be queued automatically by Registry itself. If they're not, apt_queue will find them. This cron job is a vestige from the old, unreliable Pharos system, which did occasionally fail at queueing requests. It may no longer be needed, but we'll keep it around as a failsafe. Ingest During the ingest process, depositors upload bags (tar files) to their receiving buckets. A cron scans for new bags, creating a WorkItem in the Registry for each one, and then putting that WorkItem's ID into NSQ for the Ingest workers to pick up. Ingest workers perform a number of functions, including bag validation, format identification, and more. Then they move files into preservation storage and record the results of their work in the Registry. Note In the diagrams below, all ingest workers read to and write from both Redis and NSQ. Arrows are omitted to minimize clutter. NSQ NSQ keeps track of which tasks need to go to which workers. For Ingest, a cron job running in the ingest_bucket_reader container creates and queues new WorkItems for each new bag that shows up in a receiving bucket. For deletion and restoration, users make requests through the Registry Web UI, and Registry pushes the WorkItem IDs into NSQ's deletion and restoration topics. For fixity checks, a cron job running in apt_queue_fixity queries Registry for files in S3 or Wasabi storage that have not had a fixity check in 90 days. It pushes the IDs of those files into NSQ, usually in batches of 2,500-3,000 every half hour. Redis/Elasticache Redis stores info about what work has already been done on any task in the ingest process. New workers can pick up partially completed tasks and pick up work where the last worker left off. The housekeeping data in Redis ensures two things: that no work is skipped or left incomplete that completed work is not unnecessarily repeated The Pre-Fetch Worker The pre-fetch worker streams a bag from a depositor's receiving bucket through a series of functions to do the following: Gather a list of files in the bag. Calculated fixity values on those files. Extract tag files, manifests, and tag manifests to an S3 staging bucket for later processing. The Validation Worker The validation worker ensures a bag is valid according to its BagIt profile. APTrust currently supports APTrust and BTR (Beyond the Repository) BagIt formats. A bag is valid if it contains all of the payload files mentioned in the manifests, and if the checksums of those files match the checksums in the manifests. The bag must also contain a set of BagIt standard tags with valid values, plus whatever tags are required by the specified BagIt profile. Note that profiles also specify which checksum algorithms and required and which others are permitted. The validator enforces those rules as well. If a bag is incomplete, if it contains extraneous payload files not mentioned in the manifests, or if it is missing required tags or required checksum algoriths, it will fail validation, the system will not ingest it, and processing stops here. The validation worker tends to complete its work quickly, generally in a fraction of a second. It simply analyzes all of the metadata stored in Redis by the pre-fetch worker, which as already done the hard work of inventorying contents and calculating checksums. Reingest Check The reingest checker asks Registry if this bag has ever been ingested before. Bags have unique identifiers composed of the identifier of the owning instituion, followed by a slash and the name of the bag, minus the .tar extension. For example, if the University of Virginia uploads a bag called photos.tar , the bag's Intellectual Object Identifier becomes virginia.edu/photos . If the Registry has an active (non-deleted) record of an object whose identifier matches the one we're currently ingesting, it checks each file in the new bag against records in the registry. If the file exists in Registry, the worker compares the Registry checksum to the checksum calculated by the pre-fetch worker. If the checksum has changed, or if the file does not exist in Registry, the worker flags the file as needing to be saved to preservation storage. If Registry already has a record of this file and the checksum has not changed, the file will be flagged as not needing to be copied to preservation. The reingest checker prevents us from storing duplicate copies of existing files under new UUIDs. (Everything in preservation storage is stored with a UUID key, not its original file name.) Staging Uploader The staging uploader unpacks bags from depositor ingest buckets and stores their individual files in an S3 staging bucket. Files in the staging bucket are stored with a key composed of WorkItem ID and file UUID. E.g. 34546/ddeff40b-f995-42f0-b002-389cf331193d . Subsequent workers that need to access these files can retrieve them from this staging bucket. S3 works well for write-once/read-multiple workloads. Because it's a network resource, not tied to local block storage, its contents are available to all of our ingest workers. Format Identifier The format identifier streams files from the staging bucket through a set of algorithms that compare the file's bits to known signatures in the PRONOM registry. It updates each file record in Redis with the file's mime type. Preservation Uploader The preservation upload copies files from the staging bucket to preservation storage. Preservation Verifier The preservation verifier performs a HEAD request on every item copied into preservation storage to retrieve its metadata. Where possible, it compares each file's etag in preservation storage to its md5 checksum to ensure integrity. Etag comparison isn't possible on larger files, so it compares the file size in preservation with its know file size. The verifier is basically a sanity check to provide basic assurance that files were successfully copied to preservation. Why Does the Preservation Verifier Exist? We added this step to our older ingest system in 2017 after a bug in the AWS S3 client caused a number of files to be written to S3 with zero bytes. The zero-byte bug persisted for years. (APTrust has some internal documentation on this issue.) To work around it, our old preservation uploader started asking S3, immediately after upload, how many bytes it had just sent. Oddly enough, whenever S3 received zero bytes, it would reply that it had received 8 TB. (AWS refutes this, but we have literally tens of thousands of log entries refuting their refutation.) The old preservation uploader would rewrite the file to S3 whenever S3 returned a different number of bytes than what we knew we had written. The current ingest system uses the Minio S3 client, which has never shown the zero-byte bug, but we wanted to keep this sanity check in place after experiencing the cost of not having it. Recorder The recorder records metadata about the ingest in the Registry, including info about the Intellectual Object and all of its files. For each file, it records checksums, storage records and a set of ingest Premis events. It also records Premis events for the object. Cleanup The cleanup worker deletes the original bag from the depositor's receiving bucket, then deletes all interim processing data from the S3 staging bucket and Redis. Finally, it marks the ingest as complete in the Registry and in NSQ. Restoration APTrust users initiate object and file restoration through the Registry Web UI. They click a button to restore either an individual file, or an entire intellectual object (a bag). Clicking the button causes Registry to create a restoration WorkItem, and to add the WorkItem ID to one of three NSQ topics: restore_object is the topic for object restoration. restore_file is for file restoration. restore_glacier is for Glacier restoration. Glacier restoration requires an extra step of moving files from an inaccessible Glacier vault into an accessible S3 bucket so the files can be restored. Once the Glacier restorer gets the necessary files into an accessible S3 bucket, it pushes the restoration request into the normal restore_object or restore_file topics. File restorations involve copying only a single file into a depositor's restoration bucket. Object restorations require gathering and re-bagging sets of files and moving the whole tarred bag into the restoration bucket. While the file restorer ensures a file's checksums are correct before making a restoration copy, the object restorer reassembles and validates an entire bag full of files. It does this on the fly, streaming files into a bag in the restoration bucket without those files ever touching local disk. It calculates checksums as it goes, ensuring the bag is valid and complete. If the bag is not valid or is incomplete, the restoration WorkItem will be marked as failed in the Registry. Note that because we rebuild bags when we restore them, the restored bag will not exactly match the original bag. The payload should be the same (unless the depositor deleted some files before the restoration), but the order of files within the tar archive may differ. In addition, we store the bag-info.txt, aptrust-info.txt and any other files outside the bag's payload directory, except bagit.txt and fetch.txt. We give these back to the depositor upon restoration. Depositors have been storing essential metadata in bag-info.txt, aptrust-info.txt and other non-payload files, and they want that info back when they do a restoration. Deletion Deletions are also initiated in the Registry. Once a deletion request has been created and approved by an institutional admin, Registry creates a deletion WorkItem and queues it in NSQ's delete_item topic. For file deletions, the deletion worker deletes all copies of the specified file from all known locations. That's typically either one S3 bucket, one Glacier vault, or one Wasabi bucket. Items in Standard storage are deleted from both S3 in Virginia and Glacier in Oregon. For object deletions, the deletion worker deletes all copies of all files that belong to the specified object. After deletion, the worker records deletion Premis events for all of the deleted files, changes the state of those files from A (active) to D (deleted) in the Registry, and marks the WorkItem as done. For object deletions, the worker changes the object state to D and records a deletion Premis event for the object. Bulk Deletion Bulk deletion is a new feature, as of early 2024. We do not expose this feature to depositors because of its potential danger. If you would like to initial a bulk deletion, please contact help@aptrust.org . The basic process goes like this: You send APTrust a list of object identifiers. These identify objects you want to delete. APTrust create a set of deletion requests on your behalf. Registry sends a deletion approval email to administrators at your institution showing what is to be deleted and requesting they approve or deny the request. Once an administrator approves the deletion, preservation services begins deleting the objects. We typically delete objects in batches of 500-1000 at a time. Therefore, a request to delete 2000 objects would likely result in two bulk deletions with two separate emails requiring approval. Fixity Checks A containerized cron job called apt_queue_fixity runs every half hour or so, asking the Registry for a list of files stored in S3 or Wasabi that have not had a fixity check in the past 90 days. We do not check fixity on items in Glacier or Glacier Deep storage. apt_queue_fixity pushes the Generic File IDs of files needing a fixity check into the fixity_check queue, usually in batches of 2,500 or 3,000. The fixity checker calculates checksums on the bitstream coming from S3 or Wasabi. The files never touch local disk. It records the results of each fixity check in a Premis event. The fixity checker is the only worker whose tasks are not recorded in WorkItems. It would be excessive to do so, since the system has performed over a hundred million checks, and all the data we need to capture from a check is captured in the Premis event.","title":"Overview"},{"location":"overview/#system-overview","text":"Preservation services consists of the following components: A set of Docker containers, each running a microservice to handle various aspects of the ingest, restoration, deletion and fixity processes. NSQ - a queue service for orchestrating work items. Redis/Elasticache - for sharing interim processing data between workers. S3, Glacier and Wasabi for storage The Registry REST API for storing and retrieving persistent metadata about the content of our preservation repository. While other parts of this documentation describe the components in detail, this page provides a graphical overview of the system's components, where they live, and how they communicate.","title":"System Overview"},{"location":"overview/#the-high-level","text":"The components of the system are divided into three privilege zones, with privileges enforced by IAM roles and policies. Depositors have access to the leftmost zone in the diagram below. They can upload files into their own receiving buckets for ingest, and download them from their own restoration buckets for retrieval. Depositors can access only their own buckets, no one else's. The rightmost zone is preservation storage. No one can access this except the IAM account of the Go workers, which has full access, and APTrust admins, who have read-only access. The Go service workers move items from depositor receiving buckets on the left into long-term preservation storage buckets on the right. They also move items from long-term storage back to depositor restoration buckets. The workers keep temporary housekeeping data about items in process in Redis/Elasticache, and store permanent data in the Registry. All of the working components of the system--all services using CPU and memory--live in the middle zone. Depositors cannot access anything in the middle zone. APTrust administrators can, generally through the AWS console and CloudFormation templates. The diagram below shows all of the components in their respective zones. Details follow below. Note These diagrams omit three containers that run cron jobs. The bucket reader, ingest_bucket_reader , scans depositor receiving buckets for new ingests. It creates an ingest WorkItem for each new bag and queues it in the NSQ ingest01_prefetch topic. The apt_queue_fixity worker runs every half hour or so, queueing files for fixity checks. Files stored in S3 and Wasabi are checked every 90 days. The apt_queue cron job queues deletion and restoration requests created by Registry users. Those requests should be queued automatically by Registry itself. If they're not, apt_queue will find them. This cron job is a vestige from the old, unreliable Pharos system, which did occasionally fail at queueing requests. It may no longer be needed, but we'll keep it around as a failsafe.","title":"The High Level"},{"location":"overview/#ingest","text":"During the ingest process, depositors upload bags (tar files) to their receiving buckets. A cron scans for new bags, creating a WorkItem in the Registry for each one, and then putting that WorkItem's ID into NSQ for the Ingest workers to pick up. Ingest workers perform a number of functions, including bag validation, format identification, and more. Then they move files into preservation storage and record the results of their work in the Registry. Note In the diagrams below, all ingest workers read to and write from both Redis and NSQ. Arrows are omitted to minimize clutter.","title":"Ingest"},{"location":"overview/#nsq","text":"NSQ keeps track of which tasks need to go to which workers. For Ingest, a cron job running in the ingest_bucket_reader container creates and queues new WorkItems for each new bag that shows up in a receiving bucket. For deletion and restoration, users make requests through the Registry Web UI, and Registry pushes the WorkItem IDs into NSQ's deletion and restoration topics. For fixity checks, a cron job running in apt_queue_fixity queries Registry for files in S3 or Wasabi storage that have not had a fixity check in 90 days. It pushes the IDs of those files into NSQ, usually in batches of 2,500-3,000 every half hour.","title":"NSQ"},{"location":"overview/#rediselasticache","text":"Redis stores info about what work has already been done on any task in the ingest process. New workers can pick up partially completed tasks and pick up work where the last worker left off. The housekeeping data in Redis ensures two things: that no work is skipped or left incomplete that completed work is not unnecessarily repeated","title":"Redis/Elasticache"},{"location":"overview/#the-pre-fetch-worker","text":"The pre-fetch worker streams a bag from a depositor's receiving bucket through a series of functions to do the following: Gather a list of files in the bag. Calculated fixity values on those files. Extract tag files, manifests, and tag manifests to an S3 staging bucket for later processing.","title":"The Pre-Fetch Worker"},{"location":"overview/#the-validation-worker","text":"The validation worker ensures a bag is valid according to its BagIt profile. APTrust currently supports APTrust and BTR (Beyond the Repository) BagIt formats. A bag is valid if it contains all of the payload files mentioned in the manifests, and if the checksums of those files match the checksums in the manifests. The bag must also contain a set of BagIt standard tags with valid values, plus whatever tags are required by the specified BagIt profile. Note that profiles also specify which checksum algorithms and required and which others are permitted. The validator enforces those rules as well. If a bag is incomplete, if it contains extraneous payload files not mentioned in the manifests, or if it is missing required tags or required checksum algoriths, it will fail validation, the system will not ingest it, and processing stops here. The validation worker tends to complete its work quickly, generally in a fraction of a second. It simply analyzes all of the metadata stored in Redis by the pre-fetch worker, which as already done the hard work of inventorying contents and calculating checksums.","title":"The Validation Worker"},{"location":"overview/#reingest-check","text":"The reingest checker asks Registry if this bag has ever been ingested before. Bags have unique identifiers composed of the identifier of the owning instituion, followed by a slash and the name of the bag, minus the .tar extension. For example, if the University of Virginia uploads a bag called photos.tar , the bag's Intellectual Object Identifier becomes virginia.edu/photos . If the Registry has an active (non-deleted) record of an object whose identifier matches the one we're currently ingesting, it checks each file in the new bag against records in the registry. If the file exists in Registry, the worker compares the Registry checksum to the checksum calculated by the pre-fetch worker. If the checksum has changed, or if the file does not exist in Registry, the worker flags the file as needing to be saved to preservation storage. If Registry already has a record of this file and the checksum has not changed, the file will be flagged as not needing to be copied to preservation. The reingest checker prevents us from storing duplicate copies of existing files under new UUIDs. (Everything in preservation storage is stored with a UUID key, not its original file name.)","title":"Reingest Check"},{"location":"overview/#staging-uploader","text":"The staging uploader unpacks bags from depositor ingest buckets and stores their individual files in an S3 staging bucket. Files in the staging bucket are stored with a key composed of WorkItem ID and file UUID. E.g. 34546/ddeff40b-f995-42f0-b002-389cf331193d . Subsequent workers that need to access these files can retrieve them from this staging bucket. S3 works well for write-once/read-multiple workloads. Because it's a network resource, not tied to local block storage, its contents are available to all of our ingest workers.","title":"Staging Uploader"},{"location":"overview/#format-identifier","text":"The format identifier streams files from the staging bucket through a set of algorithms that compare the file's bits to known signatures in the PRONOM registry. It updates each file record in Redis with the file's mime type.","title":"Format Identifier"},{"location":"overview/#preservation-uploader","text":"The preservation upload copies files from the staging bucket to preservation storage.","title":"Preservation Uploader"},{"location":"overview/#preservation-verifier","text":"The preservation verifier performs a HEAD request on every item copied into preservation storage to retrieve its metadata. Where possible, it compares each file's etag in preservation storage to its md5 checksum to ensure integrity. Etag comparison isn't possible on larger files, so it compares the file size in preservation with its know file size. The verifier is basically a sanity check to provide basic assurance that files were successfully copied to preservation.","title":"Preservation Verifier"},{"location":"overview/#why-does-the-preservation-verifier-exist","text":"We added this step to our older ingest system in 2017 after a bug in the AWS S3 client caused a number of files to be written to S3 with zero bytes. The zero-byte bug persisted for years. (APTrust has some internal documentation on this issue.) To work around it, our old preservation uploader started asking S3, immediately after upload, how many bytes it had just sent. Oddly enough, whenever S3 received zero bytes, it would reply that it had received 8 TB. (AWS refutes this, but we have literally tens of thousands of log entries refuting their refutation.) The old preservation uploader would rewrite the file to S3 whenever S3 returned a different number of bytes than what we knew we had written. The current ingest system uses the Minio S3 client, which has never shown the zero-byte bug, but we wanted to keep this sanity check in place after experiencing the cost of not having it.","title":"Why Does the Preservation Verifier Exist?"},{"location":"overview/#recorder","text":"The recorder records metadata about the ingest in the Registry, including info about the Intellectual Object and all of its files. For each file, it records checksums, storage records and a set of ingest Premis events. It also records Premis events for the object.","title":"Recorder"},{"location":"overview/#cleanup","text":"The cleanup worker deletes the original bag from the depositor's receiving bucket, then deletes all interim processing data from the S3 staging bucket and Redis. Finally, it marks the ingest as complete in the Registry and in NSQ.","title":"Cleanup"},{"location":"overview/#restoration","text":"APTrust users initiate object and file restoration through the Registry Web UI. They click a button to restore either an individual file, or an entire intellectual object (a bag). Clicking the button causes Registry to create a restoration WorkItem, and to add the WorkItem ID to one of three NSQ topics: restore_object is the topic for object restoration. restore_file is for file restoration. restore_glacier is for Glacier restoration. Glacier restoration requires an extra step of moving files from an inaccessible Glacier vault into an accessible S3 bucket so the files can be restored. Once the Glacier restorer gets the necessary files into an accessible S3 bucket, it pushes the restoration request into the normal restore_object or restore_file topics. File restorations involve copying only a single file into a depositor's restoration bucket. Object restorations require gathering and re-bagging sets of files and moving the whole tarred bag into the restoration bucket. While the file restorer ensures a file's checksums are correct before making a restoration copy, the object restorer reassembles and validates an entire bag full of files. It does this on the fly, streaming files into a bag in the restoration bucket without those files ever touching local disk. It calculates checksums as it goes, ensuring the bag is valid and complete. If the bag is not valid or is incomplete, the restoration WorkItem will be marked as failed in the Registry. Note that because we rebuild bags when we restore them, the restored bag will not exactly match the original bag. The payload should be the same (unless the depositor deleted some files before the restoration), but the order of files within the tar archive may differ. In addition, we store the bag-info.txt, aptrust-info.txt and any other files outside the bag's payload directory, except bagit.txt and fetch.txt. We give these back to the depositor upon restoration. Depositors have been storing essential metadata in bag-info.txt, aptrust-info.txt and other non-payload files, and they want that info back when they do a restoration.","title":"Restoration"},{"location":"overview/#deletion","text":"Deletions are also initiated in the Registry. Once a deletion request has been created and approved by an institutional admin, Registry creates a deletion WorkItem and queues it in NSQ's delete_item topic. For file deletions, the deletion worker deletes all copies of the specified file from all known locations. That's typically either one S3 bucket, one Glacier vault, or one Wasabi bucket. Items in Standard storage are deleted from both S3 in Virginia and Glacier in Oregon. For object deletions, the deletion worker deletes all copies of all files that belong to the specified object. After deletion, the worker records deletion Premis events for all of the deleted files, changes the state of those files from A (active) to D (deleted) in the Registry, and marks the WorkItem as done. For object deletions, the worker changes the object state to D and records a deletion Premis event for the object.","title":"Deletion"},{"location":"overview/#bulk-deletion","text":"Bulk deletion is a new feature, as of early 2024. We do not expose this feature to depositors because of its potential danger. If you would like to initial a bulk deletion, please contact help@aptrust.org . The basic process goes like this: You send APTrust a list of object identifiers. These identify objects you want to delete. APTrust create a set of deletion requests on your behalf. Registry sends a deletion approval email to administrators at your institution showing what is to be deleted and requesting they approve or deny the request. Once an administrator approves the deletion, preservation services begins deleting the objects. We typically delete objects in batches of 500-1000 at a time. Therefore, a request to delete 2000 objects would likely result in two bulk deletions with two separate emails requiring approval.","title":"Bulk Deletion"},{"location":"overview/#fixity-checks","text":"A containerized cron job called apt_queue_fixity runs every half hour or so, asking the Registry for a list of files stored in S3 or Wasabi that have not had a fixity check in the past 90 days. We do not check fixity on items in Glacier or Glacier Deep storage. apt_queue_fixity pushes the Generic File IDs of files needing a fixity check into the fixity_check queue, usually in batches of 2,500 or 3,000. The fixity checker calculates checksums on the bitstream coming from S3 or Wasabi. The files never touch local disk. It records the results of each fixity check in a Premis event. The fixity checker is the only worker whose tasks are not recorded in WorkItems. It would be excessive to do so, since the system has performed over a hundred million checks, and all the data we need to capture from a check is captured in the Premis event.","title":"Fixity Checks"},{"location":"settings/","text":"Settings Settings Files For developer and test setups, we store settings in simple .env files that each worker reads at startup. You'll notice files like .env.test in Preserv's source repo. You can tell workers which .env file to read by specifying APT_ENV on the command line. For example, APT_ENV=test apt_fixity would run the apt_fixity worker using the configuration settings in .env.test. If you want to read settings from .env.local, run APT_ENV=local apt_fixity . Settings Injection Through AWS Parameter Store For our staging, demo, and production environments, we ship an empty .env file inside the Docker container. Before Amazon's ECS service starts a new container instance, it runs a \"sidecar\" app that pulls variables from AWS parameter store into environment variables inside each container. The code in config.go uses Viper's AutomaticEnv method to pull in environment variables. Any variables not defined in the .env file will be fetched from the environment, if present. This allows us to use simple .env config files on developer machines and in CI tests, without exposing sensitive information in our public Git repos. For staging, demo, and production systems, all config settings are centralized in Parameter Store. Note that variable names in Parameter Store follow the pattern ENV/PRESERVE/VAR_NAME , where ENV is the name of the environment (staging, demo, production) and VAR_NAME is the setting name. For example, the name of the variable containing the demo NSQ url is /DEMO/PRESERV/NSQ_URL . Definitions The definitions below pertain to all Preserv workers. In addition to these, each worker has specific settings describing number of workers and buffer size. Those are described on worker-specific pages. In general, settings ending in WORKERS describe how many go routines (concurrent processes) a worker should run. Settings ending in BUFFER_SIZE describe the desired size of internal queue buffers. On a macro level, the buffer size settings tell the work how many items to accept from NSQ in each batch. If a worker has two WORKERS and a BUFFER_SIZE of 20, each go routine will accept up to 20 items at a time from NSQ. The Docker container then can be working on up to 40 NSQ items at a time. Variable Name Definition BUCKET_GLACIER_DEEP_OH The name of the Glacier Deep preservation bucket in Ohio. This is for storage option Glacier-Deep-OH. BUCKET_GLACIER_DEEP_OR The name of the Glacier Deep preservation bucket in Oregon. This is for storage option Glacier-Deep-OR. BUCKET_GLACIER_DEEP_VA The name of the Glacier Deep preservation bucket in Virginia. This is for storage option Glacier-Deep-VA. BUCKET_GLACIER_OH The name of the Glacier preservation bucket in Ohio. This is for storage option Glacier-OH. BUCKET_GLACIER_OR The name of the Glacier preservation bucket in Oregon. This is for storage option Glacier-OR and Standard. BUCKET_GLACIER_VA The name of the Glacier preservation bucket in Virginia. This is for storage option Glacier-VA. BUCKET_STANDARD_VA The name of the S3 bucket for standard storage in Virginia. This is for storage option Standard. BUCKET_WASABI_OR The name of the Wasabi preservation bucket in Oregon. BUCKET_WASABI_VA The name of the Wasabi preservation bucket in Virginia. MAX_DAYS_SINCE_LAST_FIXITY The interval at which the fixity checker should check files. In production and demo, this is set to 90, so that we run fixity checks every 90 days. In staging, you can set this down to one day if you want to force fixity checks to run. Typical staging setting is 14. MAX_FIXITY_ITEMS_PER_RUN The maximum number of files that the queue_fixity worker should queue on each run. The default value is 2500. NSQ_LOOKUP The hostname and port of the NSQ lookup daemon. This usually has a format like hostname:port or ip_addr:port. The lookup daemon tells NSQ clients (our workers) how to connect to any and all available NSQ instances. NSQ_URL The hostname and port of our NSQ service. PRESERV_REGISTRY_API_KEY The API key workers use to access the Registry. PRESERV_REGISTRY_API_USER The API user email address that our workers use to access the Registry. This account must have the APTrust admin role, as it accesses the admin API. PRESERV_REGISTRY_URL The URL of the Registry. Workers read and write WorkItems and other data in this Registry. QUEUE_FIXITY_INTERVAL This describes how often, in minutes, the queue fixity worker should check for files requiring fixity checks. We usually set this to 30. REDIS_URL The Redis (or Elasitiche) URL. Ingest workers connect to this service to store, retrieve and update interim processing data. On dev and CI machines, we use Redis. In AWS environments, we use Elasticache. S3_AWS_HOST The generic hostname for AWS S3: s3.amazonsws.com. S3_AWS_KEY The Access Key ID used to access items in S3 buckets and in Glacier. This account should have full privileges in S3 and Glacier. S3_AWS_SECRET The AWS Secret Access Key used to interact with S3 and Glacier. S3_WASABI_HOST_OR The hostname of Wasabi's Oregon S3 service. s3.us-west-1.wasabisys.com S3_WASABI_HOST_VA The hostname of Wasabi's Virginia S3 service. s3.us-east-1.wasabisys.com S3_WASABI_KEY The Access Key ID used to access Wasabi buckets. S3_WASABI_SECRET The Secret Access Key used to access Wasabi buckets. STAGING_BUCKET The name of the AWS S3 bucket into which the staging uploader copies the files it unpacks from tarred bags in the receiving buckets. The format identifier and other workers will access files in this bucket during later stages of ingest.","title":"Settings"},{"location":"settings/#settings","text":"","title":"Settings"},{"location":"settings/#settings-files","text":"For developer and test setups, we store settings in simple .env files that each worker reads at startup. You'll notice files like .env.test in Preserv's source repo. You can tell workers which .env file to read by specifying APT_ENV on the command line. For example, APT_ENV=test apt_fixity would run the apt_fixity worker using the configuration settings in .env.test. If you want to read settings from .env.local, run APT_ENV=local apt_fixity .","title":"Settings Files"},{"location":"settings/#settings-injection-through-aws-parameter-store","text":"For our staging, demo, and production environments, we ship an empty .env file inside the Docker container. Before Amazon's ECS service starts a new container instance, it runs a \"sidecar\" app that pulls variables from AWS parameter store into environment variables inside each container. The code in config.go uses Viper's AutomaticEnv method to pull in environment variables. Any variables not defined in the .env file will be fetched from the environment, if present. This allows us to use simple .env config files on developer machines and in CI tests, without exposing sensitive information in our public Git repos. For staging, demo, and production systems, all config settings are centralized in Parameter Store. Note that variable names in Parameter Store follow the pattern ENV/PRESERVE/VAR_NAME , where ENV is the name of the environment (staging, demo, production) and VAR_NAME is the setting name. For example, the name of the variable containing the demo NSQ url is /DEMO/PRESERV/NSQ_URL .","title":"Settings Injection Through AWS Parameter Store"},{"location":"settings/#definitions","text":"The definitions below pertain to all Preserv workers. In addition to these, each worker has specific settings describing number of workers and buffer size. Those are described on worker-specific pages. In general, settings ending in WORKERS describe how many go routines (concurrent processes) a worker should run. Settings ending in BUFFER_SIZE describe the desired size of internal queue buffers. On a macro level, the buffer size settings tell the work how many items to accept from NSQ in each batch. If a worker has two WORKERS and a BUFFER_SIZE of 20, each go routine will accept up to 20 items at a time from NSQ. The Docker container then can be working on up to 40 NSQ items at a time. Variable Name Definition BUCKET_GLACIER_DEEP_OH The name of the Glacier Deep preservation bucket in Ohio. This is for storage option Glacier-Deep-OH. BUCKET_GLACIER_DEEP_OR The name of the Glacier Deep preservation bucket in Oregon. This is for storage option Glacier-Deep-OR. BUCKET_GLACIER_DEEP_VA The name of the Glacier Deep preservation bucket in Virginia. This is for storage option Glacier-Deep-VA. BUCKET_GLACIER_OH The name of the Glacier preservation bucket in Ohio. This is for storage option Glacier-OH. BUCKET_GLACIER_OR The name of the Glacier preservation bucket in Oregon. This is for storage option Glacier-OR and Standard. BUCKET_GLACIER_VA The name of the Glacier preservation bucket in Virginia. This is for storage option Glacier-VA. BUCKET_STANDARD_VA The name of the S3 bucket for standard storage in Virginia. This is for storage option Standard. BUCKET_WASABI_OR The name of the Wasabi preservation bucket in Oregon. BUCKET_WASABI_VA The name of the Wasabi preservation bucket in Virginia. MAX_DAYS_SINCE_LAST_FIXITY The interval at which the fixity checker should check files. In production and demo, this is set to 90, so that we run fixity checks every 90 days. In staging, you can set this down to one day if you want to force fixity checks to run. Typical staging setting is 14. MAX_FIXITY_ITEMS_PER_RUN The maximum number of files that the queue_fixity worker should queue on each run. The default value is 2500. NSQ_LOOKUP The hostname and port of the NSQ lookup daemon. This usually has a format like hostname:port or ip_addr:port. The lookup daemon tells NSQ clients (our workers) how to connect to any and all available NSQ instances. NSQ_URL The hostname and port of our NSQ service. PRESERV_REGISTRY_API_KEY The API key workers use to access the Registry. PRESERV_REGISTRY_API_USER The API user email address that our workers use to access the Registry. This account must have the APTrust admin role, as it accesses the admin API. PRESERV_REGISTRY_URL The URL of the Registry. Workers read and write WorkItems and other data in this Registry. QUEUE_FIXITY_INTERVAL This describes how often, in minutes, the queue fixity worker should check for files requiring fixity checks. We usually set this to 30. REDIS_URL The Redis (or Elasitiche) URL. Ingest workers connect to this service to store, retrieve and update interim processing data. On dev and CI machines, we use Redis. In AWS environments, we use Elasticache. S3_AWS_HOST The generic hostname for AWS S3: s3.amazonsws.com. S3_AWS_KEY The Access Key ID used to access items in S3 buckets and in Glacier. This account should have full privileges in S3 and Glacier. S3_AWS_SECRET The AWS Secret Access Key used to interact with S3 and Glacier. S3_WASABI_HOST_OR The hostname of Wasabi's Oregon S3 service. s3.us-west-1.wasabisys.com S3_WASABI_HOST_VA The hostname of Wasabi's Virginia S3 service. s3.us-east-1.wasabisys.com S3_WASABI_KEY The Access Key ID used to access Wasabi buckets. S3_WASABI_SECRET The Secret Access Key used to access Wasabi buckets. STAGING_BUCKET The name of the AWS S3 bucket into which the staging uploader copies the files it unpacks from tarred bags in the receiving buckets. The format identifier and other workers will access files in this bucket during later stages of ingest.","title":"Definitions"},{"location":"structure/","text":"Code Structure The preservation services code repository includes the following directories and files: Name Description .github Contains config files for GitHub services like Dependabot. apps Contains Go source files with a main() function that can be compiled into executable binaries. There's one directory and one file for each app. audit Contains code to run a quick, lightweight audit of files in preservation storage. bagit Contains code used to build, parse, and validate BagIt packages. bin Contains Mac and Linux binaries for Minio, NSQ, and Redis. These are used in integration and end-to-end (e2e) tests. cfn Contains a template ( cfn-preserv-cluster.tmpl ) and a yaml file ( cfn-preserv-cluster.yml ) for deploying preservation services to AWS's Fargate/ECS service. constants Contains constants used throughout the codebase. deletion Contains code used by the deletion worker to permenantly remove files from preservation storage. docker Probably no longer used. Consider deleting this. e2e Contains end-to-end tests. See End to End Tests . fixity Contains code used by the fixity worker to run fixity checks. ingest Contains code used by ingest workers to ingest bags. models Contains a number of data models. See the following three entries for details. models/common Contains utility models used throughout preservation services. The most important of these are Config , which provides access to configuration settings and Context , which provides access to essential services such as logging, Registry, Redis, NSQ, S3, Glacier and Wasabi. models/registry Contains models that match core Registry models (intellectual object, generic file, work item, checksum, etc.) These models include extra fields for metadata used during the ingest process that Registry itself doesn't care about. Data in these models is stored in Redis, so it can be shared among workers. models/service Contains models specific to preservation services' internal processing. Data in these models is stored in Redis, so it can be shared among workers. network Contains code to access external network services, including NSQ, Redis, Registry and Glacier. platform Includes platform-specific code for Posix and Windows operating systems. (This code is currently unnecessary, but may become necessary if we decide to share bagging and validation code with Windows in the future.) profiles Contains BagIt profiles supported by Preserv's ingest process. Currently, that's limited to the latest versions of the APTrust and BTR (Beyond the Repository) profiles. The profiles are in DART format, which is richer and more specific than the standard format. DART can convert these back to standard format using its export fuction. This directory also contains the default.sig file used by Siegfried, which is the core of the format identification worker. restoration Contains code used by the Glacier, file, and object restoration workers. scripts Contains two Ruby scripts. build.rb builds the entire suite of Preserv apps, writing the executables into bin/go-bin . test.rb runs unit, integration, and end-to-end tests, which require spinning up a number of locally running external services. See the testing documentation for details. testdata Contains a number of files used in unit/integration/e2e tests. These include files to be bagged, bags to be validated, and JSON files in e2e_results to be matched against the expected outcomes of end-to-end tests. util Contains utility functions used throughout the codebase. workers Contains code to harness the contents of the deletion, fixity, ingest, and restoration directories into usable, NSQ-connected workers. These workers are then loaded by the apps in the apps directory. See Anatomy of a Worker for details on how these pieces fit together. .env These are settings files for different environments (dev, test, integration, etc.) See Settings or the comments in the files themselves for info about which settings are available and what they mean. Dockerfile.build This file contains instructions for building Docker containers in which to run preservation services. Use make to build the containers, as described on the Docker page. Makefile Includes commands to build and publish the Docker containers, and to update the CloudFormation template. See Docker docker-compose.yml This is an historical artifact used in early, proof-of-concept Docker builds. We may use it as a refence if we move to Kubernetes.","title":"Code Structure"},{"location":"structure/#code-structure","text":"The preservation services code repository includes the following directories and files: Name Description .github Contains config files for GitHub services like Dependabot. apps Contains Go source files with a main() function that can be compiled into executable binaries. There's one directory and one file for each app. audit Contains code to run a quick, lightweight audit of files in preservation storage. bagit Contains code used to build, parse, and validate BagIt packages. bin Contains Mac and Linux binaries for Minio, NSQ, and Redis. These are used in integration and end-to-end (e2e) tests. cfn Contains a template ( cfn-preserv-cluster.tmpl ) and a yaml file ( cfn-preserv-cluster.yml ) for deploying preservation services to AWS's Fargate/ECS service. constants Contains constants used throughout the codebase. deletion Contains code used by the deletion worker to permenantly remove files from preservation storage. docker Probably no longer used. Consider deleting this. e2e Contains end-to-end tests. See End to End Tests . fixity Contains code used by the fixity worker to run fixity checks. ingest Contains code used by ingest workers to ingest bags. models Contains a number of data models. See the following three entries for details. models/common Contains utility models used throughout preservation services. The most important of these are Config , which provides access to configuration settings and Context , which provides access to essential services such as logging, Registry, Redis, NSQ, S3, Glacier and Wasabi. models/registry Contains models that match core Registry models (intellectual object, generic file, work item, checksum, etc.) These models include extra fields for metadata used during the ingest process that Registry itself doesn't care about. Data in these models is stored in Redis, so it can be shared among workers. models/service Contains models specific to preservation services' internal processing. Data in these models is stored in Redis, so it can be shared among workers. network Contains code to access external network services, including NSQ, Redis, Registry and Glacier. platform Includes platform-specific code for Posix and Windows operating systems. (This code is currently unnecessary, but may become necessary if we decide to share bagging and validation code with Windows in the future.) profiles Contains BagIt profiles supported by Preserv's ingest process. Currently, that's limited to the latest versions of the APTrust and BTR (Beyond the Repository) profiles. The profiles are in DART format, which is richer and more specific than the standard format. DART can convert these back to standard format using its export fuction. This directory also contains the default.sig file used by Siegfried, which is the core of the format identification worker. restoration Contains code used by the Glacier, file, and object restoration workers. scripts Contains two Ruby scripts. build.rb builds the entire suite of Preserv apps, writing the executables into bin/go-bin . test.rb runs unit, integration, and end-to-end tests, which require spinning up a number of locally running external services. See the testing documentation for details. testdata Contains a number of files used in unit/integration/e2e tests. These include files to be bagged, bags to be validated, and JSON files in e2e_results to be matched against the expected outcomes of end-to-end tests. util Contains utility functions used throughout the codebase. workers Contains code to harness the contents of the deletion, fixity, ingest, and restoration directories into usable, NSQ-connected workers. These workers are then loaded by the apps in the apps directory. See Anatomy of a Worker for details on how these pieces fit together. .env These are settings files for different environments (dev, test, integration, etc.) See Settings or the comments in the files themselves for info about which settings are available and what they mean. Dockerfile.build This file contains instructions for building Docker containers in which to run preservation services. Use make to build the containers, as described on the Docker page. Makefile Includes commands to build and publish the Docker containers, and to update the CloudFormation template. See Docker docker-compose.yml This is an historical artifact used in early, proof-of-concept Docker builds. We may use it as a refence if we move to Kubernetes.","title":"Code Structure"},{"location":"testing/","text":"Testing Because workflow pipelines in preservation services are long and involve many steps, it is essential to maintain a full set of unit, integration, and end-to-end tests. Without these, it's impossible to detect subtle regressions caused by new features or changes to settings. In general, Preserv functions should be broken down into the simplest possible units and should have no side effects. As far as possible, each function should be testable, and unit tests should test for both successful outcomes and for potential failures. These goals are not always possible, especially for functions that interact with outside resources, but we should take them as guidelines in our coding practice. TL;DR To run the entire test suite: ./scipts/test.rb integration && ./scripts/test.rb e2e Nerds and masochists, read on. The Test Script The test.rb script in the scripts folder manages unit, integration and end-to-end tests. In addition to running test suites, it orchestrates the start-up and shut down of external services such as Minio, NSQ, Redis and Registry. That is, it runs a full APTrust environment on your local machine, with a local Minio server standing in for S3 and Glacier. For help, simply run ./scripts/test.rb with no arguments. Unit Tests To run unit tests, call this from the Preserv's top-level directory: ./scripts/test.rb units The script will do the following: Delete and recreate folders in $HOME/tmp Start a local Redis server using the binary in the project's bin directory Run all of the unit tests & print results Shut down Redis and Minio Files in $HOME/tmp remain after the tests complete. You can inspect logs in $HOME/tmp/logs and S3 items in $HOME/tmp/minio. Integration Tests ./scripts/test.rb integration will run the integration test suite. This does everything described in the unit tests above, plus the following: Start an instance of NSQ from the local bin directory Start an instance of Registry Empty the test database, apt_registry_integration Load all fixtures into apt_registry_integration Starts all Preserv services, including the ingest_bucket_reader, apt_queue, and apt_queue_fixity, which are responsible for pushing work items into NSQ Run integration tests & print results Shut down the Registry After integration tests run, you can inspect artifacts left in the ~/tmp directory and in the database. Note that the integration test suite also runs the unit test suite. End to End Tests ./scripts/test.rb e2e runs end-to-end tests. Preserv's end-to-end tests are the most likely to catch regressions. These tests ingest, delete, and restore a number of files and objects, and run fixity checks. They also test business logic such as two-factor deletion and alert generation. The tests exercise all of Preserv's major functions and then test post-conditions in S3, Redis, NSQ, and Registry. For example, if an ingest succeeded, the expected post-conditions include: all files present in the right S3 (local Minio) buckets all interim files deleted from the S3 (local Minio) staging bucket interim data deleted from Redis object present in Registry all files present in Registry all expected checksums, storage records and premis events in Registry work item complete in Registry nsq item marked complete The end-to-end tests run through expected success and expected failure scenarios. Like other test suites, they leave artifacts in ~/tmp and in the Registry's apt_registry_integration database. To examine the state of the Registry after end-to-end tests, change into the Registry's root directory and run APT_ENV=integration ./registry serve . Putting It All Together The following command runs all the tests: ./scipts/test.rb integration && ./scripts/test.rb e2e Note the the integration test suite implicitly runs the unit tests. Also note that the && will prevent the end-to-end tests from running if the integration tests fail. Interactive Tests ./scripts/test.rb interactive will not run any automated tests, but will spin up a whole APTrust environment on which you can run manual tests. The environment includes Minio, NSQ, Redis, and Registry. This option is useful in cases where you want to test a fix for a bag that failed ingest on a live system due to an error. Did your fix work? Try this: Download the failed bag from the depositor's receiving bucket. Run ./scripts/test.rb interactive Copy the bag into ~/tmp/minio/aptrust.receiving.test.test.edu The bucket reader will find the bag in a few seconds and queue it for ingest. You can watch it go through at http://localhost:8080 . The logins should be pre-populated. You can tail the logs in ~/tmp/logs to see what the workers are doing. And you can look directly into the Redis interim processing data. See Querying Redis for more info. Local bin Directory Inside the preservation-services repo is a bin directory containing OS-specific binaries of Redis, NSQ and Minio. The test.rb script detects which operating system you're currently running and selects the right binaries from either bin/linux or bin/osx .","title":"Testing"},{"location":"testing/#testing","text":"Because workflow pipelines in preservation services are long and involve many steps, it is essential to maintain a full set of unit, integration, and end-to-end tests. Without these, it's impossible to detect subtle regressions caused by new features or changes to settings. In general, Preserv functions should be broken down into the simplest possible units and should have no side effects. As far as possible, each function should be testable, and unit tests should test for both successful outcomes and for potential failures. These goals are not always possible, especially for functions that interact with outside resources, but we should take them as guidelines in our coding practice.","title":"Testing"},{"location":"testing/#tldr","text":"To run the entire test suite: ./scipts/test.rb integration && ./scripts/test.rb e2e Nerds and masochists, read on.","title":"TL;DR"},{"location":"testing/#the-test-script","text":"The test.rb script in the scripts folder manages unit, integration and end-to-end tests. In addition to running test suites, it orchestrates the start-up and shut down of external services such as Minio, NSQ, Redis and Registry. That is, it runs a full APTrust environment on your local machine, with a local Minio server standing in for S3 and Glacier. For help, simply run ./scripts/test.rb with no arguments.","title":"The Test Script"},{"location":"testing/#unit-tests","text":"To run unit tests, call this from the Preserv's top-level directory: ./scripts/test.rb units The script will do the following: Delete and recreate folders in $HOME/tmp Start a local Redis server using the binary in the project's bin directory Run all of the unit tests & print results Shut down Redis and Minio Files in $HOME/tmp remain after the tests complete. You can inspect logs in $HOME/tmp/logs and S3 items in $HOME/tmp/minio.","title":"Unit Tests"},{"location":"testing/#integration-tests","text":"./scripts/test.rb integration will run the integration test suite. This does everything described in the unit tests above, plus the following: Start an instance of NSQ from the local bin directory Start an instance of Registry Empty the test database, apt_registry_integration Load all fixtures into apt_registry_integration Starts all Preserv services, including the ingest_bucket_reader, apt_queue, and apt_queue_fixity, which are responsible for pushing work items into NSQ Run integration tests & print results Shut down the Registry After integration tests run, you can inspect artifacts left in the ~/tmp directory and in the database. Note that the integration test suite also runs the unit test suite.","title":"Integration Tests"},{"location":"testing/#end-to-end-tests","text":"./scripts/test.rb e2e runs end-to-end tests. Preserv's end-to-end tests are the most likely to catch regressions. These tests ingest, delete, and restore a number of files and objects, and run fixity checks. They also test business logic such as two-factor deletion and alert generation. The tests exercise all of Preserv's major functions and then test post-conditions in S3, Redis, NSQ, and Registry. For example, if an ingest succeeded, the expected post-conditions include: all files present in the right S3 (local Minio) buckets all interim files deleted from the S3 (local Minio) staging bucket interim data deleted from Redis object present in Registry all files present in Registry all expected checksums, storage records and premis events in Registry work item complete in Registry nsq item marked complete The end-to-end tests run through expected success and expected failure scenarios. Like other test suites, they leave artifacts in ~/tmp and in the Registry's apt_registry_integration database. To examine the state of the Registry after end-to-end tests, change into the Registry's root directory and run APT_ENV=integration ./registry serve .","title":"End to End Tests"},{"location":"testing/#putting-it-all-together","text":"The following command runs all the tests: ./scipts/test.rb integration && ./scripts/test.rb e2e Note the the integration test suite implicitly runs the unit tests. Also note that the && will prevent the end-to-end tests from running if the integration tests fail.","title":"Putting It All Together"},{"location":"testing/#interactive-tests","text":"./scripts/test.rb interactive will not run any automated tests, but will spin up a whole APTrust environment on which you can run manual tests. The environment includes Minio, NSQ, Redis, and Registry. This option is useful in cases where you want to test a fix for a bag that failed ingest on a live system due to an error. Did your fix work? Try this: Download the failed bag from the depositor's receiving bucket. Run ./scripts/test.rb interactive Copy the bag into ~/tmp/minio/aptrust.receiving.test.test.edu The bucket reader will find the bag in a few seconds and queue it for ingest. You can watch it go through at http://localhost:8080 . The logins should be pre-populated. You can tail the logs in ~/tmp/logs to see what the workers are doing. And you can look directly into the Redis interim processing data. See Querying Redis for more info.","title":"Interactive Tests"},{"location":"testing/#local-bin-directory","text":"Inside the preservation-services repo is a bin directory containing OS-specific binaries of Redis, NSQ and Minio. The test.rb script detects which operating system you're currently running and selects the right binaries from either bin/linux or bin/osx .","title":"Local bin Directory"},{"location":"components/","text":"Components Preservation Services (aka Preserv) consists of the following components. Click on any component name to find out more about it. Component Description Redis / Elasticache A shared data store where workers keep metadata about the ingest process. NSQ A queue service that distributes tasks to ingest, restoration, deltion, and fixity workers. Registry A REST API providing access to the authoritative, permanent data store describing all items in preservation storage and all events related to those items. The Registry also contains authoritative WorkItem records that track the status of all ingest, restoration and deletion tasks. S3 The storage service used throughout all phases of ingest, restoration, preservation, fixity checking, and deletion. The term \"S3\" includes Glacier and Wasabi storage.","title":"Components"},{"location":"components/#components","text":"Preservation Services (aka Preserv) consists of the following components. Click on any component name to find out more about it. Component Description Redis / Elasticache A shared data store where workers keep metadata about the ingest process. NSQ A queue service that distributes tasks to ingest, restoration, deltion, and fixity workers. Registry A REST API providing access to the authoritative, permanent data store describing all items in preservation storage and all events related to those items. The Registry also contains authoritative WorkItem records that track the status of all ingest, restoration and deletion tasks. S3 The storage service used throughout all phases of ingest, restoration, preservation, fixity checking, and deletion. The term \"S3\" includes Glacier and Wasabi storage.","title":"Components"},{"location":"components/nsq/","text":"NSQ NSQ is the queue service that tells all our workers what tasks need to be completed. This includes ingest, restoration, deletion and fixity workers. NSQ includes topics and channels. A topic holds a list of tasks that need to be completed. Each topic can have one or more channels. Workers subscribe to channels to get their work. Note NSQ allows multiple channels per topic because in certain environments mutiple workers need to take action when an item appears in a queue. In Preserv, we use only one channel per topic. In Preserv, each topic has only one channel, and workers subscribe to channels as described in the table below. The source column describes how items get into this queue. The content column describes what's in the queue message. When the content is a WorkItem ID, the worker pulls a copy of the WorkItem from the Registry, marks it as \"in progress\" and begins its work. When the the content is a GenericFile ID, the worker retrieves the GenericFile record from the Registry, runs a fixity check, and records the outcome in a Premis Event withouth ever creating a WorkItem. Topic Worker Content Purpose Source delete_item apt_delete WorkItem ID Deletes files and/or objects from preservation storage and records deletion Premis Event. Deletions are pushed into the queue by the Registry when they're approved by an institutional admin. The apt_queue service runs periodically to catch any deletion and restoration requests that may not have made it from Registry to NSQ due to network errors. fixity_check apt_fixity GenericFile ID Tells apt_fixity which files to check. The apt_queue_fixity process pushes about 2500 files into this queue every 20-30 minutes. The exact number and frequency are defined by params QUEUE_FIXITY_INTERVAL and MAX_FIXITY_ITEMS_PER_RUN , both of which are in AWS Parameter Store. ingest01_ prefetch ingest _pre_ fetch WorkItem ID Scans new bags in depositors' receiving buckets and collects metadata for bag validation. The ingest_bucket_reader, a cron job running in its own container, checks the buckets every few minutes. When it finds new bags with no associated WorkItem, it creates a new Ingest WorkItem in the Registry and pushes the WorkItem ID into this queue. ingest02 _bag_ validation ingest_ validator WorkItem ID Validates a bag to ensure all files are present, all checksums match, no extraneous payload files exist, and the bag conforms to the BagIt profile noted in bag-info.txt. The ingest_pre_fetch worker pushes WorkItem IDs into this queue for each item it successfully processes. ingest03_ reingest_ check reingest_ manager WorkItem ID Checks files in the bag against known files in the Registry to see if any of them are re-ingests. See the reingest manager page for more info. Filled by ingest_validator. ingest04_ staging ingest_ staging_ uploader WorkItem ID Unpacks tarred bags from depositor receiving buckets and uploads the individual files to an S3 staging bucket so subsequent workers can process them. Filled by reingest_manager. ingest05_ format_ identification ingest_ format_ identifier WorkItem ID Runs individual files from the staging bucket through the Siegfried file identifier, which uses the PRONOM registry under the hood to identify file formats. Filled by ingest_ staging_ uploader. ingest06_ storage ingest_ preservation_ uploader WorkItem ID Copies individual files from the S3 staging bucket to preservation storage in S3, Glacier, and/or Wasabi. Filled by ingest_ format_ identifier. ingest07_ storage _validation ingest_ preservation_ verifier WorkItem ID Verifies that items copied to preservation storage actually got there by performing a HEAD request and ensuring file size matches. Filled by ingest_ preservation_ uploader. ingest08_ record ingest_ recorder WorkItem ID Records the results of an ingest in Registry. This includes the object record, file records, storage records, checksums, and premis events. Filled by ingest_ preservation_ verifier. ingest09_ cleanup ingest_ cleanup WorkItem ID Deletes interim processing data, including the tarred bag in the depositor's receiving bucket, the individual files in the staging bucket, and the processing metadata in Redis/Elasticache. Filled by ingest_recorder. restore_ file file_ restorer WorkItem ID Restores individual files by moving them from preservation storage to the depositor's restoration bucket. Filled by Registry when a user clicks the Restore File button, with backup from apt_queue in case Registry queueing fails. This topic can also be filled by restore_glacier worker when it determines that a Glacier file has been moved into an S3 bucket for retrieval. restore_ glacier glacier_ restorer WorkItem ID Moves files from Glacier and Glacier Deep Archive into an S3 bucket so they can be restored to depositors. Filled by Registry when a user clicks the Restore File button for an object in Glacier or Glacier Deep Archive. Registry queueing fails, apt_queue will find the item and queue it. restore_ object bag_ restorer WorkItem ID Restores entire objects (bags) to depositors by collecting all files, bagging them, validating the bag, and pushing it into the depositor's restoration bucket. Filled by Registry when a user clicks the Restore File button, with backup from apt_queue in case Registry queueing fails. This topic can also be filled by restore_glacier worker when it determines that all of a bag's Glacier files have been moved into an S3 bucket for retrieval. Duplicate Tasks in NSQ Under Message Delivery Guarantees NSQ documentation says: NSQ guarantees that a message will be delivered at least once, though duplicate messages are possible. Consumers should expect this and de-dupe or perform idempotent operations. It's fairly common to get duplicate messages in most of the NSQ topics. This generally happens in two ways: In the fixity topic, apt_queue_fixity finds 2500 files in the Registry that have not had a fixity check in 90 days. It queues the GenericFile ID of those files in NSQ's fixity_check topic. Occasionally, apt_fixity can't get through all 2500 files before the next run of apt_queue_fixity , which then queues incomplete the GenericFile IDs again. In all other topics, NSQ may not hear back from a worker before NSQ's internal timeout. NSQ thinks the worker is stuck or dead, so it requeues the WorkItem ID itself. This is most likely to happen when long-running operations like ingest_pre_fetch , ingest_format_identifier and ingest_preservation_uploader are dealing with huge files. The solutions to these issues are: The apt_fixity checks the LastFixity timestamp on each GenericFile before it actually starts work. If the file was just checked, it tells NSQ the task is complete and logs a message saying it's skipping that file because it was recently checked. All other workers fetch a copy of the WorkItem from the Registry before they go to work. If the WorkItem Node is non-empty, the process knows some worker is already on it, and it skips the task. Workers also check a number of other properties on the WorkItem to ensure the item is in the right stage of processing, has not completed or been cancelled, has the Retry flag set to true, etc. For more info, search the code repo for ShouldSkipThis . You'll find various methods in various packages, since the logic for what's to be skipped differs according to what's being done. All ingest workers share the same skip logic. Bag restoration, file restoration, Glacier restoration and deletion have their own logic, but regardless of the worker, the function is always called ShouldSkipThis Queue Settings in AWS Parameter Store Each worker pulls three performance tuning parameters from Parameter Store. The settings are listed below. Descriptions of the settings appear below. Queue Topic Settings delete_item APT_DELETE_BUFFER_SIZE, APT_DELETE_WORKERS, APT_DELETE_MAX_ATTEMPTS fixity_check APT_FIXITY_BUFFER_SIZE, APT_FIXITY_WORKERS, APT_FIXITY_MAX_ATTEMPTS ingest01_prefetch INGEST_PRE_FETCH_BUFFER_SIZE, INGEST_PRE_FETCH_WORKERS, INGEST_PRE_FETCH_MAX_ATTEMPTS ingest02_bag_validation INGEST_VALIDATOR_BUFFER_SIZE, INGEST_VALIDATOR_WORKERS, INGEST_VALIDATOR_MAX_ATTEMPTS ingest03_reingest_check REINGEST_MANAGER_BUFFER_SIZE, REINGEST_MANAGER_WORKERS, REINGEST_MANAGER_MAX_ATTEMPTS ingest04_staging INGEST_STAGING_UPLOADER_BUFFER_SIZE, INGEST_STAGING_UPLOADER_WORKERS, STAGING_UPLOADER_MAX_ATTEMPTS ingest05_format_identification INGEST_FORMAT_IDENTIFIER_WORKERS, INGEST_FORMAT_IDENTIFIER_BUFFER_SIZE, INGEST_FORMAT_IDENTIFIER_MAX_ATTEMPTS ingest06_storage INGEST_PRESERVATION_UPLOADER_BUFFER_SIZE, INGEST_PRESERVATION_UPLOADER_WORKERS, INGEST_PRESERVATION_UPLOADER_MAX_ATTEMPTS ingest07_storage_validation INGEST_PRESERVATION_VERIFIER_BUFFER_SIZE, INGEST_PRESERVATION_VERIFIER_WORKERS, INGEST_PRESERVATION_VERIFIER_MAX_ATTEMPTS ingest08_record INGEST_RECORDER_BUFFER_SIZE, INGEST_RECORDER_WORKERS, INGEST_RECORDER_MAX_ATTEMPTS ingest09_cleanup INGEST_CLEANUP_BUFFER_SIZE, INGEST_CLEANUP_WORKERS, INGEST_CLEANUP_MAX_ATTEMPTS restore_file FILE_RESTORER_BUFFER_SIZE, FILE_RESTORER_WORKERS, FILE_RESTORER_MAX_ATTEMPTS restore_glacier GLACIER_RESTORER_BUFFER_SIZE, GLACIER_RESTORER_WORKERS, GLACIER_RESTORER_MAX_ATTEMPTS restore_object BAG_RESTORER_BUFFER_SIZE, BAG_RESTORER_WORKERS, BAG_RESTORER_MAX_ATTEMPTS Workers Setting Params ending in WORKERS describe the number of go routines a worker should use to do its work. A setting of 3 indicates 3 go routines, which means the worker can process three NSQ WorkItems simultaneously. (Think of a Java or C++ app running three independent threads at all times, with each thread capable of handing an NSQ task from stat to finish.) Because our workers run in very small containers with >512 MB of RAM, we tend to limit the number of workers to 2, or in some cases, such as the format identifier, to 1. (The format identifier uses a lot of memory, and more than one concurrent go routine can cause it to crash with an out-of-memory exception.) When running in larger containers, you can increase the number of workers. Note, however, that ingest_preservation_uploader can also use a lot of memory and can crash if you set the number of workers too high. Important It's generally not a good idea to set the number of workers for ingest_recorder above 2. This worker issues a huge number of requests to the Registry at the end of the ingest process. Too many recorders can make the Registry very slow, or can cause it to scale out to more containers than we really need to run. It's better (and likely just as fast) to keep these items queued in the record topic than to have hundreds of requests stuck in Registry's TCP buffers. Buffer Size Setting Params ending in BUFFER_SIZE describe the number of NSQ messages to hold in each worker's internal queue. This setting has some interesting ramifications for scaling. Let's say you set BUFFER_SIZE to 10 for the ingest_staging_uploader , and you also set the number of workers for ingest_staging_uploader to 2. Each of the ingest validator's two go routines will tell NSQ it's ready to accept 10 items. Assuming NSQ has that many items, it will give 20 tasks to ingest_staging_uploader all at once, and it will wait until the each go routine has finished its set of 10 before it hands that go routine 10 more. This last point is important for scaling. Let's say a worker gets 20 items from NSQ and its CPU and RAM usage go straight to 100%. This will trigger our ECS auto-scaler to add a new ingest_staging_uploader container. However, the first container will not relinquish the batch of 20 files it's working on, and it's possible that the new container will have nothing to work on at all. In this case, container 1 may continue to max out all its resources for several hours, while container 2 remains idle. For this reason, we tend to set BUFFER_SIZE to a low number (3-5) for our long-running, resource-intensive workers. The following workers are resource-intensive and/or long-running, and should have relatively small buffer sizes: Worker Heavily Used Resource apt_fixity High network I/O when fetching files from S3. High CPU usage when calculating checksums. bag_restorer High network I/O when fetching files from S3. High CPU usage when calculating checksums. file_restorer High network I/O when fetching files from S3. High CPU usage when calculating checksums. ingest_format_identifier High network I/O and CPU. Extremely high memory. ingest_pre_fetch High network I/O when fetching files from S3. High CPU usage when calculating checksums. ingest_preservation_uploader High network I/O. When uploading very large files, high memory usage for upload buffers. ingest_staging_uploader High network I/O. When uploading very large files, high memory usage for upload buffers. The remaining workers tend to finish to their tasks quickly, often in seconds, so they can tolerate higher buffer sizes. Max Attempts Setting The MAX_ATTEMPTS settings describe how many times a worker should retry an NSQ task before giving up due to transient errors. Transient errors almost always fall into two categories: Network problems, such as timeout or connection reset by peer Unavailable remote services, such S3, Glacier, Wasabi, Redis, NSQ or Registry responding with HTTP 503 (Service Unavailable) or not responding at all. Of the errors above, #1 accounts for more than 99% of cases. When a task fails due to transient errors, the worker requeues it. After requeuing it MAX_ATTEMPS times, the worker stops trying to complete the task and does the following: Marks the taks as failed in NSQ. Logs specific information about why it's quitting. Updates the WorkItem in Registry, setting: Retry = false NeedsAdminReview = true Note = Detailed information about what happened and why the worker won't complete the task. An APTrust admin can easily find these items in the Registry by search for WorkItems where NeedsAdminReview = true. The admin can then requeue them if he/she knows the underlying issue is resolved. When these issues do occur, they are almost always network errors and it makes no sense to requeue this items until we know the network has recovered. Fatal Errors The only fatal errors that will cause workers to permenantly give up on an item are: Invalid bags. These occasinally appear in depositors' receiving buckets. We can't process them, so we don't. Invalid bags may remain in the receiving buckets for up to 14 days, so we and the depositors can examine them. Their interim processing data may persist in Redis as well, for diagnostic/forensic purposes. APTrust admins must manually delete the Redis data. Files missing from preservation. This has never happened, but if it were to happen, restoration and deletion processes would log a fatal error and set the NeedsAdminReview flag on the appropriate WorkItem. The APTrust admin will have to manually investigate the issue. Invalid restoration package. This would happen if the object restorer reassembled a bag for restoration and then found the bag was invalid. This has never happened. Once again, this would require a manual investigation by an APTrust admin. NSQ Admin Functions All NSQ admin features are available to APTrust admins through the Registry UI. Features include starting, pausing, deleting and emptying topics and channels. Important The most graceful way to stop all processing in the system is to pause all of the topics and channels. The workers will complete the items in their internal go routine queues and will then sit idle. The Registry UI includes a feature to pause and unpause all topics and/or channels at once. When paused, topics will continue to queue new tasks, but will not pass those tasks out to workers. When you unpause the topics and channels, workers will resume work immediately. Manually Requeuing The Registry UI allows APTrust admins to manually requeue any item that has not completed processing. (I.e. You can requeue a completed ingest, restoration, or deletion. There's no point.) To requeue an item, go to it's WorkItem detail page and click the Requeue button. You should generally requeue to the last attempted stage of processing, which is pre-selected in the UI. The APTrust admin may requeue to any earlier stage of processing if he/she thinks that's appropriate.","title":"NSQ"},{"location":"components/nsq/#nsq","text":"NSQ is the queue service that tells all our workers what tasks need to be completed. This includes ingest, restoration, deletion and fixity workers. NSQ includes topics and channels. A topic holds a list of tasks that need to be completed. Each topic can have one or more channels. Workers subscribe to channels to get their work. Note NSQ allows multiple channels per topic because in certain environments mutiple workers need to take action when an item appears in a queue. In Preserv, we use only one channel per topic. In Preserv, each topic has only one channel, and workers subscribe to channels as described in the table below. The source column describes how items get into this queue. The content column describes what's in the queue message. When the content is a WorkItem ID, the worker pulls a copy of the WorkItem from the Registry, marks it as \"in progress\" and begins its work. When the the content is a GenericFile ID, the worker retrieves the GenericFile record from the Registry, runs a fixity check, and records the outcome in a Premis Event withouth ever creating a WorkItem. Topic Worker Content Purpose Source delete_item apt_delete WorkItem ID Deletes files and/or objects from preservation storage and records deletion Premis Event. Deletions are pushed into the queue by the Registry when they're approved by an institutional admin. The apt_queue service runs periodically to catch any deletion and restoration requests that may not have made it from Registry to NSQ due to network errors. fixity_check apt_fixity GenericFile ID Tells apt_fixity which files to check. The apt_queue_fixity process pushes about 2500 files into this queue every 20-30 minutes. The exact number and frequency are defined by params QUEUE_FIXITY_INTERVAL and MAX_FIXITY_ITEMS_PER_RUN , both of which are in AWS Parameter Store. ingest01_ prefetch ingest _pre_ fetch WorkItem ID Scans new bags in depositors' receiving buckets and collects metadata for bag validation. The ingest_bucket_reader, a cron job running in its own container, checks the buckets every few minutes. When it finds new bags with no associated WorkItem, it creates a new Ingest WorkItem in the Registry and pushes the WorkItem ID into this queue. ingest02 _bag_ validation ingest_ validator WorkItem ID Validates a bag to ensure all files are present, all checksums match, no extraneous payload files exist, and the bag conforms to the BagIt profile noted in bag-info.txt. The ingest_pre_fetch worker pushes WorkItem IDs into this queue for each item it successfully processes. ingest03_ reingest_ check reingest_ manager WorkItem ID Checks files in the bag against known files in the Registry to see if any of them are re-ingests. See the reingest manager page for more info. Filled by ingest_validator. ingest04_ staging ingest_ staging_ uploader WorkItem ID Unpacks tarred bags from depositor receiving buckets and uploads the individual files to an S3 staging bucket so subsequent workers can process them. Filled by reingest_manager. ingest05_ format_ identification ingest_ format_ identifier WorkItem ID Runs individual files from the staging bucket through the Siegfried file identifier, which uses the PRONOM registry under the hood to identify file formats. Filled by ingest_ staging_ uploader. ingest06_ storage ingest_ preservation_ uploader WorkItem ID Copies individual files from the S3 staging bucket to preservation storage in S3, Glacier, and/or Wasabi. Filled by ingest_ format_ identifier. ingest07_ storage _validation ingest_ preservation_ verifier WorkItem ID Verifies that items copied to preservation storage actually got there by performing a HEAD request and ensuring file size matches. Filled by ingest_ preservation_ uploader. ingest08_ record ingest_ recorder WorkItem ID Records the results of an ingest in Registry. This includes the object record, file records, storage records, checksums, and premis events. Filled by ingest_ preservation_ verifier. ingest09_ cleanup ingest_ cleanup WorkItem ID Deletes interim processing data, including the tarred bag in the depositor's receiving bucket, the individual files in the staging bucket, and the processing metadata in Redis/Elasticache. Filled by ingest_recorder. restore_ file file_ restorer WorkItem ID Restores individual files by moving them from preservation storage to the depositor's restoration bucket. Filled by Registry when a user clicks the Restore File button, with backup from apt_queue in case Registry queueing fails. This topic can also be filled by restore_glacier worker when it determines that a Glacier file has been moved into an S3 bucket for retrieval. restore_ glacier glacier_ restorer WorkItem ID Moves files from Glacier and Glacier Deep Archive into an S3 bucket so they can be restored to depositors. Filled by Registry when a user clicks the Restore File button for an object in Glacier or Glacier Deep Archive. Registry queueing fails, apt_queue will find the item and queue it. restore_ object bag_ restorer WorkItem ID Restores entire objects (bags) to depositors by collecting all files, bagging them, validating the bag, and pushing it into the depositor's restoration bucket. Filled by Registry when a user clicks the Restore File button, with backup from apt_queue in case Registry queueing fails. This topic can also be filled by restore_glacier worker when it determines that all of a bag's Glacier files have been moved into an S3 bucket for retrieval.","title":"NSQ"},{"location":"components/nsq/#duplicate-tasks-in-nsq","text":"Under Message Delivery Guarantees NSQ documentation says: NSQ guarantees that a message will be delivered at least once, though duplicate messages are possible. Consumers should expect this and de-dupe or perform idempotent operations. It's fairly common to get duplicate messages in most of the NSQ topics. This generally happens in two ways: In the fixity topic, apt_queue_fixity finds 2500 files in the Registry that have not had a fixity check in 90 days. It queues the GenericFile ID of those files in NSQ's fixity_check topic. Occasionally, apt_fixity can't get through all 2500 files before the next run of apt_queue_fixity , which then queues incomplete the GenericFile IDs again. In all other topics, NSQ may not hear back from a worker before NSQ's internal timeout. NSQ thinks the worker is stuck or dead, so it requeues the WorkItem ID itself. This is most likely to happen when long-running operations like ingest_pre_fetch , ingest_format_identifier and ingest_preservation_uploader are dealing with huge files. The solutions to these issues are: The apt_fixity checks the LastFixity timestamp on each GenericFile before it actually starts work. If the file was just checked, it tells NSQ the task is complete and logs a message saying it's skipping that file because it was recently checked. All other workers fetch a copy of the WorkItem from the Registry before they go to work. If the WorkItem Node is non-empty, the process knows some worker is already on it, and it skips the task. Workers also check a number of other properties on the WorkItem to ensure the item is in the right stage of processing, has not completed or been cancelled, has the Retry flag set to true, etc. For more info, search the code repo for ShouldSkipThis . You'll find various methods in various packages, since the logic for what's to be skipped differs according to what's being done. All ingest workers share the same skip logic. Bag restoration, file restoration, Glacier restoration and deletion have their own logic, but regardless of the worker, the function is always called ShouldSkipThis","title":"Duplicate Tasks in NSQ"},{"location":"components/nsq/#queue-settings-in-aws-parameter-store","text":"Each worker pulls three performance tuning parameters from Parameter Store. The settings are listed below. Descriptions of the settings appear below. Queue Topic Settings delete_item APT_DELETE_BUFFER_SIZE, APT_DELETE_WORKERS, APT_DELETE_MAX_ATTEMPTS fixity_check APT_FIXITY_BUFFER_SIZE, APT_FIXITY_WORKERS, APT_FIXITY_MAX_ATTEMPTS ingest01_prefetch INGEST_PRE_FETCH_BUFFER_SIZE, INGEST_PRE_FETCH_WORKERS, INGEST_PRE_FETCH_MAX_ATTEMPTS ingest02_bag_validation INGEST_VALIDATOR_BUFFER_SIZE, INGEST_VALIDATOR_WORKERS, INGEST_VALIDATOR_MAX_ATTEMPTS ingest03_reingest_check REINGEST_MANAGER_BUFFER_SIZE, REINGEST_MANAGER_WORKERS, REINGEST_MANAGER_MAX_ATTEMPTS ingest04_staging INGEST_STAGING_UPLOADER_BUFFER_SIZE, INGEST_STAGING_UPLOADER_WORKERS, STAGING_UPLOADER_MAX_ATTEMPTS ingest05_format_identification INGEST_FORMAT_IDENTIFIER_WORKERS, INGEST_FORMAT_IDENTIFIER_BUFFER_SIZE, INGEST_FORMAT_IDENTIFIER_MAX_ATTEMPTS ingest06_storage INGEST_PRESERVATION_UPLOADER_BUFFER_SIZE, INGEST_PRESERVATION_UPLOADER_WORKERS, INGEST_PRESERVATION_UPLOADER_MAX_ATTEMPTS ingest07_storage_validation INGEST_PRESERVATION_VERIFIER_BUFFER_SIZE, INGEST_PRESERVATION_VERIFIER_WORKERS, INGEST_PRESERVATION_VERIFIER_MAX_ATTEMPTS ingest08_record INGEST_RECORDER_BUFFER_SIZE, INGEST_RECORDER_WORKERS, INGEST_RECORDER_MAX_ATTEMPTS ingest09_cleanup INGEST_CLEANUP_BUFFER_SIZE, INGEST_CLEANUP_WORKERS, INGEST_CLEANUP_MAX_ATTEMPTS restore_file FILE_RESTORER_BUFFER_SIZE, FILE_RESTORER_WORKERS, FILE_RESTORER_MAX_ATTEMPTS restore_glacier GLACIER_RESTORER_BUFFER_SIZE, GLACIER_RESTORER_WORKERS, GLACIER_RESTORER_MAX_ATTEMPTS restore_object BAG_RESTORER_BUFFER_SIZE, BAG_RESTORER_WORKERS, BAG_RESTORER_MAX_ATTEMPTS","title":"Queue Settings in AWS Parameter Store"},{"location":"components/nsq/#workers-setting","text":"Params ending in WORKERS describe the number of go routines a worker should use to do its work. A setting of 3 indicates 3 go routines, which means the worker can process three NSQ WorkItems simultaneously. (Think of a Java or C++ app running three independent threads at all times, with each thread capable of handing an NSQ task from stat to finish.) Because our workers run in very small containers with >512 MB of RAM, we tend to limit the number of workers to 2, or in some cases, such as the format identifier, to 1. (The format identifier uses a lot of memory, and more than one concurrent go routine can cause it to crash with an out-of-memory exception.) When running in larger containers, you can increase the number of workers. Note, however, that ingest_preservation_uploader can also use a lot of memory and can crash if you set the number of workers too high. Important It's generally not a good idea to set the number of workers for ingest_recorder above 2. This worker issues a huge number of requests to the Registry at the end of the ingest process. Too many recorders can make the Registry very slow, or can cause it to scale out to more containers than we really need to run. It's better (and likely just as fast) to keep these items queued in the record topic than to have hundreds of requests stuck in Registry's TCP buffers.","title":"Workers Setting"},{"location":"components/nsq/#buffer-size-setting","text":"Params ending in BUFFER_SIZE describe the number of NSQ messages to hold in each worker's internal queue. This setting has some interesting ramifications for scaling. Let's say you set BUFFER_SIZE to 10 for the ingest_staging_uploader , and you also set the number of workers for ingest_staging_uploader to 2. Each of the ingest validator's two go routines will tell NSQ it's ready to accept 10 items. Assuming NSQ has that many items, it will give 20 tasks to ingest_staging_uploader all at once, and it will wait until the each go routine has finished its set of 10 before it hands that go routine 10 more. This last point is important for scaling. Let's say a worker gets 20 items from NSQ and its CPU and RAM usage go straight to 100%. This will trigger our ECS auto-scaler to add a new ingest_staging_uploader container. However, the first container will not relinquish the batch of 20 files it's working on, and it's possible that the new container will have nothing to work on at all. In this case, container 1 may continue to max out all its resources for several hours, while container 2 remains idle. For this reason, we tend to set BUFFER_SIZE to a low number (3-5) for our long-running, resource-intensive workers. The following workers are resource-intensive and/or long-running, and should have relatively small buffer sizes: Worker Heavily Used Resource apt_fixity High network I/O when fetching files from S3. High CPU usage when calculating checksums. bag_restorer High network I/O when fetching files from S3. High CPU usage when calculating checksums. file_restorer High network I/O when fetching files from S3. High CPU usage when calculating checksums. ingest_format_identifier High network I/O and CPU. Extremely high memory. ingest_pre_fetch High network I/O when fetching files from S3. High CPU usage when calculating checksums. ingest_preservation_uploader High network I/O. When uploading very large files, high memory usage for upload buffers. ingest_staging_uploader High network I/O. When uploading very large files, high memory usage for upload buffers. The remaining workers tend to finish to their tasks quickly, often in seconds, so they can tolerate higher buffer sizes.","title":"Buffer Size Setting"},{"location":"components/nsq/#max-attempts-setting","text":"The MAX_ATTEMPTS settings describe how many times a worker should retry an NSQ task before giving up due to transient errors. Transient errors almost always fall into two categories: Network problems, such as timeout or connection reset by peer Unavailable remote services, such S3, Glacier, Wasabi, Redis, NSQ or Registry responding with HTTP 503 (Service Unavailable) or not responding at all. Of the errors above, #1 accounts for more than 99% of cases. When a task fails due to transient errors, the worker requeues it. After requeuing it MAX_ATTEMPS times, the worker stops trying to complete the task and does the following: Marks the taks as failed in NSQ. Logs specific information about why it's quitting. Updates the WorkItem in Registry, setting: Retry = false NeedsAdminReview = true Note = Detailed information about what happened and why the worker won't complete the task. An APTrust admin can easily find these items in the Registry by search for WorkItems where NeedsAdminReview = true. The admin can then requeue them if he/she knows the underlying issue is resolved. When these issues do occur, they are almost always network errors and it makes no sense to requeue this items until we know the network has recovered.","title":"Max Attempts Setting"},{"location":"components/nsq/#fatal-errors","text":"The only fatal errors that will cause workers to permenantly give up on an item are: Invalid bags. These occasinally appear in depositors' receiving buckets. We can't process them, so we don't. Invalid bags may remain in the receiving buckets for up to 14 days, so we and the depositors can examine them. Their interim processing data may persist in Redis as well, for diagnostic/forensic purposes. APTrust admins must manually delete the Redis data. Files missing from preservation. This has never happened, but if it were to happen, restoration and deletion processes would log a fatal error and set the NeedsAdminReview flag on the appropriate WorkItem. The APTrust admin will have to manually investigate the issue. Invalid restoration package. This would happen if the object restorer reassembled a bag for restoration and then found the bag was invalid. This has never happened. Once again, this would require a manual investigation by an APTrust admin.","title":"Fatal Errors"},{"location":"components/nsq/#nsq-admin-functions","text":"All NSQ admin features are available to APTrust admins through the Registry UI. Features include starting, pausing, deleting and emptying topics and channels. Important The most graceful way to stop all processing in the system is to pause all of the topics and channels. The workers will complete the items in their internal go routine queues and will then sit idle. The Registry UI includes a feature to pause and unpause all topics and/or channels at once. When paused, topics will continue to queue new tasks, but will not pass those tasks out to workers. When you unpause the topics and channels, workers will resume work immediately.","title":"NSQ Admin Functions"},{"location":"components/nsq/#manually-requeuing","text":"The Registry UI allows APTrust admins to manually requeue any item that has not completed processing. (I.e. You can requeue a completed ingest, restoration, or deletion. There's no point.) To requeue an item, go to it's WorkItem detail page and click the Requeue button. You should generally requeue to the last attempted stage of processing, which is pre-selected in the UI. The APTrust admin may requeue to any earlier stage of processing if he/she thinks that's appropriate.","title":"Manually Requeuing"},{"location":"components/redis/","text":"Redis Preserv uses Redis (running as Elasticache in AWS) to hold interim processing data that is accessible to all ingest workers. The first worker in the ingest pipeline, ingest_pre_fetch , reads the contents of a tarred bag from an S3 receiving bucket and records metadata, including: General info about the bag, including its name, owning institution, size, number of payload files, and more. The object info also includes parsed versions of the bag's tag files. General info about each file in the tarred bag, including it's path, size, and checksums from the bag's manifests. A \"work result\" describing when the worker started its task, when it completed, and any errors it encountered along the way. The ingest_pre_fetch worker dumps this data into Redis, in JSON format, so the next worker can access it. Each successive worker adds information to the object, file, and work result records in Redis so subsequent workers know what to do with the incoming content. For example, the reingest_manager checks to see if we already have copies of a bag's files in preservation storage. If so, and if the checksums have not changed, it flags the files as not needing to be saved. The later workers ingest_preservation_uploader and ingest_preservation_verifier know not to copy these files to preservation storage and not to bother verifying the copies. The ingest_recorder knows not to record new ingest events or checksums for these files. We chose Redis as our interim processing cache for a number of reasons: The workers often perform huge volumes of reads and writes that would bog down a relational database. The data naturally fits into a key-value data store. We don't want or need to decompose the JSON into searchable tables or MongoDB-type documents. We always want the whole blob. Interim processing data is ephemeral. We may retain it for a few seconds in the case of small bags, or 2-3 days in the case of larger bags. In most cases, we keep it for 1-10 minutes. Redis' disk-backed \"aof\" file, which allows in-memory data to persist through reboots, is enough of a durability guarantee for our needs. By the time a bag reaches the ingest_recorder worker, the JSON data in Redis contains all of the information we will need to record in the Registry, including: The object record, with its title, description, owning institution and other metadata. All of the file records, and there can be tens of thousands of these. All checksums, premis events, and storage records for all files. Note On live systems, we configure Redis to write to an .aof file (append-only file) so that data persists through system reboots. On dev and test machines, using an .aof file is optional. Structure of Redis Data The key for each WorkItem in Redis is the WorkItem ID. The value is a hash of subkeys. The value for each subkey is a blob of JSON data. Data for an ingest with WorkItem ID 5678 will look like this: \"5678\" \"object:example.edu/BagOfPhotos\": [JSON ~1-3 kb] \"workresult:ingest01_prefetch\": [JSON ~200-500 bytes] \"workresult:ingest02_bag_validation\": [JSON ~200-500 bytes] \"workresult:ingest03_reingest_check\": [JSON ~200-500 bytes ] \"workresult:ingest04_staging\": [JSON ~200-500 bytes ] \"workresult:ingest05_format_identification\": [JSON ~200-500 bytes ] \"workresult:ingest06_storage\": [JSON ~200-500 bytes ] \"workresult:ingest07_storage_validation\": [JSON ~200-500 bytes ] \"workresult:ingest08_record\": [JSON ~200-500 bytes ] \"workresult:ingest09_cleanup\": [JSON ~200-500 bytes ] \"file:test.edu/BagOfPhotos/file001\" [JSON ~1-3 kb] \"file:test.edu/BagOfPhotos/file002\" [JSON ~1-3 kb] ... Lots more files ... \"file:test.edu/BagOfPhotos/file999\" [JSON ~1-3 kb] Redis Memory Usage As you can see from the outline above, there's a direct linear relation between the number of files in a bag and the amount of interim processing data in Redis. In the past, we've ingested bags with over 300,000 files. Ingesting too many files at once can cause Redis to run out of memory. Consider the impact of Redis keep 300,000 3kb JSON blobs in memory at once. Currently, Elasticache does not scale like the rest of our system. We can't add memory to deal with busy periods and then scale back down when the load is light. In our production system, we have chosen to use an Elasticache instance that has more memory than we generally need so that Redis doesn't run out of memory during periods of very heavy ingest. We may be paying an extra $30/month to have more memory then we need, but this is a worthwhile tradeoff, as it has helped the system weather a number of ingest floods without problems. Querying Redis You can query Redis directly using the redis-cli on your local machine. Simply run redis-cli , and the CLI will connect to your local Redis instance running on the default port of 6379. To query Redis/Elasticache in one of the live environments, you'll have to log into a bastion host and run the redis-cli from there. You'll need to look in Parameter Store for the proper Redis/Elasticache endpoint for your current environment. You can then connect with: redis-cli -h <host name or ip> -p 6379 Getting a List of Keys / WorkItems Redis records use WorkItem IDs as the main key, and then various subkeys for records related to that WorkItem ID. To get a list of all WorkItem IDs in Redis, run keys \"*\" You should see output like this: 1) \"24719\" 2) \"24553\" 3) \"24627\" 4) \"24516\" 5) \"24514\" 6) \"24796\" 7) \"24450\" 8) \"24487\" 9) \"24764\" 10) \"24564\" 11) \"24557\" 12) \"24657\" That shows Redis has records for twelve WorkItems with IDs ranging from 24719 to 24657. Note that because keys in Redis are strings, the IDs are strings, not numbers. The records stored under each WorkItem ID are hashes, each with its own key and value. Note Redis documentation warns against running keys \"*\" in production, as it may return millions of results. Because queue sizes are limited in our systems, and because we delete Redis data as soon as ingests complete, we will generally have fewer than 200 keys at any given time. This means it's generally safe to run keys \"*\" in production. However, see the note on listing records for a WorkItem below. Listing Records for a Key / WorkItem To see what info Redis has about WorkItem 24657, run this: hkeys \"24657\" You'll see output like this: 1) \"file:virginia.edu/LibraETD-0r9673755/aptrust-info.txt\" 2) \"workresult:ingest02_bag_validation\" 3) \"file:virginia.edu/LibraETD-0r9673755/tagmanifest-md5.txt\" 4) \"file:virginia.edu/LibraETD-0r9673755/manifest-sha256.txt\" 5) \"file:virginia.edu/LibraETD-0r9673755/data/Phylindia Gant MA 2016\" 6) \"file:virginia.edu/LibraETD-0r9673755/tagmanifest-sha256.txt\" 7) \"file:virginia.edu/LibraETD-0r9673755/data/work.json\" 8) \"file:virginia.edu/LibraETD-0r9673755/bagit.txt\" 9) \"file:virginia.edu/LibraETD-0r9673755/manifest-md5.txt\" 10) \"file:virginia.edu/LibraETD-0r9673755/data/embargo.json\" 11) \"workresult:ingest01_prefetch\" 12) \"file:virginia.edu/LibraETD-0r9673755/bag-info.txt\" 13) \"object:virginia.edu/LibraETD-0r9673755\" 14) \"file:virginia.edu/LibraETD-0r9673755/data/fileset-1.json\" 15) \"file:virginia.edu/LibraETD-0r9673755/data/Lewis_Gwendolyn_2015.pdf\" Note that the keys follow a pattern. Those beginning with file: contain information about a file being ingested. Those beginning with workresult: contain info about the result of a step of the ingest process. So workresult:ingest01_prefetch has info about the result of the pre-fetch step of ingest, while workresult:ingest02_bag_validation has info about the validation step. Each ingest WorkItem has a single object: key, composed of the object prefix plus the object identifier. In the case above, it's object:virginia.edu/LibraETD-0r9673755 . This key contains information about the object and is discussed below. Note Some objects contain hundreds of thousands of files. Running hkeys on those will return more info than you want. Try querying the object record first and looking at the file count, as described under Examining an Object Record below. Examining a Work Result Record All of the Redis data is stored in JSON format. Let's see what's in workresult:ingest01_prefetch . We can do that by running the hget command. Note that both the key (24657) and the field (workresult:ingest01_prefetch) must be quoted. hget \"24657\" \"workresult:ingest01_prefetch\" { \"attempt\" : 2 , \"operation\" : \"ingest01_prefetch\" , \"host\" : \"ip-10-0-79-215.ec2.internal\" , \"pid\" : 1 , \"started_at\" : \"2022-10-20T17:37:16.742934015Z\" , \"finished_at\" : \"2022-10-20T17:37:17.665739918Z\" , \"errors\" : null } This tells us that after two attempts, the pre-fetch worker succeeded (because there are no errors). We see when the worker started and finished, and the host field tells us which worker did the work. Examining a File Record Now let's look at a file record. hget \"24657\" \"file:virginia.edu/LibraETD-0r9673755/data/Lewis_Gwendolyn_2015.pdf\" { \"checksums\" : [{ \"algorithm\" : \"md5\" , \"datetime\" : \"2022-10-20T17:36:14.854845867Z\" , \"digest\" : \"813f03c186f0cf350e4a7b23eb1c757e\" , \"source\" : \"manifest\" }], \"copied_to_staging_at\" : \"2022-11-28T03:00:00Z\" , \"file_format\" : \"image/png\" \"format_identified_by\" : \"siegfried\" , \"format_identified_at\" : \"2022-11-28T02:00:00Z\" , \"format_match_type\" : \"signature\" , \"file_modified\" : \"2022-11-26T00:00:00Z\" , \"is_reingest\" : false , \"needs_save\" : true , \"object_identifier\" : \"virginia.edu/LibraETD-0r9673755\" , \"path_in_bag\" : \"data/Lewis_Gwendolyn_2015.pdf\" , \"registry_urls\" : [], \"saved_to_registry_at\" : \"0001-01-01T00:00:00Z\" , \"size\" : 0 , \"storage_option\" : \"Standard\" , \"storage_records\" : [], \"uuid\" : \"bf9d5811-0ad6-43f1-8406-83e14735b4a1\" } This tells us that the file arrived with one checksum in the bag manifest, an md5. It also tells us the UUID that will become the file name in the preservation bucket. This file is bound for Standard storage, but has not yet been copied to staging, run through format identification, or saved to the registry, as those timestamps are all empty. This file record will expand as additional workers do their processing. The validation worker will add its own checksums to the checksum list (with source = \"ingest\" instead of source = \"manifest\" so we know that we calculated the checksums). The reingest worker will see if this file has ever been ingested before, and may change needs_save to false if the checksum matches the checksum of the copy we already have. Examining an Object Record Runing this command to get the object record: hget \"24657\" \"object:virginia.edu/LibraETD-0r9673755\" We get the following results: { \"copied_to_staging_at\" : \"0001-01-01T00:00:00Z\" , \"deleted_from_receiving_at\" : \"0001-01-01T00:00:00Z\" , \"etag\" : \"ed17d88a33e8c095ab3674e08184b398\" , \"file_count\" : 11 , \"has_fetch_txt\" : false , \"institution\" : \"virginia.edu\" , \"institution_id\" : 34 , \"is_reingest\" : false , \"manifests\" : [ \"md5\" , \"sha256\" ], \"parsable_tag_files\" : [ \"bagit.txt\" , \"bag-info.txt\" , \"aptrust-info.txt\" ], \"recheck_registry_identifiers\" : false , \"s3_bucket\" : \"aptrust.receiving.test.virginia.edu\" , \"s3_key\" : \"LibraETD-0r9673755.tar\" , \"saved_to_registry_at\" : \"0001-01-01T00:00:00Z\" , \"serialization\" : \"application/tar\" , \"should_delete_from_receiving\" : true , \"size\" : 1576960 , \"storage_option\" : \"Standard\" , \"tag_files\" : [ \"bagit.txt\" , \"bag-info.txt\" , \"aptrust-info.txt\" ], \"tag_manifests\" : [ \"md5\" , \"sha256\" ], \"tags\" : [{ \"tag_file\" : \"bagit.txt\" , \"tag_name\" : \"BagIt-Version\" , \"value\" : \"0.97\" }, { \"tag_file\" : \"bagit.txt\" , \"tag_name\" : \"Tag-File-Character-Encoding\" , \"value\" : \"UTF-8\" }, { \"tag_file\" : \"bag-info.txt\" , \"tag_name\" : \"Bag-Count\" , \"value\" : \"1\" }, { \"tag_file\" : \"bag-info.txt\" , \"tag_name\" : \"Bag-Group-Identifier\" , \"value\" : \"LibraETD\" }, { \"tag_file\" : \"bag-info.txt\" , \"tag_name\" : \"Bag-Software-Agent\" , \"value\" : \"bagit.py v1.8.1 u003chttps://github.com/LibraryOfCongress/bagit-pythonu003e\" }, { \"tag_file\" : \"bag-info.txt\" , \"tag_name\" : \"Bagging-Date\" , \"value\" : \"2022-06-23\" }, { \"tag_file\" : \"bag-info.txt\" , \"tag_name\" : \"Internal-Sender-Description\" , \"value\" : \"\" \"\" }, { \"tag_file\" : \"bag-info.txt\" , \"tag_name\" : \"Internal-Sender-Identifier\" , \"value\" : \"0r9673755\" }, { \"tag_file\" : \"bag-info.txt\" , \"tag_name\" : \"Payload-Oxum\" , \"value\" : \"1553270.4\" }, { \"tag_file\" : \"bag-info.txt\" , \"tag_name\" : \"Source-Organization\" , \"value\" : \"virginia.edu\" }, { \"tag_file\" : \"aptrust-info.txt\" , \"tag_name\" : \"Title\" , \"value\" : \"\" Alluvial Fa ns as Po tent ial Si tes f or Preserva t io n o f Bio - sig natures o n Mars \"\" }, { \"tag_file\" : \"aptrust-info.txt\" , \"tag_name\" : \"Description\" , \"value\" : \"\" Wa ter is a n impor tant ce ntral t heme i n as tr obiology.I n order f or li fe as we k n ow i t t o suppor t i tself o n a n o t her pla net , a o f wa ter is required.There are several s o f wa ter t ha t have bee n discovered o n Mars.A t prese nt , t he majori t y o f t he s o f wa ter are u na vailable , bu t t here were periods i n Marsxe 2 x 80 x 99 geologic his t ory t ha t wa ter was available f or li fe .The ne x t s te p i n discoveri n g li fe is looki n g f or bio - sig natures .Bio - sig natures ca n be a n y t hi n g fr om iso t opes t o eleme nts t o DNA.A me t hod t o preserve t he bio - sig natures is ne cessary t o s tu dy t he bio - sig natures i n prese nt day.Preserved bio - sig natures keep i nf orma t io n abou t ex t i n c t Mars habi ta bili t y i nta c t t o s tu dy furt her whe n f ou n d.O ne naturall y occurri n g way t o preserve bio - sig natures is clay hydrogels.Clay hydrogels are bio - polymers t ha t f orm whe n clay mi nerals come i n co nta c t wi t h a n d i ntera c t wi t h wa ter .The clay a n d wa ter crea te a co nf i n i n g e n viro n me nt pro te c t i n g DNA a n d RNA fun c t io ns i ns ide t he polymer.Po tent ial si tes t o look f or clay hydrogels are alluvial fans , ple nt i ful i n t he sou t her n hemisphere o f Mars.Alluvial fans are charac ter ized as la n d f orms t ha t make a semi co n ical shape ou t o f sedime nt deposi ts f ormi n g as wa ter carries t he sedime nt fr om a s tee p slope ups trea m t o a n u n co nf i ne d drai na ge ou tlet dow nstrea m , o ften i n flatter terra i n .Due t o t heir mi neral composi t io n , wa ter origi n , a n d loca t io n t hrough - ou t Mars , alluvial fans are t he per fe c t si te t o i n ves t iga te i n t he search f or preserved bio - sig natures . \"\" }, { \"tag_file\" : \"aptrust-info.txt\" , \"tag_name\" : \"Access\" , \"value\" : \"Consortia\" }, { \"tag_file\" : \"aptrust-info.txt\" , \"tag_name\" : \"Storage\" , \"value\" : \"Standard\" }, { \"tag_file\" : \"aptrust-info.txt\" , \"tag_name\" : \"Storage-Option\" , \"value\" : \"Standard\" }] } There's quite a bit of information here. Most of it is used during bag validation, and then copied into the registry when ingest completes. Note that a WorkItem's object record is visible to APTrust admins on the Registry's WorkItem detail page. The object's size and file count attributes can be useful in determining why an object moves slowly through the ingest process. Multi-terabyte objects take a long time to read and copy, while bags containing tens of thousands of files incur enormous overhead (checksumming, format identification, re-ingest lookups, copying to S3, etc. have to be performed on each file).","title":"Redis"},{"location":"components/redis/#redis","text":"Preserv uses Redis (running as Elasticache in AWS) to hold interim processing data that is accessible to all ingest workers. The first worker in the ingest pipeline, ingest_pre_fetch , reads the contents of a tarred bag from an S3 receiving bucket and records metadata, including: General info about the bag, including its name, owning institution, size, number of payload files, and more. The object info also includes parsed versions of the bag's tag files. General info about each file in the tarred bag, including it's path, size, and checksums from the bag's manifests. A \"work result\" describing when the worker started its task, when it completed, and any errors it encountered along the way. The ingest_pre_fetch worker dumps this data into Redis, in JSON format, so the next worker can access it. Each successive worker adds information to the object, file, and work result records in Redis so subsequent workers know what to do with the incoming content. For example, the reingest_manager checks to see if we already have copies of a bag's files in preservation storage. If so, and if the checksums have not changed, it flags the files as not needing to be saved. The later workers ingest_preservation_uploader and ingest_preservation_verifier know not to copy these files to preservation storage and not to bother verifying the copies. The ingest_recorder knows not to record new ingest events or checksums for these files. We chose Redis as our interim processing cache for a number of reasons: The workers often perform huge volumes of reads and writes that would bog down a relational database. The data naturally fits into a key-value data store. We don't want or need to decompose the JSON into searchable tables or MongoDB-type documents. We always want the whole blob. Interim processing data is ephemeral. We may retain it for a few seconds in the case of small bags, or 2-3 days in the case of larger bags. In most cases, we keep it for 1-10 minutes. Redis' disk-backed \"aof\" file, which allows in-memory data to persist through reboots, is enough of a durability guarantee for our needs. By the time a bag reaches the ingest_recorder worker, the JSON data in Redis contains all of the information we will need to record in the Registry, including: The object record, with its title, description, owning institution and other metadata. All of the file records, and there can be tens of thousands of these. All checksums, premis events, and storage records for all files. Note On live systems, we configure Redis to write to an .aof file (append-only file) so that data persists through system reboots. On dev and test machines, using an .aof file is optional.","title":"Redis"},{"location":"components/redis/#structure-of-redis-data","text":"The key for each WorkItem in Redis is the WorkItem ID. The value is a hash of subkeys. The value for each subkey is a blob of JSON data. Data for an ingest with WorkItem ID 5678 will look like this: \"5678\" \"object:example.edu/BagOfPhotos\": [JSON ~1-3 kb] \"workresult:ingest01_prefetch\": [JSON ~200-500 bytes] \"workresult:ingest02_bag_validation\": [JSON ~200-500 bytes] \"workresult:ingest03_reingest_check\": [JSON ~200-500 bytes ] \"workresult:ingest04_staging\": [JSON ~200-500 bytes ] \"workresult:ingest05_format_identification\": [JSON ~200-500 bytes ] \"workresult:ingest06_storage\": [JSON ~200-500 bytes ] \"workresult:ingest07_storage_validation\": [JSON ~200-500 bytes ] \"workresult:ingest08_record\": [JSON ~200-500 bytes ] \"workresult:ingest09_cleanup\": [JSON ~200-500 bytes ] \"file:test.edu/BagOfPhotos/file001\" [JSON ~1-3 kb] \"file:test.edu/BagOfPhotos/file002\" [JSON ~1-3 kb] ... Lots more files ... \"file:test.edu/BagOfPhotos/file999\" [JSON ~1-3 kb]","title":"Structure of Redis Data"},{"location":"components/redis/#redis-memory-usage","text":"As you can see from the outline above, there's a direct linear relation between the number of files in a bag and the amount of interim processing data in Redis. In the past, we've ingested bags with over 300,000 files. Ingesting too many files at once can cause Redis to run out of memory. Consider the impact of Redis keep 300,000 3kb JSON blobs in memory at once. Currently, Elasticache does not scale like the rest of our system. We can't add memory to deal with busy periods and then scale back down when the load is light. In our production system, we have chosen to use an Elasticache instance that has more memory than we generally need so that Redis doesn't run out of memory during periods of very heavy ingest. We may be paying an extra $30/month to have more memory then we need, but this is a worthwhile tradeoff, as it has helped the system weather a number of ingest floods without problems.","title":"Redis Memory Usage"},{"location":"components/redis/#querying-redis","text":"You can query Redis directly using the redis-cli on your local machine. Simply run redis-cli , and the CLI will connect to your local Redis instance running on the default port of 6379. To query Redis/Elasticache in one of the live environments, you'll have to log into a bastion host and run the redis-cli from there. You'll need to look in Parameter Store for the proper Redis/Elasticache endpoint for your current environment. You can then connect with: redis-cli -h <host name or ip> -p 6379","title":"Querying Redis"},{"location":"components/redis/#getting-a-list-of-keys-workitems","text":"Redis records use WorkItem IDs as the main key, and then various subkeys for records related to that WorkItem ID. To get a list of all WorkItem IDs in Redis, run keys \"*\" You should see output like this: 1) \"24719\" 2) \"24553\" 3) \"24627\" 4) \"24516\" 5) \"24514\" 6) \"24796\" 7) \"24450\" 8) \"24487\" 9) \"24764\" 10) \"24564\" 11) \"24557\" 12) \"24657\" That shows Redis has records for twelve WorkItems with IDs ranging from 24719 to 24657. Note that because keys in Redis are strings, the IDs are strings, not numbers. The records stored under each WorkItem ID are hashes, each with its own key and value. Note Redis documentation warns against running keys \"*\" in production, as it may return millions of results. Because queue sizes are limited in our systems, and because we delete Redis data as soon as ingests complete, we will generally have fewer than 200 keys at any given time. This means it's generally safe to run keys \"*\" in production. However, see the note on listing records for a WorkItem below.","title":"Getting a List of Keys / WorkItems"},{"location":"components/redis/#listing-records-for-a-key-workitem","text":"To see what info Redis has about WorkItem 24657, run this: hkeys \"24657\" You'll see output like this: 1) \"file:virginia.edu/LibraETD-0r9673755/aptrust-info.txt\" 2) \"workresult:ingest02_bag_validation\" 3) \"file:virginia.edu/LibraETD-0r9673755/tagmanifest-md5.txt\" 4) \"file:virginia.edu/LibraETD-0r9673755/manifest-sha256.txt\" 5) \"file:virginia.edu/LibraETD-0r9673755/data/Phylindia Gant MA 2016\" 6) \"file:virginia.edu/LibraETD-0r9673755/tagmanifest-sha256.txt\" 7) \"file:virginia.edu/LibraETD-0r9673755/data/work.json\" 8) \"file:virginia.edu/LibraETD-0r9673755/bagit.txt\" 9) \"file:virginia.edu/LibraETD-0r9673755/manifest-md5.txt\" 10) \"file:virginia.edu/LibraETD-0r9673755/data/embargo.json\" 11) \"workresult:ingest01_prefetch\" 12) \"file:virginia.edu/LibraETD-0r9673755/bag-info.txt\" 13) \"object:virginia.edu/LibraETD-0r9673755\" 14) \"file:virginia.edu/LibraETD-0r9673755/data/fileset-1.json\" 15) \"file:virginia.edu/LibraETD-0r9673755/data/Lewis_Gwendolyn_2015.pdf\" Note that the keys follow a pattern. Those beginning with file: contain information about a file being ingested. Those beginning with workresult: contain info about the result of a step of the ingest process. So workresult:ingest01_prefetch has info about the result of the pre-fetch step of ingest, while workresult:ingest02_bag_validation has info about the validation step. Each ingest WorkItem has a single object: key, composed of the object prefix plus the object identifier. In the case above, it's object:virginia.edu/LibraETD-0r9673755 . This key contains information about the object and is discussed below. Note Some objects contain hundreds of thousands of files. Running hkeys on those will return more info than you want. Try querying the object record first and looking at the file count, as described under Examining an Object Record below.","title":"Listing Records for a Key / WorkItem"},{"location":"components/redis/#examining-a-work-result-record","text":"All of the Redis data is stored in JSON format. Let's see what's in workresult:ingest01_prefetch . We can do that by running the hget command. Note that both the key (24657) and the field (workresult:ingest01_prefetch) must be quoted. hget \"24657\" \"workresult:ingest01_prefetch\" { \"attempt\" : 2 , \"operation\" : \"ingest01_prefetch\" , \"host\" : \"ip-10-0-79-215.ec2.internal\" , \"pid\" : 1 , \"started_at\" : \"2022-10-20T17:37:16.742934015Z\" , \"finished_at\" : \"2022-10-20T17:37:17.665739918Z\" , \"errors\" : null } This tells us that after two attempts, the pre-fetch worker succeeded (because there are no errors). We see when the worker started and finished, and the host field tells us which worker did the work.","title":"Examining a Work Result Record"},{"location":"components/redis/#examining-a-file-record","text":"Now let's look at a file record. hget \"24657\" \"file:virginia.edu/LibraETD-0r9673755/data/Lewis_Gwendolyn_2015.pdf\" { \"checksums\" : [{ \"algorithm\" : \"md5\" , \"datetime\" : \"2022-10-20T17:36:14.854845867Z\" , \"digest\" : \"813f03c186f0cf350e4a7b23eb1c757e\" , \"source\" : \"manifest\" }], \"copied_to_staging_at\" : \"2022-11-28T03:00:00Z\" , \"file_format\" : \"image/png\" \"format_identified_by\" : \"siegfried\" , \"format_identified_at\" : \"2022-11-28T02:00:00Z\" , \"format_match_type\" : \"signature\" , \"file_modified\" : \"2022-11-26T00:00:00Z\" , \"is_reingest\" : false , \"needs_save\" : true , \"object_identifier\" : \"virginia.edu/LibraETD-0r9673755\" , \"path_in_bag\" : \"data/Lewis_Gwendolyn_2015.pdf\" , \"registry_urls\" : [], \"saved_to_registry_at\" : \"0001-01-01T00:00:00Z\" , \"size\" : 0 , \"storage_option\" : \"Standard\" , \"storage_records\" : [], \"uuid\" : \"bf9d5811-0ad6-43f1-8406-83e14735b4a1\" } This tells us that the file arrived with one checksum in the bag manifest, an md5. It also tells us the UUID that will become the file name in the preservation bucket. This file is bound for Standard storage, but has not yet been copied to staging, run through format identification, or saved to the registry, as those timestamps are all empty. This file record will expand as additional workers do their processing. The validation worker will add its own checksums to the checksum list (with source = \"ingest\" instead of source = \"manifest\" so we know that we calculated the checksums). The reingest worker will see if this file has ever been ingested before, and may change needs_save to false if the checksum matches the checksum of the copy we already have.","title":"Examining a File Record"},{"location":"components/redis/#examining-an-object-record","text":"Runing this command to get the object record: hget \"24657\" \"object:virginia.edu/LibraETD-0r9673755\" We get the following results: { \"copied_to_staging_at\" : \"0001-01-01T00:00:00Z\" , \"deleted_from_receiving_at\" : \"0001-01-01T00:00:00Z\" , \"etag\" : \"ed17d88a33e8c095ab3674e08184b398\" , \"file_count\" : 11 , \"has_fetch_txt\" : false , \"institution\" : \"virginia.edu\" , \"institution_id\" : 34 , \"is_reingest\" : false , \"manifests\" : [ \"md5\" , \"sha256\" ], \"parsable_tag_files\" : [ \"bagit.txt\" , \"bag-info.txt\" , \"aptrust-info.txt\" ], \"recheck_registry_identifiers\" : false , \"s3_bucket\" : \"aptrust.receiving.test.virginia.edu\" , \"s3_key\" : \"LibraETD-0r9673755.tar\" , \"saved_to_registry_at\" : \"0001-01-01T00:00:00Z\" , \"serialization\" : \"application/tar\" , \"should_delete_from_receiving\" : true , \"size\" : 1576960 , \"storage_option\" : \"Standard\" , \"tag_files\" : [ \"bagit.txt\" , \"bag-info.txt\" , \"aptrust-info.txt\" ], \"tag_manifests\" : [ \"md5\" , \"sha256\" ], \"tags\" : [{ \"tag_file\" : \"bagit.txt\" , \"tag_name\" : \"BagIt-Version\" , \"value\" : \"0.97\" }, { \"tag_file\" : \"bagit.txt\" , \"tag_name\" : \"Tag-File-Character-Encoding\" , \"value\" : \"UTF-8\" }, { \"tag_file\" : \"bag-info.txt\" , \"tag_name\" : \"Bag-Count\" , \"value\" : \"1\" }, { \"tag_file\" : \"bag-info.txt\" , \"tag_name\" : \"Bag-Group-Identifier\" , \"value\" : \"LibraETD\" }, { \"tag_file\" : \"bag-info.txt\" , \"tag_name\" : \"Bag-Software-Agent\" , \"value\" : \"bagit.py v1.8.1 u003chttps://github.com/LibraryOfCongress/bagit-pythonu003e\" }, { \"tag_file\" : \"bag-info.txt\" , \"tag_name\" : \"Bagging-Date\" , \"value\" : \"2022-06-23\" }, { \"tag_file\" : \"bag-info.txt\" , \"tag_name\" : \"Internal-Sender-Description\" , \"value\" : \"\" \"\" }, { \"tag_file\" : \"bag-info.txt\" , \"tag_name\" : \"Internal-Sender-Identifier\" , \"value\" : \"0r9673755\" }, { \"tag_file\" : \"bag-info.txt\" , \"tag_name\" : \"Payload-Oxum\" , \"value\" : \"1553270.4\" }, { \"tag_file\" : \"bag-info.txt\" , \"tag_name\" : \"Source-Organization\" , \"value\" : \"virginia.edu\" }, { \"tag_file\" : \"aptrust-info.txt\" , \"tag_name\" : \"Title\" , \"value\" : \"\" Alluvial Fa ns as Po tent ial Si tes f or Preserva t io n o f Bio - sig natures o n Mars \"\" }, { \"tag_file\" : \"aptrust-info.txt\" , \"tag_name\" : \"Description\" , \"value\" : \"\" Wa ter is a n impor tant ce ntral t heme i n as tr obiology.I n order f or li fe as we k n ow i t t o suppor t i tself o n a n o t her pla net , a o f wa ter is required.There are several s o f wa ter t ha t have bee n discovered o n Mars.A t prese nt , t he majori t y o f t he s o f wa ter are u na vailable , bu t t here were periods i n Marsxe 2 x 80 x 99 geologic his t ory t ha t wa ter was available f or li fe .The ne x t s te p i n discoveri n g li fe is looki n g f or bio - sig natures .Bio - sig natures ca n be a n y t hi n g fr om iso t opes t o eleme nts t o DNA.A me t hod t o preserve t he bio - sig natures is ne cessary t o s tu dy t he bio - sig natures i n prese nt day.Preserved bio - sig natures keep i nf orma t io n abou t ex t i n c t Mars habi ta bili t y i nta c t t o s tu dy furt her whe n f ou n d.O ne naturall y occurri n g way t o preserve bio - sig natures is clay hydrogels.Clay hydrogels are bio - polymers t ha t f orm whe n clay mi nerals come i n co nta c t wi t h a n d i ntera c t wi t h wa ter .The clay a n d wa ter crea te a co nf i n i n g e n viro n me nt pro te c t i n g DNA a n d RNA fun c t io ns i ns ide t he polymer.Po tent ial si tes t o look f or clay hydrogels are alluvial fans , ple nt i ful i n t he sou t her n hemisphere o f Mars.Alluvial fans are charac ter ized as la n d f orms t ha t make a semi co n ical shape ou t o f sedime nt deposi ts f ormi n g as wa ter carries t he sedime nt fr om a s tee p slope ups trea m t o a n u n co nf i ne d drai na ge ou tlet dow nstrea m , o ften i n flatter terra i n .Due t o t heir mi neral composi t io n , wa ter origi n , a n d loca t io n t hrough - ou t Mars , alluvial fans are t he per fe c t si te t o i n ves t iga te i n t he search f or preserved bio - sig natures . \"\" }, { \"tag_file\" : \"aptrust-info.txt\" , \"tag_name\" : \"Access\" , \"value\" : \"Consortia\" }, { \"tag_file\" : \"aptrust-info.txt\" , \"tag_name\" : \"Storage\" , \"value\" : \"Standard\" }, { \"tag_file\" : \"aptrust-info.txt\" , \"tag_name\" : \"Storage-Option\" , \"value\" : \"Standard\" }] } There's quite a bit of information here. Most of it is used during bag validation, and then copied into the registry when ingest completes. Note that a WorkItem's object record is visible to APTrust admins on the Registry's WorkItem detail page. The object's size and file count attributes can be useful in determining why an object moves slowly through the ingest process. Multi-terabyte objects take a long time to read and copy, while bags containing tens of thousands of files incur enormous overhead (checksumming, format identification, re-ingest lookups, copying to S3, etc. have to be performed on each file).","title":"Examining an Object Record"},{"location":"components/registry/","text":"Registry As far as Preserv is concerned, Registry is a REST API containing the following info. WorkItems Registry workers get WorkItem IDs from NSQ topics, and then look up those work items through the Registry REST API. Each worker determines whether it should work on the item, based on data it contains. Workers may reject items if they see any of the following info in the WorkItem record: The item has a non-empty node and pid, indicating it is already in process by another worker. The item's Action or Stage does not match the Action and Stage the worker should be working on. The item is in some final state, such as Cleanup/Succeeded or Cancelled . In all of these cases, the worker will drop the item without doing any work. If a Preserv worker chooses to work on an item, it updates the WorkItem record in the Registry with its Node and PID, and changes the status from Pending to Started . Fixity Check Work Items Unlike all other queue topics, the fixity check topic does not contain WorkItem IDs. It contains GenericFile IDs. These are the IDs of files that need a scheduled fixity check. The fixity work retrieves the Generic File record from Registry, ensures the file has not been checked in 90 days, then runs its check. When it's done, it creates a Premis Event describing the result of the check. Objects, Files, Storage Records, Checksums and Events During ingest, the re-ingest checker pulls Intellectual Object and Generic Files from Registry to see if an object or its component files has ever been ingested before. See the Reingest Manager page for details on that. Near the end of the ingest process, the Recorder saves data about the ingest to the Registry, including: The Intellectual Object record. One Generic File record for each file in the object. One Storage Record for each copy of the Generic File. These point to where the file is stored in preservation. Most storage options result in a single storage record. The Standard option results in two records: one for S3 in Virginia and one for Glacier in Oregon. Checksums for each file ingested. The old system (Exchange) recorded only md5 and sha256 checksums. The new system records md5, sha1, sha256 and sha512 checksums for each file. Note that we only check the sha256 during scheduled fixity checks. A set of Premis Events for each file, including: One fixity check indicating the file's initial fixity matched what's in the bag's manifest. One identifier assignment describing the file's URL in preservation storage. A second identifier assignment, if the file is in Standard storage, describing the URL of the file's second copy in Oregon. One ingestion event, indicating the file has been ingested into preservation storage. One replication event, if the file is in Standard storage, indicating a second copy of the file was saved to Glacier/Oregon. Four message digest calculations: one each for md5, sha1, sha256 and sha512 A set of Premis Events for the object itself, including: Ingestion of the object into preservation storage. Creation of the object record in Registry. Identifier assignment, indicating the assignment of this object's unique identifier in Registry. Access assignment, based on the value of the Access tag in the bag's aptrust-info.txt file.","title":"Registry"},{"location":"components/registry/#registry","text":"As far as Preserv is concerned, Registry is a REST API containing the following info.","title":"Registry"},{"location":"components/registry/#workitems","text":"Registry workers get WorkItem IDs from NSQ topics, and then look up those work items through the Registry REST API. Each worker determines whether it should work on the item, based on data it contains. Workers may reject items if they see any of the following info in the WorkItem record: The item has a non-empty node and pid, indicating it is already in process by another worker. The item's Action or Stage does not match the Action and Stage the worker should be working on. The item is in some final state, such as Cleanup/Succeeded or Cancelled . In all of these cases, the worker will drop the item without doing any work. If a Preserv worker chooses to work on an item, it updates the WorkItem record in the Registry with its Node and PID, and changes the status from Pending to Started .","title":"WorkItems"},{"location":"components/registry/#fixity-check-work-items","text":"Unlike all other queue topics, the fixity check topic does not contain WorkItem IDs. It contains GenericFile IDs. These are the IDs of files that need a scheduled fixity check. The fixity work retrieves the Generic File record from Registry, ensures the file has not been checked in 90 days, then runs its check. When it's done, it creates a Premis Event describing the result of the check.","title":"Fixity Check Work Items"},{"location":"components/registry/#objects-files-storage-records-checksums-and-events","text":"During ingest, the re-ingest checker pulls Intellectual Object and Generic Files from Registry to see if an object or its component files has ever been ingested before. See the Reingest Manager page for details on that. Near the end of the ingest process, the Recorder saves data about the ingest to the Registry, including: The Intellectual Object record. One Generic File record for each file in the object. One Storage Record for each copy of the Generic File. These point to where the file is stored in preservation. Most storage options result in a single storage record. The Standard option results in two records: one for S3 in Virginia and one for Glacier in Oregon. Checksums for each file ingested. The old system (Exchange) recorded only md5 and sha256 checksums. The new system records md5, sha1, sha256 and sha512 checksums for each file. Note that we only check the sha256 during scheduled fixity checks. A set of Premis Events for each file, including: One fixity check indicating the file's initial fixity matched what's in the bag's manifest. One identifier assignment describing the file's URL in preservation storage. A second identifier assignment, if the file is in Standard storage, describing the URL of the file's second copy in Oregon. One ingestion event, indicating the file has been ingested into preservation storage. One replication event, if the file is in Standard storage, indicating a second copy of the file was saved to Glacier/Oregon. Four message digest calculations: one each for md5, sha1, sha256 and sha512 A set of Premis Events for the object itself, including: Ingestion of the object into preservation storage. Creation of the object record in Registry. Identifier assignment, indicating the assignment of this object's unique identifier in Registry. Access assignment, based on the value of the Access tag in the bag's aptrust-info.txt file.","title":"Objects, Files, Storage Records, Checksums and Events"},{"location":"components/s3/","text":"S3 Preserv uses S3 to recieve bags from depositors, to store them in long-term preservation, and to restore files and objects to depositors. Preserv also uses a staging bucket to process items during ingest. In addition, Glacier, Glacier Deep Archive, and Wasabi provide preservation storage buckets. Receiving Buckets To deposit materials into APTrust, depositors upload tarred bags to their receiving bucket. A cron job periodically scans receiving buckets for new bags, creating a Registry WorkItem and an NSQ message for each new bag. Receiving buckets are accessible to APTrust users and admins at the owning institution. Bucket names follow the patterns below. In each case, inst identifitier is the institution's identifier, which also happens to be its domain name (virginia.edu, georgetown.edu, etc.). aptrust.receiving.<inst identifier> # Production Environment aptrust.receiving.test.<inst identifier> # Demo Environment aptrust.receiving.staging.<inst identifier> # Staging Environment Restoration Buckets Users and admins at depositing institutions also have access to their institution's restoration bucket. When a depositor requests a restoration, Preserv copies files and objects to this bucket for depositors to download. Restoration buckets follow this naming scheme: aptrust.restore.<inst identifier> # Production Environment aptrust.restore.test.<inst identifier> # Demo Environment aptrust.restore.staging.<inst identifier> # Staging Environment When Preserv restores an entire object, it creates a new bag from all of the object's preserved files and puts the bag in the depositor's restoration bucket. If the University of Virginia were to restore an object called BagOfPhotos, it would appear in the restoration bucket like this: aptrust.restore.virginia.edu/BagOfPhotos.tar When restoring a single file from that bag, it would appear in a subdirectory like this: aptrust.restore.virginia.edu/BagOfPhotos/data/photo.jpg Staging Bucket During ingest, the ingest_staging_uploader extracts files from tarred bags in the receiving buckets and copies them into a staging bucket for further processing. The staging bucket is not public and is not accessible to any depositors. Files remain in the staging bucket until ingest completes, at which point the ingest_cleaup worker deletes them. If a bag fails ingest due to too many transient errors (usually network errors), its files will remain in the staging bucket so that when an APTrust admin requeues the WorkItem, Preserv can pick up where it left off and continue the ingest. The staging buckets are: aptrust.prod.staging # Production environment aptrust.test.staging # Demo environment aptrust.staging.staging # Staging environment If an item fails ingest due to transient errors and an APTrust admin decides to cancel the ingest instead of requeueing it, the admin will need to manually delete the files from staging. Inside the staging bucket is a folder for each Ingest WorkItem in process. Inside that folder are the bag's manifests, tag manifests, and tag files, as well as the bag's payload files. Manifests and tag files are stored under the same name they had in the original bag (bag-info.txt, manifest-sha256.txt, etc.). Payload files are stored with UUID names instead of their original paths. They'll have these same UUID names when copied to preservation storage. The staging bucket entries for WorkItem 5432 on the demo system would look like this: aptrust.test.staging/5432 aptrust.test.staging/5432/aptrust-info.txt aptrust.test.staging/5432/bag-info.txt aptrust.test.staging/5432/manifest-sha256.txt aptrust.test.staging/5432/1e3fa668-73b7-4885-98e1-f16170b7ad54 aptrust.test.staging/5432/49c5ebc5-e840-4fcb-b851-c8f02cd4953d aptrust.test.staging/5432/5a757826-31a1-406d-a889-83c05c245213 aptrust.test.staging/5432/e6b32693-6ed9-4974-946c-0ce1808015aa Preservation Buckets Preservation buckets are for long-term preservation storage. They are not publicly accessible and nothing inside them is publicly accessible. APTrust admins can list bucket contents and retrieve files but cannot delete anything from these buckets. All deletions must be done by the depositing insitution, through the Registry, and all must be approved by an administrator at the depositing institution before Preserv will carry them out. Files in the preservation buckets are stored with UUID names and have the following metadata: Name Description x-amz-meta-md5 The md5 checksum we calculated for this file on the most recent ingest. x-amz-meta-sha256 The sha256 checksum we calculated for this file on the most recent ingest. x-amz-meta-bagpath The original path the file inside the bag that the depositor submitted. E.g. data/photos/image01.jpg . Note : In Wasabi, this field is called x-amz-meta-bagpath-encoded , due to a bug in Wasabi's parsing of HTTP headers. Wasabi will not accept any header with two or more consecutive spaces, which we sometimes encounter in depositor file names. Then encoded version of the bagpath uses URL query string encoding to replace spaces with %20. x-amz-meta-institution The identifier (domain name) of the institution that owns the bag. x-amz-meta-bag The Registry's identifier for the bag to which this file belongs. E.g. virginia.edu/BagOfPhotos . Note Due to an early depositor decision back in 2014, preservation buckets generally do not use encryption. The exceptions are Glacier archives and Wasabi buckets, where encryption is mandatory. Also due to a 2014 depositor decision, versioning is turned off on all preservation buckets. Newer versions of files overwrite older versions. If a depositor explicitly wants to save two versions of an object, they have to upload bags with names bag BagOfPhotos_v1.tar and BagOfPhotos_v2.tar . Production Preservation Bucket Names Name Storage Option Encrypted Versioned Description aptrust.preservation.storage Standard No No Standard S3 preservation storage bucket in Virginia. Everything in this bucket is replicated to aptrust.preservation.oregon. aptrust.preservation.oregon Standard Yes No Glacier storage for items using the stanard storage option. Everything in here is replicated from aptrust.preservation.storage. aptrust.preservation.glacier-deep.oh Glacier-Deep-OH Yes No Glacier Deep Archive in Ohio. aptrust.preservation.glacier-deep.or Glacier-Deep-OR Yes No Glacier Deep Archive in Oregon. aptrust.preservation.glacier-deep.va Glacier-Deep-VA Yes No Glacier Deep Archive in Virginie. aptrust.preservation.glacier.oh Glacier-OH Yes No Glacier archive in Ohio. aptrust.preservation.glacier.or Glacier-OR Yes No Glacier archive in Oregon. aptrust.preservation.glacier.va Glacier-VA Yes No Glacier archive in Virginia. aptrust-production-wasabi-or Wasabi-OR Yes No Wasabi storage in Oregon. aptrust-production-wasabi-va Wasabi-VA Yes No Wasabi storage in Virginia. Demo Preservation Bucket Names Name Storage Option Encrypted Versioned Description aptrust.test. preservation.storage Standard No No Standard S3 preservation storage bucket in Virginia. Everything in this bucket is replicated to aptrust.test. preservation.oregon. aptrust.test. preservation.oregon Standard Yes No Glacier storage for items using the stanard storage option. Everything in here is replicated from aptrust.test. preservation.storage. aptrust.test. preservation.glacier-deep.oh Glacier-Deep-OH Yes No Glacier Deep Archive in Ohio. aptrust.test. preservation.glacier-deep.or Glacier-Deep-OR Yes No Glacier Deep Archive in Oregon. aptrust.test. preservation.glacier-deep.va Glacier-Deep-VA Yes No Glacier Deep Archive in Virginia. aptrust.test. preservation.glacier.oh Glacier-OH Yes No Glacier archive in Ohio. aptrust.test. preservation.glacier.or Glacier-OR Yes No Glacier archive in Oregon. aptrust.test. preservation.glacier.va Glacier-VA Yes No Glacier archive in Virginia. aptrust-demo-wasabi-or Wasabi-OR Yes No Wasabi storage in Oregon. aptrust-demo-wasabi-va Wasabi-VA Yes No Wasabi storage in Virginia. Staging Preservation Bucket Names Name Storage Option Encrypted Versioned Description aptrust.staging. preservation.storage Standard No No Standard S3 preservation storage bucket in Virginia. Everything in this bucket is replicated to aptrust.staging. preservation.oregon. aptrust.staging. preservation.oregon Standard Yes No Glacier storage for items using the stanard storage option. Everything in here is replicated from aptrust.staging. preservation.storage. aptrust.staging. preservation.glacier-deep.oh Glacier-Deep-OH Yes No Glacier Deep Archive in Ohio. aptrust.staging. preservation.glacier-deep.or Glacier-Deep-OR Yes No Glacier Deep Archive in Oregon. aptrust.staging. preservation.glacier-deep.va Glacier-Deep-VA Yes No Glacier Deep Archive in Virginia. aptrust.staging. preservation.glacier.oh Glacier-OH Yes No Glacier archive in Ohio. aptrust.staging. preservation.glacier.or Glacier-OR Yes No Glacier archive in Oregon. aptrust.staging. preservation.glacier.va Glacier-VA Yes No Glacier archive in Virginia. aptrust-staging-wasabi-or Wasabi-OR Yes No Wasabi storage in Oregon. aptrust-staging-wasabi-va Wasabi-VA Yes No Wasabi storage in Virginia.","title":"S3"},{"location":"components/s3/#s3","text":"Preserv uses S3 to recieve bags from depositors, to store them in long-term preservation, and to restore files and objects to depositors. Preserv also uses a staging bucket to process items during ingest. In addition, Glacier, Glacier Deep Archive, and Wasabi provide preservation storage buckets.","title":"S3"},{"location":"components/s3/#receiving-buckets","text":"To deposit materials into APTrust, depositors upload tarred bags to their receiving bucket. A cron job periodically scans receiving buckets for new bags, creating a Registry WorkItem and an NSQ message for each new bag. Receiving buckets are accessible to APTrust users and admins at the owning institution. Bucket names follow the patterns below. In each case, inst identifitier is the institution's identifier, which also happens to be its domain name (virginia.edu, georgetown.edu, etc.). aptrust.receiving.<inst identifier> # Production Environment aptrust.receiving.test.<inst identifier> # Demo Environment aptrust.receiving.staging.<inst identifier> # Staging Environment","title":"Receiving Buckets"},{"location":"components/s3/#restoration-buckets","text":"Users and admins at depositing institutions also have access to their institution's restoration bucket. When a depositor requests a restoration, Preserv copies files and objects to this bucket for depositors to download. Restoration buckets follow this naming scheme: aptrust.restore.<inst identifier> # Production Environment aptrust.restore.test.<inst identifier> # Demo Environment aptrust.restore.staging.<inst identifier> # Staging Environment When Preserv restores an entire object, it creates a new bag from all of the object's preserved files and puts the bag in the depositor's restoration bucket. If the University of Virginia were to restore an object called BagOfPhotos, it would appear in the restoration bucket like this: aptrust.restore.virginia.edu/BagOfPhotos.tar When restoring a single file from that bag, it would appear in a subdirectory like this: aptrust.restore.virginia.edu/BagOfPhotos/data/photo.jpg","title":"Restoration Buckets"},{"location":"components/s3/#staging-bucket","text":"During ingest, the ingest_staging_uploader extracts files from tarred bags in the receiving buckets and copies them into a staging bucket for further processing. The staging bucket is not public and is not accessible to any depositors. Files remain in the staging bucket until ingest completes, at which point the ingest_cleaup worker deletes them. If a bag fails ingest due to too many transient errors (usually network errors), its files will remain in the staging bucket so that when an APTrust admin requeues the WorkItem, Preserv can pick up where it left off and continue the ingest. The staging buckets are: aptrust.prod.staging # Production environment aptrust.test.staging # Demo environment aptrust.staging.staging # Staging environment If an item fails ingest due to transient errors and an APTrust admin decides to cancel the ingest instead of requeueing it, the admin will need to manually delete the files from staging. Inside the staging bucket is a folder for each Ingest WorkItem in process. Inside that folder are the bag's manifests, tag manifests, and tag files, as well as the bag's payload files. Manifests and tag files are stored under the same name they had in the original bag (bag-info.txt, manifest-sha256.txt, etc.). Payload files are stored with UUID names instead of their original paths. They'll have these same UUID names when copied to preservation storage. The staging bucket entries for WorkItem 5432 on the demo system would look like this: aptrust.test.staging/5432 aptrust.test.staging/5432/aptrust-info.txt aptrust.test.staging/5432/bag-info.txt aptrust.test.staging/5432/manifest-sha256.txt aptrust.test.staging/5432/1e3fa668-73b7-4885-98e1-f16170b7ad54 aptrust.test.staging/5432/49c5ebc5-e840-4fcb-b851-c8f02cd4953d aptrust.test.staging/5432/5a757826-31a1-406d-a889-83c05c245213 aptrust.test.staging/5432/e6b32693-6ed9-4974-946c-0ce1808015aa","title":"Staging Bucket"},{"location":"components/s3/#preservation-buckets","text":"Preservation buckets are for long-term preservation storage. They are not publicly accessible and nothing inside them is publicly accessible. APTrust admins can list bucket contents and retrieve files but cannot delete anything from these buckets. All deletions must be done by the depositing insitution, through the Registry, and all must be approved by an administrator at the depositing institution before Preserv will carry them out. Files in the preservation buckets are stored with UUID names and have the following metadata: Name Description x-amz-meta-md5 The md5 checksum we calculated for this file on the most recent ingest. x-amz-meta-sha256 The sha256 checksum we calculated for this file on the most recent ingest. x-amz-meta-bagpath The original path the file inside the bag that the depositor submitted. E.g. data/photos/image01.jpg . Note : In Wasabi, this field is called x-amz-meta-bagpath-encoded , due to a bug in Wasabi's parsing of HTTP headers. Wasabi will not accept any header with two or more consecutive spaces, which we sometimes encounter in depositor file names. Then encoded version of the bagpath uses URL query string encoding to replace spaces with %20. x-amz-meta-institution The identifier (domain name) of the institution that owns the bag. x-amz-meta-bag The Registry's identifier for the bag to which this file belongs. E.g. virginia.edu/BagOfPhotos . Note Due to an early depositor decision back in 2014, preservation buckets generally do not use encryption. The exceptions are Glacier archives and Wasabi buckets, where encryption is mandatory. Also due to a 2014 depositor decision, versioning is turned off on all preservation buckets. Newer versions of files overwrite older versions. If a depositor explicitly wants to save two versions of an object, they have to upload bags with names bag BagOfPhotos_v1.tar and BagOfPhotos_v2.tar .","title":"Preservation Buckets"},{"location":"components/s3/#production-preservation-bucket-names","text":"Name Storage Option Encrypted Versioned Description aptrust.preservation.storage Standard No No Standard S3 preservation storage bucket in Virginia. Everything in this bucket is replicated to aptrust.preservation.oregon. aptrust.preservation.oregon Standard Yes No Glacier storage for items using the stanard storage option. Everything in here is replicated from aptrust.preservation.storage. aptrust.preservation.glacier-deep.oh Glacier-Deep-OH Yes No Glacier Deep Archive in Ohio. aptrust.preservation.glacier-deep.or Glacier-Deep-OR Yes No Glacier Deep Archive in Oregon. aptrust.preservation.glacier-deep.va Glacier-Deep-VA Yes No Glacier Deep Archive in Virginie. aptrust.preservation.glacier.oh Glacier-OH Yes No Glacier archive in Ohio. aptrust.preservation.glacier.or Glacier-OR Yes No Glacier archive in Oregon. aptrust.preservation.glacier.va Glacier-VA Yes No Glacier archive in Virginia. aptrust-production-wasabi-or Wasabi-OR Yes No Wasabi storage in Oregon. aptrust-production-wasabi-va Wasabi-VA Yes No Wasabi storage in Virginia.","title":"Production Preservation Bucket Names"},{"location":"components/s3/#demo-preservation-bucket-names","text":"Name Storage Option Encrypted Versioned Description aptrust.test. preservation.storage Standard No No Standard S3 preservation storage bucket in Virginia. Everything in this bucket is replicated to aptrust.test. preservation.oregon. aptrust.test. preservation.oregon Standard Yes No Glacier storage for items using the stanard storage option. Everything in here is replicated from aptrust.test. preservation.storage. aptrust.test. preservation.glacier-deep.oh Glacier-Deep-OH Yes No Glacier Deep Archive in Ohio. aptrust.test. preservation.glacier-deep.or Glacier-Deep-OR Yes No Glacier Deep Archive in Oregon. aptrust.test. preservation.glacier-deep.va Glacier-Deep-VA Yes No Glacier Deep Archive in Virginia. aptrust.test. preservation.glacier.oh Glacier-OH Yes No Glacier archive in Ohio. aptrust.test. preservation.glacier.or Glacier-OR Yes No Glacier archive in Oregon. aptrust.test. preservation.glacier.va Glacier-VA Yes No Glacier archive in Virginia. aptrust-demo-wasabi-or Wasabi-OR Yes No Wasabi storage in Oregon. aptrust-demo-wasabi-va Wasabi-VA Yes No Wasabi storage in Virginia.","title":"Demo Preservation Bucket Names"},{"location":"components/s3/#staging-preservation-bucket-names","text":"Name Storage Option Encrypted Versioned Description aptrust.staging. preservation.storage Standard No No Standard S3 preservation storage bucket in Virginia. Everything in this bucket is replicated to aptrust.staging. preservation.oregon. aptrust.staging. preservation.oregon Standard Yes No Glacier storage for items using the stanard storage option. Everything in here is replicated from aptrust.staging. preservation.storage. aptrust.staging. preservation.glacier-deep.oh Glacier-Deep-OH Yes No Glacier Deep Archive in Ohio. aptrust.staging. preservation.glacier-deep.or Glacier-Deep-OR Yes No Glacier Deep Archive in Oregon. aptrust.staging. preservation.glacier-deep.va Glacier-Deep-VA Yes No Glacier Deep Archive in Virginia. aptrust.staging. preservation.glacier.oh Glacier-OH Yes No Glacier archive in Ohio. aptrust.staging. preservation.glacier.or Glacier-OR Yes No Glacier archive in Oregon. aptrust.staging. preservation.glacier.va Glacier-VA Yes No Glacier archive in Virginia. aptrust-staging-wasabi-or Wasabi-OR Yes No Wasabi storage in Oregon. aptrust-staging-wasabi-va Wasabi-VA Yes No Wasabi storage in Virginia.","title":"Staging Preservation Bucket Names"},{"location":"workers/","text":"Overview All Preserv workers run in Docker containers on Amazon's ECS service, which adds and removes containers as the current workload requires. Preserv workers also share the following capabilities, inherited from the Base Worker : A Context object providing connections to NSQ, Redis, and the Registry. A set of common methods described in the ServiceWorker interface for handling workflow decisions and storing and fetch housekeeping data. Channels for processing tasks and errors. A structure for gracefully handling unexpected shutdowns sent via SIGINT and SIGTERM. Note that because some containers run on AWS spot instances, we do get unexpected SIGINT and SIGTERM. Framework and Configuration While each worker has a different set of responsibilities, each essentially follows the same pattern in its workflow: Get a message from NSQ. Get a WorkItem record from Registry and determine whether to work on the item or skip it. (The fixity checker get a Generic File record instead of a WorkItem.) If the item needs work, update the WorkItem in Registry to say it's being worked on. Tell NSQ it's being worked on. Get data from Redis describing the current state of processing for this item. The Redis data was created by prior workers, or by a previous processing attempt by the same worker. Do your task (validate the bag, copy its files to a staging bucket, etc.), updating interim data Redis as you go. If the worker failed, push it into the error channel to log details about the failure. Otherwise... Update the WorkItem in Registry to say the worker did its job and the item is ready for the next step. Mark the NSQ message as complete. Push the item into the next NSQ topic. While the Base worker provides a mechanism for completing all these steps, the specific workers supply a policy describing to the Base: What needs to be done during processing. When items should be skipped. How errors should be handled. Which NSQ topics successful and failed items should go into. Each worker's policy is implemented as a set of fields and functions. For example, NextQueueTopic is a simple field on each worker describing which NSQ topic a message should go into after the worker successfully completes its work. Meanwhile, each worker supplies a structure containing a single Run() method to the Base worker. The Base worker walks through its usual framework steps described above, calls Run() when appropriate, and then looks as the ProcessingError object returns by Run() to determine what to do next. Non-Fatal Errors If the error is nil, the Base worker follows steps 7-10 above, pushing the item along to the next worker. If Run() returns an error marked as non-fatal, the worker requeues the task to be picked up again in a few minutes by any instance of the same worker type. (E.g. If the preservation uploader requeues an item, any other instance of preservation uploader can pick it up later.) When the item is picked up again, the worker fetches the interim processing data from Redis and does only the work that has not yet been completed. For example, it's common for the preservation uploader to encounter network errors, especially when uploading large sets of files. If it fails, the next uploader can see that 95 of the 100 files have already been successfully uploaded, and it will upload only the remaining 5. If processing fails with a non-fatal error too many times, the WorkItem is marked as NeedsAdminReview and processing stops. An APTrust admin can requeue the item in the Registry when appropriate. The most common non-fatal errors are network connection errors and unavailable service errors (e.g. Registry, Redis, S3, Glacier, or Wasabi cannot respond to requests). It makes sense for the APTrust admin to wait to requeue these items until the network and external services are known to be in a healthy state. Note that each worker's MAX_ATTEMPTS setting in Parameter Store determines how many non-fatal errors it will tolerate before giving up on an item. Fatal Errors If Run() returns a fatal error, the worker does the following: Logs detailed info on the fatal error. Sets the following attributes on the WorkItem and saves it to Registry: Retry = false (so other workers won't retry it) NeedsAdminReview = true (so APTrust admins know to look into it) Note = a brief summary of the fatal erro Tells NSQ the item failed. The most common fatal error is a bag validation error. Someone sent us a bag in the wrong format, or a bag whose manifest doesn't agree with its payload. We cannot ingest these bags. This error is truly fatal, and no further work should be done. Less common fatal errors include: Lack of permissions on some essential external resource, such as an S3 bucket. This should not happen after initial launch. A rare problem recording data in Registry. This happens when the ingest recorder is killed by a SIGTERM or SIGINT between the moment of issuing a POST request and being able to process the Registry's response. This does happen on occasion when spot instances die. The symptom is an \"identifier already exists\" error in a WorkItem note. When this occurs, an APTrust admin should delete the WorkItem's registry data and requeue the item back to the Receive/Pending stage. Thanks to the reingest manager, the system is smart enough to handle this case intelligently. Source Files File Description ingest/base.go The base ingest worker includes methods and properties common to all ingest workers. It also defines the Run() interface that all ingest workers implement. workers/base.go The worker base class defines methods and fields common to all workers. While ingest/base is concerned with a worker's internal operations, worker/base is concerned with how a worker interacts with its context, including NSQ, Registry, Redis and S3. The base worker is essentially a framework containing essential methods, objects (like the Context object) and channels. Each worker customizes the basic framework to do its job. See \"workers\" below. workers The workers directory includes a file for each worker. These files simply pass in a configuration to the base worker, telling it which NSQ topics to read from and write to, and defining functions to process tasks and handle errors. restoration/base.go The base restoration worker. deletion/manager.go Handles file and object deletion. apps The apps directory contains a simple harness for each worker to be compiled into a standalone executable.","title":"Overview"},{"location":"workers/#overview","text":"All Preserv workers run in Docker containers on Amazon's ECS service, which adds and removes containers as the current workload requires. Preserv workers also share the following capabilities, inherited from the Base Worker : A Context object providing connections to NSQ, Redis, and the Registry. A set of common methods described in the ServiceWorker interface for handling workflow decisions and storing and fetch housekeeping data. Channels for processing tasks and errors. A structure for gracefully handling unexpected shutdowns sent via SIGINT and SIGTERM. Note that because some containers run on AWS spot instances, we do get unexpected SIGINT and SIGTERM.","title":"Overview"},{"location":"workers/#framework-and-configuration","text":"While each worker has a different set of responsibilities, each essentially follows the same pattern in its workflow: Get a message from NSQ. Get a WorkItem record from Registry and determine whether to work on the item or skip it. (The fixity checker get a Generic File record instead of a WorkItem.) If the item needs work, update the WorkItem in Registry to say it's being worked on. Tell NSQ it's being worked on. Get data from Redis describing the current state of processing for this item. The Redis data was created by prior workers, or by a previous processing attempt by the same worker. Do your task (validate the bag, copy its files to a staging bucket, etc.), updating interim data Redis as you go. If the worker failed, push it into the error channel to log details about the failure. Otherwise... Update the WorkItem in Registry to say the worker did its job and the item is ready for the next step. Mark the NSQ message as complete. Push the item into the next NSQ topic. While the Base worker provides a mechanism for completing all these steps, the specific workers supply a policy describing to the Base: What needs to be done during processing. When items should be skipped. How errors should be handled. Which NSQ topics successful and failed items should go into. Each worker's policy is implemented as a set of fields and functions. For example, NextQueueTopic is a simple field on each worker describing which NSQ topic a message should go into after the worker successfully completes its work. Meanwhile, each worker supplies a structure containing a single Run() method to the Base worker. The Base worker walks through its usual framework steps described above, calls Run() when appropriate, and then looks as the ProcessingError object returns by Run() to determine what to do next.","title":"Framework and Configuration"},{"location":"workers/#non-fatal-errors","text":"If the error is nil, the Base worker follows steps 7-10 above, pushing the item along to the next worker. If Run() returns an error marked as non-fatal, the worker requeues the task to be picked up again in a few minutes by any instance of the same worker type. (E.g. If the preservation uploader requeues an item, any other instance of preservation uploader can pick it up later.) When the item is picked up again, the worker fetches the interim processing data from Redis and does only the work that has not yet been completed. For example, it's common for the preservation uploader to encounter network errors, especially when uploading large sets of files. If it fails, the next uploader can see that 95 of the 100 files have already been successfully uploaded, and it will upload only the remaining 5. If processing fails with a non-fatal error too many times, the WorkItem is marked as NeedsAdminReview and processing stops. An APTrust admin can requeue the item in the Registry when appropriate. The most common non-fatal errors are network connection errors and unavailable service errors (e.g. Registry, Redis, S3, Glacier, or Wasabi cannot respond to requests). It makes sense for the APTrust admin to wait to requeue these items until the network and external services are known to be in a healthy state. Note that each worker's MAX_ATTEMPTS setting in Parameter Store determines how many non-fatal errors it will tolerate before giving up on an item.","title":"Non-Fatal Errors"},{"location":"workers/#fatal-errors","text":"If Run() returns a fatal error, the worker does the following: Logs detailed info on the fatal error. Sets the following attributes on the WorkItem and saves it to Registry: Retry = false (so other workers won't retry it) NeedsAdminReview = true (so APTrust admins know to look into it) Note = a brief summary of the fatal erro Tells NSQ the item failed. The most common fatal error is a bag validation error. Someone sent us a bag in the wrong format, or a bag whose manifest doesn't agree with its payload. We cannot ingest these bags. This error is truly fatal, and no further work should be done. Less common fatal errors include: Lack of permissions on some essential external resource, such as an S3 bucket. This should not happen after initial launch. A rare problem recording data in Registry. This happens when the ingest recorder is killed by a SIGTERM or SIGINT between the moment of issuing a POST request and being able to process the Registry's response. This does happen on occasion when spot instances die. The symptom is an \"identifier already exists\" error in a WorkItem note. When this occurs, an APTrust admin should delete the WorkItem's registry data and requeue the item back to the Receive/Pending stage. Thanks to the reingest manager, the system is smart enough to handle this case intelligently.","title":"Fatal Errors"},{"location":"workers/#source-files","text":"File Description ingest/base.go The base ingest worker includes methods and properties common to all ingest workers. It also defines the Run() interface that all ingest workers implement. workers/base.go The worker base class defines methods and fields common to all workers. While ingest/base is concerned with a worker's internal operations, worker/base is concerned with how a worker interacts with its context, including NSQ, Registry, Redis and S3. The base worker is essentially a framework containing essential methods, objects (like the Context object) and channels. Each worker customizes the basic framework to do its job. See \"workers\" below. workers The workers directory includes a file for each worker. These files simply pass in a configuration to the base worker, telling it which NSQ topics to read from and write to, and defining functions to process tasks and handle errors. restoration/base.go The base restoration worker. deletion/manager.go Handles file and object deletion. apps The apps directory contains a simple harness for each worker to be compiled into a standalone executable.","title":"Source Files"},{"location":"workers/anatomy/","text":"Anatomy of a Worker Most workers consist of three components: A task definition file that implements task-specific business logic. A worker file that defines workflow policy including which NSQ channels to listen to, how to handle errors, and more. An app file that allows a worker to be compiled to a standalone application. Task Definition Files The task definition file implements functions that the worker will perform on data. This file focuses on a single data-centric operation. For example, the fixity checker calculates fixity on a file in preservation storage. The bag validator validates a bag before ingest. The file restorer restores individual files. All of these files implement a method called Run() to do the work. Run() returns an integer describing the number of tasks processed and a slice of ProcessingError objects describing what, if anything went wrong during processing. Worker Files Worker files include a Context object that allows the worker to talk to NSQ, Redis, Registry and S3/Glacier/Wasabi. The Base Worker defines a number of methods that use the context object to perform tasks common to all workers, such as getting work items from the registry, registering a listener with NSQ, etc. The base worker also handles interrupt and kill signals from the operating system. Workers also include a Settings object that describes which NSQ topics it should read from and write to, how many times it should attempt its task, etc. The worker source files tend to be slim because they don't implement much logic. They simply load code from an underlying task file, configure it with custom settings, and wrap it all into the worker framework that can talk to all of the external services (NSQ, Redis, Registry, and S3/Glacier/Wasabi). For example, if you look at workers/ingest_format_identifier.go , you'll see the code does the following: Creates a new Context object providing clients for Redis, Registry, NSQ and S3. Creates a settings object telling the worker which NSQ topic to read from, which topic to push to if processing completes successfully, how many times to attempt its task, how to handle fatal errors, how many items to keep in its internal work queue, etc. Creates an underlying task handler, which can be any object that implements the Run() interface (aka a Runnable). Creates a worker object with the required context, settings, and Runnable. Tells the worker to start listening to NSQ. It will listen to the channel specified in the Settings. All workers follow this pattern. Note The cron jobs apt_queue , apt_queue_fixity and ingest_bucket_reader don't have task files. Because they are designed to run and exit, rather than listen forever, their logic is implemented directly in the worker files. App Files Preserv's apps directory contains a directory for each worker. Inside each directory is a single file with a main() method, which is required by Go for building executables. (Go allows only one main method per directory, which is why the app files are all in separate directories.) The format identifier app shows the pattern for app files, which do the following: Read options from the command line. These are optional, but allow us to override the following key settings that typically come in through the environment, the .env file, or AWS's parameter store: a. ChannelBufferSize - This is the number of items the worker should keep in its internal work queue. b. NumWorkers - The number of go routines the worker should run. For example, if this is set to 2, the worker will run 2 concurrent instances of its task handler. (Two concurrent Runnables.) c. MaxAttempts defines the maximum number of non-fatal errors a task can encounter before it quits. Tasks that fail with non-fatal errors are marked as failed in the Registry, with the WorkItem.NeedsAdminReview flag set to true. The APTrust admin can requeue these items later. Create a worker with the specified options. Remember that the worker starts listening to its NSQ channel as soon as it is created. Sets up a blocking channel for this worker to listen to. Nothing happens in this channel, but because the worker has to listen to it forever, it cannot exit until the operating system kills the worker with a SIGINT or SIGKILL. Without this, the worker would exit immediately. See the Base Worker for info on how signals are handled. Source Files Worker Service Files Definition Bucket Reader Ingest No Task File Worker App Cron job that scans depositor receiving buckets for new tar files to ingest. Metadata Gatherer Ingest Task Worker App Parses a bag's tag files, calculates checksums on bag contents, and copies manifests and tag files to the ingest staging bucket. Bag Validator Ingest Task Worker App Validates a bag before ingest. Reingest Manager Ingest Task Worker App Checks if a bag is being reingested and if so, applies special processing. Staging Uploader Ingest Task Worker App Copies files from a tarred bag in a receiving bucket to the ingest staging bucket. Format Identifier Ingest Task Worker App Identifies the format of files within a bag. Preservation Uploader Ingest Task Worker App Copies files to preservation storage. Preservation Verifier Ingest Task Worker App Verifies that files copied to preservation storage are actually there. Ingest Recorder Ingest Task Worker App Records all ingest data in Registry. Cleanup Ingest Task Worker App Cleans up all of the temporary resources created during the ingest process and deletes ingested bags from receiving buckets. APT Queue Fixity Fixity No Task File Worker App Queues files for fixity checks. Fixity Checker Fixity Task Worker App Checks fixity on files in preservation storage. (S3 and Wasabi only. Does not check Glacier files.) Glacier Restorer Restoration Task Worker App Moves files from Glacier into S3 for restoration. File Restorer Restoration Task Worker App Restores individual files. Object Restorer Restoration Task Worker App Restores entire bags (intellectual objects). Deletion Manager Deletion Task Worker App Deletes files and objects from preservation storage. APT Queue Deletion and Restoration No Task File Worker App This cron job periodically scans Registry for restoration and deletion requests that have not been queued in NSQ.","title":"Anatomy of a Worker"},{"location":"workers/anatomy/#anatomy-of-a-worker","text":"Most workers consist of three components: A task definition file that implements task-specific business logic. A worker file that defines workflow policy including which NSQ channels to listen to, how to handle errors, and more. An app file that allows a worker to be compiled to a standalone application.","title":"Anatomy of a Worker"},{"location":"workers/anatomy/#task-definition-files","text":"The task definition file implements functions that the worker will perform on data. This file focuses on a single data-centric operation. For example, the fixity checker calculates fixity on a file in preservation storage. The bag validator validates a bag before ingest. The file restorer restores individual files. All of these files implement a method called Run() to do the work. Run() returns an integer describing the number of tasks processed and a slice of ProcessingError objects describing what, if anything went wrong during processing.","title":"Task Definition Files"},{"location":"workers/anatomy/#worker-files","text":"Worker files include a Context object that allows the worker to talk to NSQ, Redis, Registry and S3/Glacier/Wasabi. The Base Worker defines a number of methods that use the context object to perform tasks common to all workers, such as getting work items from the registry, registering a listener with NSQ, etc. The base worker also handles interrupt and kill signals from the operating system. Workers also include a Settings object that describes which NSQ topics it should read from and write to, how many times it should attempt its task, etc. The worker source files tend to be slim because they don't implement much logic. They simply load code from an underlying task file, configure it with custom settings, and wrap it all into the worker framework that can talk to all of the external services (NSQ, Redis, Registry, and S3/Glacier/Wasabi). For example, if you look at workers/ingest_format_identifier.go , you'll see the code does the following: Creates a new Context object providing clients for Redis, Registry, NSQ and S3. Creates a settings object telling the worker which NSQ topic to read from, which topic to push to if processing completes successfully, how many times to attempt its task, how to handle fatal errors, how many items to keep in its internal work queue, etc. Creates an underlying task handler, which can be any object that implements the Run() interface (aka a Runnable). Creates a worker object with the required context, settings, and Runnable. Tells the worker to start listening to NSQ. It will listen to the channel specified in the Settings. All workers follow this pattern. Note The cron jobs apt_queue , apt_queue_fixity and ingest_bucket_reader don't have task files. Because they are designed to run and exit, rather than listen forever, their logic is implemented directly in the worker files.","title":"Worker Files"},{"location":"workers/anatomy/#app-files","text":"Preserv's apps directory contains a directory for each worker. Inside each directory is a single file with a main() method, which is required by Go for building executables. (Go allows only one main method per directory, which is why the app files are all in separate directories.) The format identifier app shows the pattern for app files, which do the following: Read options from the command line. These are optional, but allow us to override the following key settings that typically come in through the environment, the .env file, or AWS's parameter store: a. ChannelBufferSize - This is the number of items the worker should keep in its internal work queue. b. NumWorkers - The number of go routines the worker should run. For example, if this is set to 2, the worker will run 2 concurrent instances of its task handler. (Two concurrent Runnables.) c. MaxAttempts defines the maximum number of non-fatal errors a task can encounter before it quits. Tasks that fail with non-fatal errors are marked as failed in the Registry, with the WorkItem.NeedsAdminReview flag set to true. The APTrust admin can requeue these items later. Create a worker with the specified options. Remember that the worker starts listening to its NSQ channel as soon as it is created. Sets up a blocking channel for this worker to listen to. Nothing happens in this channel, but because the worker has to listen to it forever, it cannot exit until the operating system kills the worker with a SIGINT or SIGKILL. Without this, the worker would exit immediately. See the Base Worker for info on how signals are handled.","title":"App Files"},{"location":"workers/anatomy/#source-files","text":"Worker Service Files Definition Bucket Reader Ingest No Task File Worker App Cron job that scans depositor receiving buckets for new tar files to ingest. Metadata Gatherer Ingest Task Worker App Parses a bag's tag files, calculates checksums on bag contents, and copies manifests and tag files to the ingest staging bucket. Bag Validator Ingest Task Worker App Validates a bag before ingest. Reingest Manager Ingest Task Worker App Checks if a bag is being reingested and if so, applies special processing. Staging Uploader Ingest Task Worker App Copies files from a tarred bag in a receiving bucket to the ingest staging bucket. Format Identifier Ingest Task Worker App Identifies the format of files within a bag. Preservation Uploader Ingest Task Worker App Copies files to preservation storage. Preservation Verifier Ingest Task Worker App Verifies that files copied to preservation storage are actually there. Ingest Recorder Ingest Task Worker App Records all ingest data in Registry. Cleanup Ingest Task Worker App Cleans up all of the temporary resources created during the ingest process and deletes ingested bags from receiving buckets. APT Queue Fixity Fixity No Task File Worker App Queues files for fixity checks. Fixity Checker Fixity Task Worker App Checks fixity on files in preservation storage. (S3 and Wasabi only. Does not check Glacier files.) Glacier Restorer Restoration Task Worker App Moves files from Glacier into S3 for restoration. File Restorer Restoration Task Worker App Restores individual files. Object Restorer Restoration Task Worker App Restores entire bags (intellectual objects). Deletion Manager Deletion Task Worker App Deletes files and objects from preservation storage. APT Queue Deletion and Restoration No Task File Worker App This cron job periodically scans Registry for restoration and deletion requests that have not been queued in NSQ.","title":"Source Files"},{"location":"workers/deletion/","text":"Deletion Depositors request deletions through the Registry UI. Once a deletion has been requested and approved by an institutional admin, the Registry creates a deletion WorkItem and pushes the WorkItem ID into NSQ's delete_item topic. The apt_delete worker reads from this topic and deletes files or entire intellectual objects, as requested. For single file deletions, the worker deletes all copies of the file from preservation storage, creates a deletion Premis event for the file in Registry, and marks changes the state of the Generic File object in Registry from \"A\" (active) to \"D\" (deleted). For intellectual object deletions, the worker does the above for all of the object's files, then created a deletion Premis event for the object and changes its state from \"A\" to \"D.\" Finally, it marks the WorkItem complete. For items using the Standard storage option, apt_delete expunges both the S3 copy and the Glacier copy of every file. All other storage options keep files in a single bucket or vault, so apt_delete has to expunge from only a single location. Note Also see the section on Bulk Deletions on the overview page. APTrust has internal documentation on how to create bulk deletion requests and the safeguards surrounding the process. See the internal docs for more information. Resources apt_delete uses little CPU, memory, and network bandwidth. External Services Service Function S3 Preservation Buckets Long-term storage area from which files are deleted. Glacier Preservation Buckets Long-term storage area from which files are deleted. Glacier Deep Archive Buckets Long-term storage area from which files are deleted. Wasabi Buckets Long-term storage area from which files are deleted. Registry Source of WorkItem record describing work to be done. Deletion workers update files and objects, and create deletion Premis events here. NSQ Distributes WorkItem IDs to workers and tracks their status. Source Files Worker Service Files Definition Deletion Manager Deletion Task Worker App Deletes files and objects from preservation storage. APT Queue Deletion and Restoration No Task File Worker App This cron job periodically scans Registry for restoration and deletion requests that have not been queued in NSQ.","title":"Deletion"},{"location":"workers/deletion/#deletion","text":"Depositors request deletions through the Registry UI. Once a deletion has been requested and approved by an institutional admin, the Registry creates a deletion WorkItem and pushes the WorkItem ID into NSQ's delete_item topic. The apt_delete worker reads from this topic and deletes files or entire intellectual objects, as requested. For single file deletions, the worker deletes all copies of the file from preservation storage, creates a deletion Premis event for the file in Registry, and marks changes the state of the Generic File object in Registry from \"A\" (active) to \"D\" (deleted). For intellectual object deletions, the worker does the above for all of the object's files, then created a deletion Premis event for the object and changes its state from \"A\" to \"D.\" Finally, it marks the WorkItem complete. For items using the Standard storage option, apt_delete expunges both the S3 copy and the Glacier copy of every file. All other storage options keep files in a single bucket or vault, so apt_delete has to expunge from only a single location. Note Also see the section on Bulk Deletions on the overview page. APTrust has internal documentation on how to create bulk deletion requests and the safeguards surrounding the process. See the internal docs for more information.","title":"Deletion"},{"location":"workers/deletion/#resources","text":"apt_delete uses little CPU, memory, and network bandwidth.","title":"Resources"},{"location":"workers/deletion/#external-services","text":"Service Function S3 Preservation Buckets Long-term storage area from which files are deleted. Glacier Preservation Buckets Long-term storage area from which files are deleted. Glacier Deep Archive Buckets Long-term storage area from which files are deleted. Wasabi Buckets Long-term storage area from which files are deleted. Registry Source of WorkItem record describing work to be done. Deletion workers update files and objects, and create deletion Premis events here. NSQ Distributes WorkItem IDs to workers and tracks their status.","title":"External Services"},{"location":"workers/deletion/#source-files","text":"Worker Service Files Definition Deletion Manager Deletion Task Worker App Deletes files and objects from preservation storage. APT Queue Deletion and Restoration No Task File Worker App This cron job periodically scans Registry for restoration and deletion requests that have not been queued in NSQ.","title":"Source Files"},{"location":"workers/fixity/","text":"Fixity Preserv checks fixity on all files in S3 and Wasabi storage every 90 days. We do not run fixity checks on files in Glacier or Glacier Deep archives. apt_queue_fixity runs as a cron job in its own container, querying Registry every half hour for files that have not had a fixity check in 90 days. It typically gets a batch of 2,500 files at a time, then pushes all of the Generic File IDs into NSQ's fixity_check topic. From there, the apt_fixity worker picks them up, streaming each file through the sha256 checksum algorithm and creating a Premis event to record the result in Registry. Note Although we calculate multiple checksums on ingest, we calculate only sha256 digests during fixity checks. Also note that we do not create WorkItems for fixity checks because they would add no value to this one-step process, and creating them would add over 40 million unnecessary rows to the WorkItems table each year. Avoiding Duplicate Work When a fixity batch contains a number of very large files (over 100 GB), the checker may not complete all 2,500 checks before apt_queue_fixity runs again. In that case, apt_queue_fixity will add duplicate files to NSQ's fixity_check topic. For example, the last 1,000 files that the checker did not complete on the prior run are added again as the first 1,000 files to be checked on the next run. The fixity checker includes code to dedupe these files, so it will only check each file once. If the checker didn't do this, and it got behind on several consecutive runs, it could lead to a situation where the checker keeps checking the same set of files over and over again, never advancing to checker newer files that actually need to be looked at. Settings You can change how often the apt_queue_fixity runs by adjusting the QUEUE_FIXITY_INTERVAL setting in AWS Parameter Store. You can adjust how many items are queued on each run by updating MAX_FIXITY_ITEMS_PER_RUN. You can calso adjust MAX_DAYS_SINCE_LAST_FIXITY on the staging and demo systems to make fixity checks run more or less frequently. In production, that setting should always be set to 90 (days), because that's the interval our depositor agreement specifies. Resources Fixity checks use substantial bandwidth and CPU. Checking large files may also use substantial memory. External Services Service Function S3 Preservation Buckets Long-term storage area from which files are retrieved for fixity checks. Wasabi Buckets Long-term storage area from which files are retrieved for fixity checks. Registry Source of Generic File IDs requiring fixity check. The checker saves fixity checks Premis events here. NSQ Distributes Generic File IDs to workers and tracks their status. Source Files Worker Service Files Definition APT Queue Fixity Fixity No Task File Worker App Queues files for fixity checks. Fixity Checker Fixity Task Worker App Checks fixity on files in preservation storage. (S3 and Wasabi only. Does not check Glacier files.)","title":"Fixity"},{"location":"workers/fixity/#fixity","text":"Preserv checks fixity on all files in S3 and Wasabi storage every 90 days. We do not run fixity checks on files in Glacier or Glacier Deep archives. apt_queue_fixity runs as a cron job in its own container, querying Registry every half hour for files that have not had a fixity check in 90 days. It typically gets a batch of 2,500 files at a time, then pushes all of the Generic File IDs into NSQ's fixity_check topic. From there, the apt_fixity worker picks them up, streaming each file through the sha256 checksum algorithm and creating a Premis event to record the result in Registry. Note Although we calculate multiple checksums on ingest, we calculate only sha256 digests during fixity checks. Also note that we do not create WorkItems for fixity checks because they would add no value to this one-step process, and creating them would add over 40 million unnecessary rows to the WorkItems table each year.","title":"Fixity"},{"location":"workers/fixity/#avoiding-duplicate-work","text":"When a fixity batch contains a number of very large files (over 100 GB), the checker may not complete all 2,500 checks before apt_queue_fixity runs again. In that case, apt_queue_fixity will add duplicate files to NSQ's fixity_check topic. For example, the last 1,000 files that the checker did not complete on the prior run are added again as the first 1,000 files to be checked on the next run. The fixity checker includes code to dedupe these files, so it will only check each file once. If the checker didn't do this, and it got behind on several consecutive runs, it could lead to a situation where the checker keeps checking the same set of files over and over again, never advancing to checker newer files that actually need to be looked at.","title":"Avoiding Duplicate Work"},{"location":"workers/fixity/#settings","text":"You can change how often the apt_queue_fixity runs by adjusting the QUEUE_FIXITY_INTERVAL setting in AWS Parameter Store. You can adjust how many items are queued on each run by updating MAX_FIXITY_ITEMS_PER_RUN. You can calso adjust MAX_DAYS_SINCE_LAST_FIXITY on the staging and demo systems to make fixity checks run more or less frequently. In production, that setting should always be set to 90 (days), because that's the interval our depositor agreement specifies.","title":"Settings"},{"location":"workers/fixity/#resources","text":"Fixity checks use substantial bandwidth and CPU. Checking large files may also use substantial memory.","title":"Resources"},{"location":"workers/fixity/#external-services","text":"Service Function S3 Preservation Buckets Long-term storage area from which files are retrieved for fixity checks. Wasabi Buckets Long-term storage area from which files are retrieved for fixity checks. Registry Source of Generic File IDs requiring fixity check. The checker saves fixity checks Premis events here. NSQ Distributes Generic File IDs to workers and tracks their status.","title":"External Services"},{"location":"workers/fixity/#source-files","text":"Worker Service Files Definition APT Queue Fixity Fixity No Task File Worker App Queues files for fixity checks. Fixity Checker Fixity Task Worker App Checks fixity on files in preservation storage. (S3 and Wasabi only. Does not check Glacier files.)","title":"Source Files"},{"location":"workers/restoration/","text":"Restoration When a depositor requests file or object restoration by clicking the Restore button in the Registry, the Registry creates a restoration WorkItem and queues the WorkItem ID in one of the following NSQ topics: restore_file, if the user wants to restore a single file restore_object, if the user wants to restore an entire intellectual object (bag) restore_glacier, if the object of file being restored must first be retrieved from Glacier Each of these topics has a different worker. The glacier_restorer subscribes to the restore_glacier topic. file_restorer subscribes to the restore_file topic, and bag_restorer subscribes to the restore_object topic. Glacier Restoration If a file or object is stored in Glacier only, it must first be moved from the Glacier storage tier to the S3 storage tier before we can access it. The glacier_restorer worker sends requests to AWS to move items from Glacier and Glacier Deep Archive temporarily into the S3 tier. That may take up to five hours for Glacier files and up to 12 hours for files in Glacier Deep Archive. After making the initial request, glacier_restorer polls AWS's Glacier service every few hours to see if the files have been moved. When the files have reached the S3 tier, glacier_restorer pushes the WorkItem ID of the restoration request into either NSQ's restore_file or restore_object topic for completion. From there, the restoration proceeds like a normal S3 restoration. File Restoration The file_restorer restores individual files from S3 and Wasabi into depositor's restoration buckets. Individually restored files will appear in the restoration bucket using the file identifier as the key. For example, the Generic File with identifier virginia.edu/photos/data/superbowl.jpg will be restored to aptrust.restore.virginia.edu/virginia.edu/photos/data/superbowl.jpg The file_restorer calculates checksums as it streams the file from the preservation bucket to the restoration bucket. If the calculated checksums don't match what's in the Registry, the restoration will be marked as Failed. Once the file is restored, the WorkItem is marked complete, and the URL of the restored file appears in the WorkItem Note. Object Restoration Object restoration involves restoring all of the files that make up an intellectual object, packaging them in BagIt format, and writing the entire tar file into the depositor's restoration bucket. The key of the restored bag will be the intellectual object identifier, minus the insitutional prefix, plus a \".tar\" extension. For example, the object virginia.edu/photos will be restored to aptrust.restore.virginia.edu/photos.tar . It will be a single file at the top level of the bucket, not nested in a subfolder, like a restored file. The bag_restorer does the following when restoring an object: streams all of the objects through a checsum calculator into a tar file in the depositor's restoration bucket builds manifests as the files stream through writes tag files and manifests into the tar stream validates the bag marks the WorkItem complete, with the URL of the restored bag in the WorkItem note. Validation occurs during the bagging process, so we don't have to re-read the tar file from the restoration bucket. Validation will fail if any files that Registry says are part of the bag are missing from storage or have invalid checksums. Note that we keep the original bag-info.txt and aptrust-info.txt files that we received during the last ingest of this bag, and we restore them to the tar file. In fact, we preserve all files outside of the original bag's data directory except manifests, tag manifests and fetch.txt files. We do this because it's common for depositors to include important metadata in custom tag files. Note A restored bag will not exactly match the originally submitted bag, because the bag_restorer may add files to the payload directory in any order it likes, rather than in the order they were originally added. In addition, depositors sometimes delete files from an object between initial ingest and restoration. Deleted files will not be restored, so the restored bag may have fewer files than the original ingest. Resources The glacier_restorer uses virtually no CPU, memory or network I/O. It simply issues periodic requests to AWS with small request and response sizes. The file_restorer uses minimal resources when restoring smaller files, and may use substantial resources when restoring very large files. CPU usage goes up as it calculated checksums on large files. Memory usage can be somewhat high for large files (> 100 GB) and network usage is proportional to file size. The bag_restorer can use considerable memory, CPU and bandwidth when restoring large bags or bags with high file counts. External Services Service Function S3 Preservation Buckets Long-term storage area from which files are restored. Glacier Preservation Buckets Long-term storage area from which files are restored. Glacier Deep Archive Buckets Long-term storage area from which files are restored. Wasabi Buckets Long-term storage area from which files are restored. S3 Restoration Buckets Depositor buckets to which files and bags are restored. Registry Source of WorkItem record describing work to be done. NSQ Distributes WorkItem IDs to workers and tracks their status. Source Files Worker Service Files Definition Glacier Restorer Restoration Task Worker App Moves files from Glacier into S3 for restoration. File Restorer Restoration Task Worker App Restores individual files. Object Restorer Restoration Task Worker App Restores entire bags (intellectual objects). APT Queue Deletion and Restoration No Task File Worker App This cron job periodically scans Registry for restoration and deletion requests that have not been queued in NSQ.","title":"Restoration"},{"location":"workers/restoration/#restoration","text":"When a depositor requests file or object restoration by clicking the Restore button in the Registry, the Registry creates a restoration WorkItem and queues the WorkItem ID in one of the following NSQ topics: restore_file, if the user wants to restore a single file restore_object, if the user wants to restore an entire intellectual object (bag) restore_glacier, if the object of file being restored must first be retrieved from Glacier Each of these topics has a different worker. The glacier_restorer subscribes to the restore_glacier topic. file_restorer subscribes to the restore_file topic, and bag_restorer subscribes to the restore_object topic.","title":"Restoration"},{"location":"workers/restoration/#glacier-restoration","text":"If a file or object is stored in Glacier only, it must first be moved from the Glacier storage tier to the S3 storage tier before we can access it. The glacier_restorer worker sends requests to AWS to move items from Glacier and Glacier Deep Archive temporarily into the S3 tier. That may take up to five hours for Glacier files and up to 12 hours for files in Glacier Deep Archive. After making the initial request, glacier_restorer polls AWS's Glacier service every few hours to see if the files have been moved. When the files have reached the S3 tier, glacier_restorer pushes the WorkItem ID of the restoration request into either NSQ's restore_file or restore_object topic for completion. From there, the restoration proceeds like a normal S3 restoration.","title":"Glacier Restoration"},{"location":"workers/restoration/#file-restoration","text":"The file_restorer restores individual files from S3 and Wasabi into depositor's restoration buckets. Individually restored files will appear in the restoration bucket using the file identifier as the key. For example, the Generic File with identifier virginia.edu/photos/data/superbowl.jpg will be restored to aptrust.restore.virginia.edu/virginia.edu/photos/data/superbowl.jpg The file_restorer calculates checksums as it streams the file from the preservation bucket to the restoration bucket. If the calculated checksums don't match what's in the Registry, the restoration will be marked as Failed. Once the file is restored, the WorkItem is marked complete, and the URL of the restored file appears in the WorkItem Note.","title":"File Restoration"},{"location":"workers/restoration/#object-restoration","text":"Object restoration involves restoring all of the files that make up an intellectual object, packaging them in BagIt format, and writing the entire tar file into the depositor's restoration bucket. The key of the restored bag will be the intellectual object identifier, minus the insitutional prefix, plus a \".tar\" extension. For example, the object virginia.edu/photos will be restored to aptrust.restore.virginia.edu/photos.tar . It will be a single file at the top level of the bucket, not nested in a subfolder, like a restored file. The bag_restorer does the following when restoring an object: streams all of the objects through a checsum calculator into a tar file in the depositor's restoration bucket builds manifests as the files stream through writes tag files and manifests into the tar stream validates the bag marks the WorkItem complete, with the URL of the restored bag in the WorkItem note. Validation occurs during the bagging process, so we don't have to re-read the tar file from the restoration bucket. Validation will fail if any files that Registry says are part of the bag are missing from storage or have invalid checksums. Note that we keep the original bag-info.txt and aptrust-info.txt files that we received during the last ingest of this bag, and we restore them to the tar file. In fact, we preserve all files outside of the original bag's data directory except manifests, tag manifests and fetch.txt files. We do this because it's common for depositors to include important metadata in custom tag files. Note A restored bag will not exactly match the originally submitted bag, because the bag_restorer may add files to the payload directory in any order it likes, rather than in the order they were originally added. In addition, depositors sometimes delete files from an object between initial ingest and restoration. Deleted files will not be restored, so the restored bag may have fewer files than the original ingest.","title":"Object Restoration"},{"location":"workers/restoration/#resources","text":"The glacier_restorer uses virtually no CPU, memory or network I/O. It simply issues periodic requests to AWS with small request and response sizes. The file_restorer uses minimal resources when restoring smaller files, and may use substantial resources when restoring very large files. CPU usage goes up as it calculated checksums on large files. Memory usage can be somewhat high for large files (> 100 GB) and network usage is proportional to file size. The bag_restorer can use considerable memory, CPU and bandwidth when restoring large bags or bags with high file counts.","title":"Resources"},{"location":"workers/restoration/#external-services","text":"Service Function S3 Preservation Buckets Long-term storage area from which files are restored. Glacier Preservation Buckets Long-term storage area from which files are restored. Glacier Deep Archive Buckets Long-term storage area from which files are restored. Wasabi Buckets Long-term storage area from which files are restored. S3 Restoration Buckets Depositor buckets to which files and bags are restored. Registry Source of WorkItem record describing work to be done. NSQ Distributes WorkItem IDs to workers and tracks their status.","title":"External Services"},{"location":"workers/restoration/#source-files","text":"Worker Service Files Definition Glacier Restorer Restoration Task Worker App Moves files from Glacier into S3 for restoration. File Restorer Restoration Task Worker App Restores individual files. Object Restorer Restoration Task Worker App Restores entire bags (intellectual objects). APT Queue Deletion and Restoration No Task File Worker App This cron job periodically scans Registry for restoration and deletion requests that have not been queued in NSQ.","title":"Source Files"},{"location":"workers/ingest/","text":"Ingest Workers The ingest process consists of 10 workers: one is cron job and the rest are service workers whose tasks are orchestrated by NSQ. In the table below, the Name is the friendly name of the service or worker, while the Executable is the name of the compiled binary. \"Reads From\" describes the name of the NSQ topic from which the worker gathers its tasks. \"Pushes To\" is the name of the NSQ topic to which a worker pushes its task upon successful completion. Except for the Bucket Reader, all workers use Redis to keep track of interim processing data. They read from Redis to see what past workers have done, and they write to Redis to tell future workers what they have done. For more on what goes into Redis, see the Redis documentation Click the name of any item in the table for more details. Order Name Executable Description Reads From Pushes To 0 Bucket Reader ingest_ bucket_ reader A cron job that scans for new items in receiving buckets. Creates an ingest work item in Registry and pushes the work item ID into NSQ. None ingest01_ prefetch 1 Metadata Gatherer apt_ pre_fetch Streams a bag from a receiving bucket through a number of functions to calculate checksums and parse tag files and manifests. Saves tag files and manifests to S3 staging bucket. Saves all other metadata to Redis. ingest01_ prefetch ingest02_ bag_ validation 2 Bag Validator ingest_ validator Analyzes the metdata gathered by apt_pre_fetch to ensure bag is valid. If bag is invalid, processing stops here. ingest02_ bag_ validation ingest03_ reingest_ check 3 Reingest Manager reingest_ manager Checks to see if the bag has ever been ingested before. If so, checks to see which files are new or updated. ingest03_ reingest_ check ingest04_ staging 4 Staging Uploader ingest_ staging_ uploader Unpacks the tarred bag from the receiving bucket and stores its individual files in a temporary staging bucket, where other workers can access them. ingest04_ staging ingest05_ format_ identification 5 Format Identifier ingest_ format_ identifier Streams files from the staging bucket through the Siegfried format identifier, which matches byte streams against a Pronom registry. ingest05_ format_ identification ingest06_ storage 6 Preservation Uploader ingest_ preservation_ uploader Uploads files to long-term preservation buckets in S3, Glacier, and/or Wasabi. ingest06_ storage ingest07_ storage_ validation 7 Preservation Verifier ingest_ preservation_ verifier Verifies that the files copied into long-term preservation actually arrived intact and are accessible. ingest07_ storage_ validation ingest08_ record 8 Ingest Recorder ingest_ recorder Records details of an ingest in the Registry. ingest08_ record ingest09_ cleanup 9 Cleanup ingest_ cleanup Cleans up temporary resources no longer required after ingest. These include files in the staging bucket, metadata records in Redis, and the tarred bag in the receiving bucket. ingest09_ cleanup None","title":"Ingest Workers"},{"location":"workers/ingest/#ingest-workers","text":"The ingest process consists of 10 workers: one is cron job and the rest are service workers whose tasks are orchestrated by NSQ. In the table below, the Name is the friendly name of the service or worker, while the Executable is the name of the compiled binary. \"Reads From\" describes the name of the NSQ topic from which the worker gathers its tasks. \"Pushes To\" is the name of the NSQ topic to which a worker pushes its task upon successful completion. Except for the Bucket Reader, all workers use Redis to keep track of interim processing data. They read from Redis to see what past workers have done, and they write to Redis to tell future workers what they have done. For more on what goes into Redis, see the Redis documentation Click the name of any item in the table for more details. Order Name Executable Description Reads From Pushes To 0 Bucket Reader ingest_ bucket_ reader A cron job that scans for new items in receiving buckets. Creates an ingest work item in Registry and pushes the work item ID into NSQ. None ingest01_ prefetch 1 Metadata Gatherer apt_ pre_fetch Streams a bag from a receiving bucket through a number of functions to calculate checksums and parse tag files and manifests. Saves tag files and manifests to S3 staging bucket. Saves all other metadata to Redis. ingest01_ prefetch ingest02_ bag_ validation 2 Bag Validator ingest_ validator Analyzes the metdata gathered by apt_pre_fetch to ensure bag is valid. If bag is invalid, processing stops here. ingest02_ bag_ validation ingest03_ reingest_ check 3 Reingest Manager reingest_ manager Checks to see if the bag has ever been ingested before. If so, checks to see which files are new or updated. ingest03_ reingest_ check ingest04_ staging 4 Staging Uploader ingest_ staging_ uploader Unpacks the tarred bag from the receiving bucket and stores its individual files in a temporary staging bucket, where other workers can access them. ingest04_ staging ingest05_ format_ identification 5 Format Identifier ingest_ format_ identifier Streams files from the staging bucket through the Siegfried format identifier, which matches byte streams against a Pronom registry. ingest05_ format_ identification ingest06_ storage 6 Preservation Uploader ingest_ preservation_ uploader Uploads files to long-term preservation buckets in S3, Glacier, and/or Wasabi. ingest06_ storage ingest07_ storage_ validation 7 Preservation Verifier ingest_ preservation_ verifier Verifies that the files copied into long-term preservation actually arrived intact and are accessible. ingest07_ storage_ validation ingest08_ record 8 Ingest Recorder ingest_ recorder Records details of an ingest in the Registry. ingest08_ record ingest09_ cleanup 9 Cleanup ingest_ cleanup Cleans up temporary resources no longer required after ingest. These include files in the staging bucket, metadata records in Redis, and the tarred bag in the receiving bucket. ingest09_ cleanup None","title":"Ingest Workers"},{"location":"workers/ingest/bucket-reader/","text":"The Bucket Reader The bucket reader, ingest_bucket_reader, runs as a cron job inside its own container. It scans all receiving buckets belonging to all depositors for new items. (Items here means tar files containing BagIt bags for ingest.) When it finds items in receiving buckets, it does the following: Checks the Registry to see if a WorkItem exists with action Ingest for the tar file. It matches based on the name and etag of the tar file and the ID of the institution that owns the bucket. If a matching WorkItem exists, ingest_bucket_reader does nothing. If there is no matching item... ingest_bucket_reader creates an Ingest WorkItem for the bag. ingest_bucket_reader adds the WorkItem ID to NSQ's ingest_01_prefetch topic. ingest_bucket_reader logs what it does with every item it encounters, and why. If you want to know what it's doing, check the container logs in CloudWatch. Why cron? Why not use S3 events and lambdas? When APTrust first launched in 2014, AWS lambdas didn't exist. Even if they had, we had a bigger problem. The first version of Registry, called Fluctus, was so unreliable, there was no guarantee that it would even be running when S3 events were fired. There was a high likelihood of events being missed. Our cron jobs scan everything each time they run. If the message receiver (Registry) missed cron's messages for any reason, it will get them again in a few minutes. In addition, APTrust's original design requirements explicitly stated that the system must be able to run on any Linux box, anywhere, without relying on any vendor-specific services other than S3 and Glacier. Cron jobs were portable and reliable then, and they still are today. We continue to use them because they work. To put it another way, why do extra work to lock yourself into vendor-specific services, adding another layer of complexity and incurring additional operating costs, when you can do no work and have none of those problems? If you can make a business case for that, there's a position waiting for you at Accenture. External Services Service Function S3 Receiving Buckets The reader scans depositor receiving buckets for new bags (tar files) to be ingested. Registry The reader creates new WorkItems here for each bag awaiting ingest. NSQ The reader adds WorkItem IDs for bags awaiting ingest to NSQ's ingest01_pre_fetch topic. Source Files Worker Service Files Definition Bucket Reader Ingest No Task File Worker App Cron job that scans depositor receiving buckets for new tar files to ingest.","title":"The Bucket Reader"},{"location":"workers/ingest/bucket-reader/#the-bucket-reader","text":"The bucket reader, ingest_bucket_reader, runs as a cron job inside its own container. It scans all receiving buckets belonging to all depositors for new items. (Items here means tar files containing BagIt bags for ingest.) When it finds items in receiving buckets, it does the following: Checks the Registry to see if a WorkItem exists with action Ingest for the tar file. It matches based on the name and etag of the tar file and the ID of the institution that owns the bucket. If a matching WorkItem exists, ingest_bucket_reader does nothing. If there is no matching item... ingest_bucket_reader creates an Ingest WorkItem for the bag. ingest_bucket_reader adds the WorkItem ID to NSQ's ingest_01_prefetch topic. ingest_bucket_reader logs what it does with every item it encounters, and why. If you want to know what it's doing, check the container logs in CloudWatch.","title":"The Bucket Reader"},{"location":"workers/ingest/bucket-reader/#why-cron-why-not-use-s3-events-and-lambdas","text":"When APTrust first launched in 2014, AWS lambdas didn't exist. Even if they had, we had a bigger problem. The first version of Registry, called Fluctus, was so unreliable, there was no guarantee that it would even be running when S3 events were fired. There was a high likelihood of events being missed. Our cron jobs scan everything each time they run. If the message receiver (Registry) missed cron's messages for any reason, it will get them again in a few minutes. In addition, APTrust's original design requirements explicitly stated that the system must be able to run on any Linux box, anywhere, without relying on any vendor-specific services other than S3 and Glacier. Cron jobs were portable and reliable then, and they still are today. We continue to use them because they work. To put it another way, why do extra work to lock yourself into vendor-specific services, adding another layer of complexity and incurring additional operating costs, when you can do no work and have none of those problems? If you can make a business case for that, there's a position waiting for you at Accenture.","title":"Why cron? Why not use S3 events and lambdas?"},{"location":"workers/ingest/bucket-reader/#external-services","text":"Service Function S3 Receiving Buckets The reader scans depositor receiving buckets for new bags (tar files) to be ingested. Registry The reader creates new WorkItems here for each bag awaiting ingest. NSQ The reader adds WorkItem IDs for bags awaiting ingest to NSQ's ingest01_pre_fetch topic.","title":"External Services"},{"location":"workers/ingest/bucket-reader/#source-files","text":"Worker Service Files Definition Bucket Reader Ingest No Task File Worker App Cron job that scans depositor receiving buckets for new tar files to ingest.","title":"Source Files"},{"location":"workers/ingest/cleanup/","text":"Cleanup After an ingest has been successfully recorded, the cleanup worker does the following: Deletes all of the interim ingest data from Redis Deletes all of the temporary ingest files from the staging bucket Deletes the bag (the tar file) from the depositor's receiving bucket Marks the ingest WorkItem as complete in Registry This is the only ingest work with no \"next\" queue. When it's done, it does not push the ingest WorkItem ID into another NSQ topic. It simply marks it as complete in the ingest09_cleanup topic, and that's the last NSQ hears of it. Note Cleanup does not run on ingests that fail due to transient errors. Those ingests can and should be requeued through the Registry when transient errors pass. (They are usually network errors or \"service unavailable\" errors.) Resources This worker uses little CPU and memory. It may issue a lot of deletion requests to the staging bucket, but even with those, it uses little network I/O. External Services Service Function S3 Staging Bucket Worker deletes all temp files related to this ingest from the staging bucket. S3 Receiving Buckets Worker deletes the tar file uploaded by the depositor from the receiving bucket. Redis Worker deletes all metadata related to this ingest from Redis. Registry Source of WorkItem record describing work to be done. Worker marks the ingest WorkItem complete. NSQ Worker marks the NSQ task complete. Source Files Worker Service Files Definition Cleanup Ingest Task Worker App Cleans up all of the temporary resources created during the ingest process and deletes ingested bags from receiving buckets.","title":"Cleanup"},{"location":"workers/ingest/cleanup/#cleanup","text":"After an ingest has been successfully recorded, the cleanup worker does the following: Deletes all of the interim ingest data from Redis Deletes all of the temporary ingest files from the staging bucket Deletes the bag (the tar file) from the depositor's receiving bucket Marks the ingest WorkItem as complete in Registry This is the only ingest work with no \"next\" queue. When it's done, it does not push the ingest WorkItem ID into another NSQ topic. It simply marks it as complete in the ingest09_cleanup topic, and that's the last NSQ hears of it. Note Cleanup does not run on ingests that fail due to transient errors. Those ingests can and should be requeued through the Registry when transient errors pass. (They are usually network errors or \"service unavailable\" errors.)","title":"Cleanup"},{"location":"workers/ingest/cleanup/#resources","text":"This worker uses little CPU and memory. It may issue a lot of deletion requests to the staging bucket, but even with those, it uses little network I/O.","title":"Resources"},{"location":"workers/ingest/cleanup/#external-services","text":"Service Function S3 Staging Bucket Worker deletes all temp files related to this ingest from the staging bucket. S3 Receiving Buckets Worker deletes the tar file uploaded by the depositor from the receiving bucket. Redis Worker deletes all metadata related to this ingest from Redis. Registry Source of WorkItem record describing work to be done. Worker marks the ingest WorkItem complete. NSQ Worker marks the NSQ task complete.","title":"External Services"},{"location":"workers/ingest/cleanup/#source-files","text":"Worker Service Files Definition Cleanup Ingest Task Worker App Cleans up all of the temporary resources created during the ingest process and deletes ingested bags from receiving buckets.","title":"Source Files"},{"location":"workers/ingest/format-identifier/","text":"The Format Identifier The format identifier uses Siegfried to identify file formats based on byte sequence signatures in the PRONOM registry. It streams files one by one from the staging bucket through its identification algorithms and records the results in Redis. If you look at the Redis file record , you'll notice the following fields: file_format - this is the format as identified by the format identifier format_identified_by - this will be either \"siegfried,\" indicating that Siegfried match the file to an entry in the PRONOM registry, or \"ext map,\" indicating that the file could not be matched, so it was identified by its extension. format_match_type - describes how the match was determined. \"signature\" means it was matched by Sigfried comparing it to a PRONOM signature, \"extension\" means either Siegfried or a circuit breaker (see below) matched it by extension, or \"container\" means Siegfried matched it as a container type (e.g. tar, zip, jar, rar, or certain Microsoft Office file types that contain multiple internal file streams). Short Circuiting and Extension Matching Siegfried may crash when attempting to identify certain container formats if the file is corrupt or internally inconsistent. This happens most often with proprietary Microsoft Office container formats as listed in the CrashableFormats map in the format identifier source code . When the format identifier encounters a file with a crashable extension, it skips Siegfried's PRONOM-matching algorithms and matches on extension only. In this case, the format_identified_by attribute of the Redis file record will be set to ext map , and the format_match_type will be set to extension , indicating that Siegfried didn't even attempt to identify it. In other cases, Siegfried may try and fail to do a byte signature match, then fall back to an extension match. In these cases, format_identified_by will be siegfried and format_match_type will be extension . Resources Siegfried uses large amounts of network I/O and memory because it runs large files through a number of internal functions. Memory, in particular, is a problem in our low-resource Docker containers. To prevent out-of-memory exceptions, the format identifier containers process only one file at a time. This makes the format identifier a bottleneck in the ingest pipeline. This worker is also the most likely to scale to multiple instance even under light loads. The format identifier still occasionally dies before completing its work. This may be due to out-of-memory exceptions, or it may be due to the worker running on spot instances that are killed by AWS because their owners want them back. In either case, simply requeing the item in the Registry fixes the problem. Requeue to the Format Identification stage and the identifier will pick up where the dead worker left off. External Services Service Function S3 Staging Bucket Worker streams files from staging through a format identification function to determine file format. Redis Worker updates file records in Redis with file format and some metadata about how the file format was determined. Registry Source of WorkItem record describing work to be done. NSQ Distributes WorkItem IDs to workers and tracks their status. Source Files Worker Service Files Definition Format Identifier Ingest Task Worker App Identifies the format of files within a bag.","title":"The Format Identifier"},{"location":"workers/ingest/format-identifier/#the-format-identifier","text":"The format identifier uses Siegfried to identify file formats based on byte sequence signatures in the PRONOM registry. It streams files one by one from the staging bucket through its identification algorithms and records the results in Redis. If you look at the Redis file record , you'll notice the following fields: file_format - this is the format as identified by the format identifier format_identified_by - this will be either \"siegfried,\" indicating that Siegfried match the file to an entry in the PRONOM registry, or \"ext map,\" indicating that the file could not be matched, so it was identified by its extension. format_match_type - describes how the match was determined. \"signature\" means it was matched by Sigfried comparing it to a PRONOM signature, \"extension\" means either Siegfried or a circuit breaker (see below) matched it by extension, or \"container\" means Siegfried matched it as a container type (e.g. tar, zip, jar, rar, or certain Microsoft Office file types that contain multiple internal file streams).","title":"The Format Identifier"},{"location":"workers/ingest/format-identifier/#short-circuiting-and-extension-matching","text":"Siegfried may crash when attempting to identify certain container formats if the file is corrupt or internally inconsistent. This happens most often with proprietary Microsoft Office container formats as listed in the CrashableFormats map in the format identifier source code . When the format identifier encounters a file with a crashable extension, it skips Siegfried's PRONOM-matching algorithms and matches on extension only. In this case, the format_identified_by attribute of the Redis file record will be set to ext map , and the format_match_type will be set to extension , indicating that Siegfried didn't even attempt to identify it. In other cases, Siegfried may try and fail to do a byte signature match, then fall back to an extension match. In these cases, format_identified_by will be siegfried and format_match_type will be extension .","title":"Short Circuiting and Extension Matching"},{"location":"workers/ingest/format-identifier/#resources","text":"Siegfried uses large amounts of network I/O and memory because it runs large files through a number of internal functions. Memory, in particular, is a problem in our low-resource Docker containers. To prevent out-of-memory exceptions, the format identifier containers process only one file at a time. This makes the format identifier a bottleneck in the ingest pipeline. This worker is also the most likely to scale to multiple instance even under light loads. The format identifier still occasionally dies before completing its work. This may be due to out-of-memory exceptions, or it may be due to the worker running on spot instances that are killed by AWS because their owners want them back. In either case, simply requeing the item in the Registry fixes the problem. Requeue to the Format Identification stage and the identifier will pick up where the dead worker left off.","title":"Resources"},{"location":"workers/ingest/format-identifier/#external-services","text":"Service Function S3 Staging Bucket Worker streams files from staging through a format identification function to determine file format. Redis Worker updates file records in Redis with file format and some metadata about how the file format was determined. Registry Source of WorkItem record describing work to be done. NSQ Distributes WorkItem IDs to workers and tracks their status.","title":"External Services"},{"location":"workers/ingest/format-identifier/#source-files","text":"Worker Service Files Definition Format Identifier Ingest Task Worker App Identifies the format of files within a bag.","title":"Source Files"},{"location":"workers/ingest/pre-fetch/","text":"The Metadata Gatherer The metadata gatherer, ingest_pre_fetch, streams a bag from a depositor's receiving bucket though a series of functions to do the following: Calculate checksums on all files in the bag. Parse the bag's tag files. Extract the bag's tag files and manifests. Collect general metada about the bag, including it's name, size, number of files, and owning institution. The worker stores the tag files and manifests in the S3 staging bucket. In production, that would be bucket aptrust.prod.staging . All of these files go into a folder under the WorkItem ID. For example, the pre-fetch worker would produce the following set of staging files for WorkItem 6388: aptrust.prod.staging/6388/aptrust-info.txt aptrust.prod.staging/6388/bag-info.txt aptrust.prod.staging/6388/manifest-md5.txt aptrust.prod.staging/6388/manifest-sha256.txt aptrust.prod.staging/6388/tagmanifest-md5.txt aptrust.prod.staging/6388/tagmanifest-sha256.txt The worker also stores all of the essential medata it gathered in Redis. This includes JSON records for every file in the bag, with each JSON record recording, among other things, the file's path and checksums. The next worker, the validator, will examine this data to ensure the bag is valid and can be ingested. For details on what the Redis data looks like, see the section on Querying Redis , which includes sample records. Resource Usage This worker uses a substantial amount of network bandwidth (streaming bags from receiving buckets) and CPU (for calculating multiple checksums on files). External Services Service Function S3 Receiving Buckets Worker reads tar files from depositor receiving buckets. S3 Staging Bucket Worker copies manifests and tag files (but not other files) to the staging bucket for later access by the bag validator. Redis Worker saves metadata about the bag and all of its files in JSON format to Redis, where all subsequent workers can access it. Registry Source of WorkItem record describing work to be done. NSQ Distributes WorkItem IDs to workers and tracks their status. Source Files Worker Service Files Definition Metadata Gatherer Ingest Task Worker App Parses a bag's tag files, calculates checksums on bag contents, and copies manifests and tag files to the ingest staging bucket.","title":"The Metadata Gatherer"},{"location":"workers/ingest/pre-fetch/#the-metadata-gatherer","text":"The metadata gatherer, ingest_pre_fetch, streams a bag from a depositor's receiving bucket though a series of functions to do the following: Calculate checksums on all files in the bag. Parse the bag's tag files. Extract the bag's tag files and manifests. Collect general metada about the bag, including it's name, size, number of files, and owning institution. The worker stores the tag files and manifests in the S3 staging bucket. In production, that would be bucket aptrust.prod.staging . All of these files go into a folder under the WorkItem ID. For example, the pre-fetch worker would produce the following set of staging files for WorkItem 6388: aptrust.prod.staging/6388/aptrust-info.txt aptrust.prod.staging/6388/bag-info.txt aptrust.prod.staging/6388/manifest-md5.txt aptrust.prod.staging/6388/manifest-sha256.txt aptrust.prod.staging/6388/tagmanifest-md5.txt aptrust.prod.staging/6388/tagmanifest-sha256.txt The worker also stores all of the essential medata it gathered in Redis. This includes JSON records for every file in the bag, with each JSON record recording, among other things, the file's path and checksums. The next worker, the validator, will examine this data to ensure the bag is valid and can be ingested. For details on what the Redis data looks like, see the section on Querying Redis , which includes sample records.","title":"The Metadata Gatherer"},{"location":"workers/ingest/pre-fetch/#resource-usage","text":"This worker uses a substantial amount of network bandwidth (streaming bags from receiving buckets) and CPU (for calculating multiple checksums on files).","title":"Resource Usage"},{"location":"workers/ingest/pre-fetch/#external-services","text":"Service Function S3 Receiving Buckets Worker reads tar files from depositor receiving buckets. S3 Staging Bucket Worker copies manifests and tag files (but not other files) to the staging bucket for later access by the bag validator. Redis Worker saves metadata about the bag and all of its files in JSON format to Redis, where all subsequent workers can access it. Registry Source of WorkItem record describing work to be done. NSQ Distributes WorkItem IDs to workers and tracks their status.","title":"External Services"},{"location":"workers/ingest/pre-fetch/#source-files","text":"Worker Service Files Definition Metadata Gatherer Ingest Task Worker App Parses a bag's tag files, calculates checksums on bag contents, and copies manifests and tag files to the ingest staging bucket.","title":"Source Files"},{"location":"workers/ingest/preservation-uploader/","text":"The Preservation Uploader The preservation uploader copies files from the S3 staging bucket to long-term preservation storage, which may be another S3 bucket, a Glacier vault, or a Wasabi bucket. (See Preservation Buckets for details on where they are.) When copying to the main preservation bucket in Virginia, the uploader uses AWS's bucket-to-bucket copying. When copying to Wasabi or to any region outside of us-east-1, the uploader streams the file through the Docker container to preservation storage. Ideally, it would use bucket-to-bucket copying for other AWS regions, but bucket-to-bucket is unacceptably slow for inter-region copies. This worker creates a StorageRecord for each copy of each file it moves into preservation storage. The StorageRecord includes information about where and when the file was stored. These records are attached to the file record and stored in Redis. They will eventually go into the registry. Resources The preservation uploader can use substantial amounts of memory and network I/O. External Services Service Function S3 Staging Bucket Worker copies files from staging to long-term storage. Preservation Buckets Worker copies files from staging to long-term storage. Preservation buckets may include S3, Glacier, Glacier Deep Archive, and Wasabi. Redis Worker adds storage records to each file record in Redis. The storage record describes which preservation bucket(s) the file was copied to and when. For standard storage, each file ends up with two storage records, one for S3/Virginia and one for Glacier/Oregon. All other storage options result in a single storage record. Registry Source of WorkItem record describing work to be done. NSQ Distributes WorkItem IDs to workers and tracks their status. Source Files Worker Service Files Definition Preservation Uploader Ingest Task Worker App Copies files to preservation storage.","title":"The Preservation Uploader"},{"location":"workers/ingest/preservation-uploader/#the-preservation-uploader","text":"The preservation uploader copies files from the S3 staging bucket to long-term preservation storage, which may be another S3 bucket, a Glacier vault, or a Wasabi bucket. (See Preservation Buckets for details on where they are.) When copying to the main preservation bucket in Virginia, the uploader uses AWS's bucket-to-bucket copying. When copying to Wasabi or to any region outside of us-east-1, the uploader streams the file through the Docker container to preservation storage. Ideally, it would use bucket-to-bucket copying for other AWS regions, but bucket-to-bucket is unacceptably slow for inter-region copies. This worker creates a StorageRecord for each copy of each file it moves into preservation storage. The StorageRecord includes information about where and when the file was stored. These records are attached to the file record and stored in Redis. They will eventually go into the registry.","title":"The Preservation Uploader"},{"location":"workers/ingest/preservation-uploader/#resources","text":"The preservation uploader can use substantial amounts of memory and network I/O.","title":"Resources"},{"location":"workers/ingest/preservation-uploader/#external-services","text":"Service Function S3 Staging Bucket Worker copies files from staging to long-term storage. Preservation Buckets Worker copies files from staging to long-term storage. Preservation buckets may include S3, Glacier, Glacier Deep Archive, and Wasabi. Redis Worker adds storage records to each file record in Redis. The storage record describes which preservation bucket(s) the file was copied to and when. For standard storage, each file ends up with two storage records, one for S3/Virginia and one for Glacier/Oregon. All other storage options result in a single storage record. Registry Source of WorkItem record describing work to be done. NSQ Distributes WorkItem IDs to workers and tracks their status.","title":"External Services"},{"location":"workers/ingest/preservation-uploader/#source-files","text":"Worker Service Files Definition Preservation Uploader Ingest Task Worker App Copies files to preservation storage.","title":"Source Files"},{"location":"workers/ingest/preservation-verifier/","text":"The Preservation Verifier The preservation verifier checks that all of the files copied by the preservation uploader are actually present in the preservation storage buckets. It issues a HEAD request for each file and ensures that the size matches. When possible, it also ensures that the etag matches the file's md5 checksum. (This is only possible for smaller files, not for large, mutli-part uploads.) If any check fails, the verifier marks the ingest as failed, sets a note about the missing/incorrect file in the note field of the WorkItem, and sets the WorkItem's NeedsAdminReview flag to true. In case you're wondering why this component exists, see Why Does the Preservation Verifier Exist? Resources Though this worker may issue a number of S3 requests, it does not use much network bandwidth because HEAD requests tend to return about 1 kb of data. The worker uses little CPU and memory, and tends to finish quickly. External Services Service Function Preservation Buckets Worker verifies that files were successfully copied to perservation storage, as described in the storage records retrieved from Redis. Redis Worker updates file records to indicate that files have been verified in preservation storage. Registry Source of WorkItem record describing work to be done. NSQ Distributes WorkItem IDs to workers and tracks their status. Source Files Worker Service Files Definition Preservation Verifier Ingest Task Worker App Verifies that files copied to preservation storage are actually there.","title":"The Preservation Verifier"},{"location":"workers/ingest/preservation-verifier/#the-preservation-verifier","text":"The preservation verifier checks that all of the files copied by the preservation uploader are actually present in the preservation storage buckets. It issues a HEAD request for each file and ensures that the size matches. When possible, it also ensures that the etag matches the file's md5 checksum. (This is only possible for smaller files, not for large, mutli-part uploads.) If any check fails, the verifier marks the ingest as failed, sets a note about the missing/incorrect file in the note field of the WorkItem, and sets the WorkItem's NeedsAdminReview flag to true. In case you're wondering why this component exists, see Why Does the Preservation Verifier Exist?","title":"The Preservation Verifier"},{"location":"workers/ingest/preservation-verifier/#resources","text":"Though this worker may issue a number of S3 requests, it does not use much network bandwidth because HEAD requests tend to return about 1 kb of data. The worker uses little CPU and memory, and tends to finish quickly.","title":"Resources"},{"location":"workers/ingest/preservation-verifier/#external-services","text":"Service Function Preservation Buckets Worker verifies that files were successfully copied to perservation storage, as described in the storage records retrieved from Redis. Redis Worker updates file records to indicate that files have been verified in preservation storage. Registry Source of WorkItem record describing work to be done. NSQ Distributes WorkItem IDs to workers and tracks their status.","title":"External Services"},{"location":"workers/ingest/preservation-verifier/#source-files","text":"Worker Service Files Definition Preservation Verifier Ingest Task Worker App Verifies that files copied to preservation storage are actually there.","title":"Source Files"},{"location":"workers/ingest/recorder/","text":"The Recorder The recorder records all metadata from the ingest in the Registry. It gets the metadata from Redis, where information about the object and files has been built up by the previous workers as they processed the ingest. For example, the metadata gatherer parsed the tag files and calculated the checksums, the reingest manager reassigned UUIDs where necessary, the format identifier set file mime types, and the preservation uploader created storage records describing where the files now live. The recorder creates or updates an intellectual object record in the Registry with the following info: Object identifier Owning institution Storage option Internal identifier and description Bag Group Identifier BagIt Profile ID Various other metadata parsed from bag-info.txt and aptrust-info.txt The recorder then creates object-level Premis events, including events for ingestion, identifier assignment, and access assignment. Once the object record exists, the recorder creates Generic File records for each file in the bag. It also records Premis Events, Checksums and Storage Records for each Generic File. The recorder sends Generic Files (with related records) in batches of 100, and it knows how to separate new files (POST/insert) from reingests (PUT/update). Resources The recorder creates quite a bit of network chatter between itself, Redis and Registry. It uses a moderate amount of CPU and memory. Note Recording bags that have thousands, or even hundreds of thousands of files can be taxing on the Registry and the underlying Postgres database. It's normal for Registry and database performance to degrade during large recording operations. External Services Service Function Redis Worker gathers all object and file metadata to be recorded in Registry. Registry Source of WorkItem record describing work to be done. Worker records all object and file metadata (plus checksums, storage records and Premis events) related to this ingest. NSQ Distributes WorkItem IDs to workers and tracks their status. Source Files Worker Service Files Definition Ingest Recorder Ingest Task Worker App Records all ingest data in Registry.","title":"The Recorder"},{"location":"workers/ingest/recorder/#the-recorder","text":"The recorder records all metadata from the ingest in the Registry. It gets the metadata from Redis, where information about the object and files has been built up by the previous workers as they processed the ingest. For example, the metadata gatherer parsed the tag files and calculated the checksums, the reingest manager reassigned UUIDs where necessary, the format identifier set file mime types, and the preservation uploader created storage records describing where the files now live. The recorder creates or updates an intellectual object record in the Registry with the following info: Object identifier Owning institution Storage option Internal identifier and description Bag Group Identifier BagIt Profile ID Various other metadata parsed from bag-info.txt and aptrust-info.txt The recorder then creates object-level Premis events, including events for ingestion, identifier assignment, and access assignment. Once the object record exists, the recorder creates Generic File records for each file in the bag. It also records Premis Events, Checksums and Storage Records for each Generic File. The recorder sends Generic Files (with related records) in batches of 100, and it knows how to separate new files (POST/insert) from reingests (PUT/update).","title":"The Recorder"},{"location":"workers/ingest/recorder/#resources","text":"The recorder creates quite a bit of network chatter between itself, Redis and Registry. It uses a moderate amount of CPU and memory. Note Recording bags that have thousands, or even hundreds of thousands of files can be taxing on the Registry and the underlying Postgres database. It's normal for Registry and database performance to degrade during large recording operations.","title":"Resources"},{"location":"workers/ingest/recorder/#external-services","text":"Service Function Redis Worker gathers all object and file metadata to be recorded in Registry. Registry Source of WorkItem record describing work to be done. Worker records all object and file metadata (plus checksums, storage records and Premis events) related to this ingest. NSQ Distributes WorkItem IDs to workers and tracks their status.","title":"External Services"},{"location":"workers/ingest/recorder/#source-files","text":"Worker Service Files Definition Ingest Recorder Ingest Task Worker App Records all ingest data in Registry.","title":"Source Files"},{"location":"workers/ingest/reingest-manager/","text":"The Reingest Manager The reingest manager checks to see whether a bag has ever been ingested before. All bags have a unique intellectual object identifier in the format institution_identifier/bag_name. When we're ingesting a bag from the University of Virginia's receiving bucket called photos.tar, the intellectual object identifier for that bag will be virginia.edu/photos. The reingest manager asks the Registry if an active (non-deleted) object with that identifier already exists. If Registry has no active record of this object, all files in the bag will be saved to preservation storage under a new UUID. If Registry does have a record of the bag, the reingest manager then compares the names and checksums of all the files in the new tarred bag to those in Registry. Files not in the Registry will be saved to preservation storage under a new UUID. Files in the Registry that have changed in the new tarred bag (i.e. those whose checksums differ from the Registry checksums) will be saved to preservation storage under Registry's existing UUID. We do not want two different versions of the same file saved under different UUIDs in storage because they cause confusion and add cost. We want only one authoritative version. Files in the bag whose checksums match Registry checksums will be marked as NeedsSave = false and will not be copied to preservation storage by the Preservation Uploader. Resources The reingest manager typically uses very little CPU, memory and network I/O. Most bags are not reingests. The manager issues a single API call to the Registry's objects endpoint, learns the object is new, and its work is done. If the Registry does have a record of the object, the reingest manager pulls file metadata records one by one from Redis, then pulls corresponding file records one by one from Registry and compares their checksums. In typical cases, this results in a dozen or so calls to each service. In very rare cases (once or twice a year), it may lead to tens of thousands of calls to each service. External Services Service Function Redis Worker retrieves object and file metadata to get identifiers that it will look up in Registry. It flags files being reingested and files needing to be saved, then saves the info back to Redis so the preservation uploader will know which files to copy to long-term storage, and what UUIDs to use as keys (S3 file names). Registry Source of WorkItem record describing work to be done. The worker queries Registry to look for existing object and file records that would indicate that the current bag is a reingest. NSQ Distributes WorkItem IDs to workers and tracks their status. Source Files Worker Service Files Definition Reingest Manager Ingest Task Worker App Checks if a bag is being reingested and if so, applies special processing.","title":"The Reingest Manager"},{"location":"workers/ingest/reingest-manager/#the-reingest-manager","text":"The reingest manager checks to see whether a bag has ever been ingested before. All bags have a unique intellectual object identifier in the format institution_identifier/bag_name. When we're ingesting a bag from the University of Virginia's receiving bucket called photos.tar, the intellectual object identifier for that bag will be virginia.edu/photos. The reingest manager asks the Registry if an active (non-deleted) object with that identifier already exists. If Registry has no active record of this object, all files in the bag will be saved to preservation storage under a new UUID. If Registry does have a record of the bag, the reingest manager then compares the names and checksums of all the files in the new tarred bag to those in Registry. Files not in the Registry will be saved to preservation storage under a new UUID. Files in the Registry that have changed in the new tarred bag (i.e. those whose checksums differ from the Registry checksums) will be saved to preservation storage under Registry's existing UUID. We do not want two different versions of the same file saved under different UUIDs in storage because they cause confusion and add cost. We want only one authoritative version. Files in the bag whose checksums match Registry checksums will be marked as NeedsSave = false and will not be copied to preservation storage by the Preservation Uploader.","title":"The Reingest Manager"},{"location":"workers/ingest/reingest-manager/#resources","text":"The reingest manager typically uses very little CPU, memory and network I/O. Most bags are not reingests. The manager issues a single API call to the Registry's objects endpoint, learns the object is new, and its work is done. If the Registry does have a record of the object, the reingest manager pulls file metadata records one by one from Redis, then pulls corresponding file records one by one from Registry and compares their checksums. In typical cases, this results in a dozen or so calls to each service. In very rare cases (once or twice a year), it may lead to tens of thousands of calls to each service.","title":"Resources"},{"location":"workers/ingest/reingest-manager/#external-services","text":"Service Function Redis Worker retrieves object and file metadata to get identifiers that it will look up in Registry. It flags files being reingested and files needing to be saved, then saves the info back to Redis so the preservation uploader will know which files to copy to long-term storage, and what UUIDs to use as keys (S3 file names). Registry Source of WorkItem record describing work to be done. The worker queries Registry to look for existing object and file records that would indicate that the current bag is a reingest. NSQ Distributes WorkItem IDs to workers and tracks their status.","title":"External Services"},{"location":"workers/ingest/reingest-manager/#source-files","text":"Worker Service Files Definition Reingest Manager Ingest Task Worker App Checks if a bag is being reingested and if so, applies special processing.","title":"Source Files"},{"location":"workers/ingest/staging-uploader/","text":"The Staging Uploader The staging uploader, ingest_staging_uploader, unpacks a tarred bag from a receiving bucket and copies all of its files into an S3 staging bucket where other ingest workers can access them. For information about what's in the staging bucket and how keys are composed, see the staging bucket documentation. By the time the staging uploader gets to work, tag files and manifests have already copied to the staging bucket under their own names (aptrust-info.txt, manifest-sha256.txt, etc.). The metadata gatherer did this early on so it and other workers could access those metadata files as needed. All other files are copied with UUID keys which are assigned by the metadata gatherer (for new files) or the reingest manager (for files being reingested). Tag files bound for preservation storage, including bag-info.txt and aptrust-info.txt will also be copied to the staging bucket under their permanent UUID. When the preservation uploader copies files from staging to preservation buckets later on, all of the files will retain their UUID in the preservation storage. Why Wait So Long to Go to Staging? Unpacking a bag and copying all of its files to a staging area is an expensive in terms of memory, network I/O and time. We don't want to take this step until we're sure that the bag is valid and that the reingest manager has been able to assign proper UUIDs. Why Use a Staging Bucket? After files are unpacked, at least two more workers, the format identifier and the preservation uploader , need to access them. If we unpack files to a local disk, other workers can't get to them. Local EBS volumes are also expensive. S3 happens to work well for this write-once, read-multiple workload. (The staging uploader writes each file once to S3. Subsequent workers read the files at least once. For now, only the format identifier and the preservation uploader read files from the staging bucket. In the future, it's possible to add more workers, such as virus scanners, format migrators, etc., to the pipeline. All of these workers can be distributed and all can have multiple instances. Note The staging bucket is in the same AWS region as receiving buckets and our ingest workers. This means copying files to staging is reasonably fast and will not incur inter-region transfer costs. Resources The staging uploader can use large amounts of memory when copying very large files because the underlying Minio library uses memory buffers for large uploads. Large bags also require a lot of network I/O. This worker is generally limited to two go routines for uploading files because the Docker container in which it runs has limited memory. Given Minio's heavy memory usage for large-file uploads, running too many go routines at once may lead to out-of-memory exceptions. When the staging uploader gets overloaded, we simply add new containers to spread the load. Like all other ingest workers, the staging uploader keeps track of which tasks it has completed in Redis. If the worker dies or requeues the task due to network errors (which are fairly common), a new worker can pick up where the old one left off without having to duplicate expensive work. External Services Service Function S3 Receiving Buckets Worker reads tar files from depositor receiving buckets, extracting payload files and tag files to be copied to staging. S3 Staging Bucket Worker copies payload files and tag files to from receiving bucket to staging bucket. Files go into staging with a UUID key, not their actual file name. Redis Worker updates Redis file records to indicate files have been copied to staging. Registry Source of WorkItem record describing work to be done. NSQ Distributes WorkItem IDs to workers and tracks their status. Source Files Worker Service Files Definition Staging Uploader Ingest Task Worker App Copies files from a tarred bag in a receiving bucket to the ingest staging bucket.","title":"The Staging Uploader"},{"location":"workers/ingest/staging-uploader/#the-staging-uploader","text":"The staging uploader, ingest_staging_uploader, unpacks a tarred bag from a receiving bucket and copies all of its files into an S3 staging bucket where other ingest workers can access them. For information about what's in the staging bucket and how keys are composed, see the staging bucket documentation. By the time the staging uploader gets to work, tag files and manifests have already copied to the staging bucket under their own names (aptrust-info.txt, manifest-sha256.txt, etc.). The metadata gatherer did this early on so it and other workers could access those metadata files as needed. All other files are copied with UUID keys which are assigned by the metadata gatherer (for new files) or the reingest manager (for files being reingested). Tag files bound for preservation storage, including bag-info.txt and aptrust-info.txt will also be copied to the staging bucket under their permanent UUID. When the preservation uploader copies files from staging to preservation buckets later on, all of the files will retain their UUID in the preservation storage.","title":"The Staging Uploader"},{"location":"workers/ingest/staging-uploader/#why-wait-so-long-to-go-to-staging","text":"Unpacking a bag and copying all of its files to a staging area is an expensive in terms of memory, network I/O and time. We don't want to take this step until we're sure that the bag is valid and that the reingest manager has been able to assign proper UUIDs.","title":"Why Wait So Long to Go to Staging?"},{"location":"workers/ingest/staging-uploader/#why-use-a-staging-bucket","text":"After files are unpacked, at least two more workers, the format identifier and the preservation uploader , need to access them. If we unpack files to a local disk, other workers can't get to them. Local EBS volumes are also expensive. S3 happens to work well for this write-once, read-multiple workload. (The staging uploader writes each file once to S3. Subsequent workers read the files at least once. For now, only the format identifier and the preservation uploader read files from the staging bucket. In the future, it's possible to add more workers, such as virus scanners, format migrators, etc., to the pipeline. All of these workers can be distributed and all can have multiple instances. Note The staging bucket is in the same AWS region as receiving buckets and our ingest workers. This means copying files to staging is reasonably fast and will not incur inter-region transfer costs.","title":"Why Use a Staging Bucket?"},{"location":"workers/ingest/staging-uploader/#resources","text":"The staging uploader can use large amounts of memory when copying very large files because the underlying Minio library uses memory buffers for large uploads. Large bags also require a lot of network I/O. This worker is generally limited to two go routines for uploading files because the Docker container in which it runs has limited memory. Given Minio's heavy memory usage for large-file uploads, running too many go routines at once may lead to out-of-memory exceptions. When the staging uploader gets overloaded, we simply add new containers to spread the load. Like all other ingest workers, the staging uploader keeps track of which tasks it has completed in Redis. If the worker dies or requeues the task due to network errors (which are fairly common), a new worker can pick up where the old one left off without having to duplicate expensive work.","title":"Resources"},{"location":"workers/ingest/staging-uploader/#external-services","text":"Service Function S3 Receiving Buckets Worker reads tar files from depositor receiving buckets, extracting payload files and tag files to be copied to staging. S3 Staging Bucket Worker copies payload files and tag files to from receiving bucket to staging bucket. Files go into staging with a UUID key, not their actual file name. Redis Worker updates Redis file records to indicate files have been copied to staging. Registry Source of WorkItem record describing work to be done. NSQ Distributes WorkItem IDs to workers and tracks their status.","title":"External Services"},{"location":"workers/ingest/staging-uploader/#source-files","text":"Worker Service Files Definition Staging Uploader Ingest Task Worker App Copies files from a tarred bag in a receiving bucket to the ingest staging bucket.","title":"Source Files"},{"location":"workers/ingest/validator/","text":"The Bag Validator The pre-fetch worker (ingest_pre_fetch) streams bags from a receiving bucket through a series of analytical functions, parsing manifests and tag files, and calculating checksums. It leaves the results of its work in Redis. (See Querying Redis for examples of what those results look like.) After the pre-fetch worker completes, the bag validator, ingest_validator, examines all of the Redis data to ensure the following: That the bag uses one of our supported BagIt profiles (either APTrust or BTR). The profile should be declared in the BagIt-Profile-Identifier tag in the bag-info.txt file. If it's missing, we assume the bag uses the APTrust profile, and we validate it accordingly. That the bag conforms to the profile, meaning: It includes all required manifests and tag manifests. It does not include forbidden manifests or tag manifests. It does not include a fetch.txt file (we don't support this at all). All required tags files are present. All required tags are present and have legal values. That all files mentioned in the manifests appear in the payload directory, and that all of their checksums match. That the payload directory contains no extraneous files (i.e. nothing more than what is mentioned in the manifests). That tag manifest checksums are correct. Note that many profiles, including APTrust and BTR (Beyond the Repository) allow for the presence of additional files outside the payload directory that are not mentioned in payload manifests. APTrust treats all such files as custom tag files if they: 1. are not in the payload (data) directory 2. do not match any manifest file naming pattern (e.g. manifest-md5.txt, manifest-sha256.txt, etc.) 3. do not match any tag manifest file naming pattern (e.g. tagmanifest-md5.txt, tagmanifest-sha256.txt, etc.) Invalid Bags Are Fatal Errors If the validator determines a bag is invalid, it marks the WorkItem as failed and adds the specific validation errors to the WorkItem.Note field, which both the depositor and APTrust admins can see in the Registry. Because we cannot ingest an invalid bag, no further work is done on the bag. Note The old Exchange ingest system, which we retired in November, 2022, used to automatically delete invalid bags from the receiving buckets. Preserv does not do that because its validator is not yet as battle-tested as Exchanges. In cases where it incorrectly marks bags as invalid (and there were several in the first week of production), we want to keep the bags available for analysis. We may have the validator automatically delete invalid bags after we've been in production long enough to trust it to handle odd edge cases. Until then, we rely on lifecycle policies in the receiving buckets to delete bags older than 60 days. Bags that pass validation move into the ingest03_reingest_check topic in NSQ. Resources The ingest_validator is very fast, using minimal CPU, memory, and network bandwidth. It generally completing its work in less than a second, because the metadata gatherer did most of the intensive work earlier. The validator fetches manifests and tag manifests stored in the staging bucket. It verifies that the tag files conform to the BagIt profile, then compares the checksums in the manifests against the checksums stored in Redis. For info on the structure of a bag's interim data in the staging bucket, see the Staging Bucket overview. External Services Service Function S3 Staging Bucket Worker reads manifests and tag files from staging during the validation process. Checksums calculated by the metadata gatherer and stored in Redis should match checksums in the manifests stored in the staging bucket. Redis Worker retrieves object and file metadata from Redis. It updates these JSON records to reflect the work it has done. Registry Source of WorkItem record describing work to be done. NSQ Distributes WorkItem IDs to workers and tracks their status. Source Files Worker Service Files Definition Bag Validator Ingest Task Worker App Validates a bag before ingest.","title":"The Bag Validator"},{"location":"workers/ingest/validator/#the-bag-validator","text":"The pre-fetch worker (ingest_pre_fetch) streams bags from a receiving bucket through a series of analytical functions, parsing manifests and tag files, and calculating checksums. It leaves the results of its work in Redis. (See Querying Redis for examples of what those results look like.) After the pre-fetch worker completes, the bag validator, ingest_validator, examines all of the Redis data to ensure the following: That the bag uses one of our supported BagIt profiles (either APTrust or BTR). The profile should be declared in the BagIt-Profile-Identifier tag in the bag-info.txt file. If it's missing, we assume the bag uses the APTrust profile, and we validate it accordingly. That the bag conforms to the profile, meaning: It includes all required manifests and tag manifests. It does not include forbidden manifests or tag manifests. It does not include a fetch.txt file (we don't support this at all). All required tags files are present. All required tags are present and have legal values. That all files mentioned in the manifests appear in the payload directory, and that all of their checksums match. That the payload directory contains no extraneous files (i.e. nothing more than what is mentioned in the manifests). That tag manifest checksums are correct. Note that many profiles, including APTrust and BTR (Beyond the Repository) allow for the presence of additional files outside the payload directory that are not mentioned in payload manifests. APTrust treats all such files as custom tag files if they: 1. are not in the payload (data) directory 2. do not match any manifest file naming pattern (e.g. manifest-md5.txt, manifest-sha256.txt, etc.) 3. do not match any tag manifest file naming pattern (e.g. tagmanifest-md5.txt, tagmanifest-sha256.txt, etc.)","title":"The Bag Validator"},{"location":"workers/ingest/validator/#invalid-bags-are-fatal-errors","text":"If the validator determines a bag is invalid, it marks the WorkItem as failed and adds the specific validation errors to the WorkItem.Note field, which both the depositor and APTrust admins can see in the Registry. Because we cannot ingest an invalid bag, no further work is done on the bag. Note The old Exchange ingest system, which we retired in November, 2022, used to automatically delete invalid bags from the receiving buckets. Preserv does not do that because its validator is not yet as battle-tested as Exchanges. In cases where it incorrectly marks bags as invalid (and there were several in the first week of production), we want to keep the bags available for analysis. We may have the validator automatically delete invalid bags after we've been in production long enough to trust it to handle odd edge cases. Until then, we rely on lifecycle policies in the receiving buckets to delete bags older than 60 days. Bags that pass validation move into the ingest03_reingest_check topic in NSQ.","title":"Invalid Bags Are Fatal Errors"},{"location":"workers/ingest/validator/#resources","text":"The ingest_validator is very fast, using minimal CPU, memory, and network bandwidth. It generally completing its work in less than a second, because the metadata gatherer did most of the intensive work earlier. The validator fetches manifests and tag manifests stored in the staging bucket. It verifies that the tag files conform to the BagIt profile, then compares the checksums in the manifests against the checksums stored in Redis. For info on the structure of a bag's interim data in the staging bucket, see the Staging Bucket overview.","title":"Resources"},{"location":"workers/ingest/validator/#external-services","text":"Service Function S3 Staging Bucket Worker reads manifests and tag files from staging during the validation process. Checksums calculated by the metadata gatherer and stored in Redis should match checksums in the manifests stored in the staging bucket. Redis Worker retrieves object and file metadata from Redis. It updates these JSON records to reflect the work it has done. Registry Source of WorkItem record describing work to be done. NSQ Distributes WorkItem IDs to workers and tracks their status.","title":"External Services"},{"location":"workers/ingest/validator/#source-files","text":"Worker Service Files Definition Bag Validator Ingest Task Worker App Validates a bag before ingest.","title":"Source Files"}]}